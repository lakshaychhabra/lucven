<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Why Your Vector Database Thinks $AAPL Means Polish Batteries | Lucven AI</title>
<meta name="keywords" content="Tokenisation">
<meta name="description" content="Why Your Vector Database Thinks $AAPL Means Polish Batteries
Your $50K vector database is returning garbage results because &ldquo;$AAPL&rdquo; tokenizes as [&quot;$&quot;, &ldquo;AA&rdquo;, &ldquo;PL&rdquo;] and now has the embedding of &ldquo;dollar &#43; AA batteries &#43; Poland&rdquo;. Your semantic search for &ldquo;Apple stock&rdquo; returns articles about Polish currency. This isn&rsquo;t a retrieval problem, it&rsquo;s tokenization murdering your embeddings.

TL;DR: Bad tokenization creates noisy embeddings. &ldquo;COVID-19&rdquo; split into [&ldquo;CO&rdquo;, &ldquo;VID&rdquo;, &ldquo;-&rdquo;, &ldquo;19&rdquo;] has embeddings mixing &ldquo;Colorado&rdquo;, &ldquo;video&rdquo;, &ldquo;negative&rdquo;, and &ldquo;2019&rdquo;. Your RAG pipeline is doomed before it starts. Fix tokenization or waste money on larger models trying to compensate.">
<meta name="author" content="Lakshay Chhabra">
<link rel="canonical" href="https://lucven.com/posts/tokenization/tokenisation_limits/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://lucven.com/favicon_io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lucven.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lucven.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lucven.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://lucven.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://lucven.com/posts/tokenization/tokenisation_limits/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:url" content="https://lucven.com/posts/tokenization/tokenisation_limits/">
  <meta property="og:site_name" content="Lucven AI">
  <meta property="og:title" content="Why Your Vector Database Thinks $AAPL Means Polish Batteries">
  <meta property="og:description" content="Why Your Vector Database Thinks $AAPL Means Polish Batteries Your $50K vector database is returning garbage results because ‚Äú$AAPL‚Äù tokenizes as [&#34;$&#34;, ‚ÄúAA‚Äù, ‚ÄúPL‚Äù] and now has the embedding of ‚Äúdollar &#43; AA batteries &#43; Poland‚Äù. Your semantic search for ‚ÄúApple stock‚Äù returns articles about Polish currency. This isn‚Äôt a retrieval problem, it‚Äôs tokenization murdering your embeddings.
TL;DR: Bad tokenization creates noisy embeddings. ‚ÄúCOVID-19‚Äù split into [‚ÄúCO‚Äù, ‚ÄúVID‚Äù, ‚Äú-‚Äù, ‚Äú19‚Äù] has embeddings mixing ‚ÄúColorado‚Äù, ‚Äúvideo‚Äù, ‚Äúnegative‚Äù, and ‚Äú2019‚Äù. Your RAG pipeline is doomed before it starts. Fix tokenization or waste money on larger models trying to compensate.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-09-06T00:10:39+05:30">
    <meta property="article:modified_time" content="2025-09-06T00:10:39+05:30">
    <meta property="article:tag" content="Tokenisation">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Why Your Vector Database Thinks $AAPL Means Polish Batteries">
<meta name="twitter:description" content="Why Your Vector Database Thinks $AAPL Means Polish Batteries
Your $50K vector database is returning garbage results because &ldquo;$AAPL&rdquo; tokenizes as [&quot;$&quot;, &ldquo;AA&rdquo;, &ldquo;PL&rdquo;] and now has the embedding of &ldquo;dollar &#43; AA batteries &#43; Poland&rdquo;. Your semantic search for &ldquo;Apple stock&rdquo; returns articles about Polish currency. This isn&rsquo;t a retrieval problem, it&rsquo;s tokenization murdering your embeddings.

TL;DR: Bad tokenization creates noisy embeddings. &ldquo;COVID-19&rdquo; split into [&ldquo;CO&rdquo;, &ldquo;VID&rdquo;, &ldquo;-&rdquo;, &ldquo;19&rdquo;] has embeddings mixing &ldquo;Colorado&rdquo;, &ldquo;video&rdquo;, &ldquo;negative&rdquo;, and &ldquo;2019&rdquo;. Your RAG pipeline is doomed before it starts. Fix tokenization or waste money on larger models trying to compensate.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://lucven.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Why Your Vector Database Thinks $AAPL Means Polish Batteries",
      "item": "https://lucven.com/posts/tokenization/tokenisation_limits/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Why Your Vector Database Thinks $AAPL Means Polish Batteries",
  "name": "Why Your Vector Database Thinks $AAPL Means Polish Batteries",
  "description": "Why Your Vector Database Thinks $AAPL Means Polish Batteries Your $50K vector database is returning garbage results because \u0026ldquo;$AAPL\u0026rdquo; tokenizes as [\u0026quot;$\u0026quot;, \u0026ldquo;AA\u0026rdquo;, \u0026ldquo;PL\u0026rdquo;] and now has the embedding of \u0026ldquo;dollar + AA batteries + Poland\u0026rdquo;. Your semantic search for \u0026ldquo;Apple stock\u0026rdquo; returns articles about Polish currency. This isn\u0026rsquo;t a retrieval problem, it\u0026rsquo;s tokenization murdering your embeddings.\nTL;DR: Bad tokenization creates noisy embeddings. \u0026ldquo;COVID-19\u0026rdquo; split into [\u0026ldquo;CO\u0026rdquo;, \u0026ldquo;VID\u0026rdquo;, \u0026ldquo;-\u0026rdquo;, \u0026ldquo;19\u0026rdquo;] has embeddings mixing \u0026ldquo;Colorado\u0026rdquo;, \u0026ldquo;video\u0026rdquo;, \u0026ldquo;negative\u0026rdquo;, and \u0026ldquo;2019\u0026rdquo;. Your RAG pipeline is doomed before it starts. Fix tokenization or waste money on larger models trying to compensate.\n",
  "keywords": [
    "Tokenisation"
  ],
  "articleBody": "Why Your Vector Database Thinks $AAPL Means Polish Batteries Your $50K vector database is returning garbage results because ‚Äú$AAPL‚Äù tokenizes as [\"$\", ‚ÄúAA‚Äù, ‚ÄúPL‚Äù] and now has the embedding of ‚Äúdollar + AA batteries + Poland‚Äù. Your semantic search for ‚ÄúApple stock‚Äù returns articles about Polish currency. This isn‚Äôt a retrieval problem, it‚Äôs tokenization murdering your embeddings.\nTL;DR: Bad tokenization creates noisy embeddings. ‚ÄúCOVID-19‚Äù split into [‚ÄúCO‚Äù, ‚ÄúVID‚Äù, ‚Äú-‚Äù, ‚Äú19‚Äù] has embeddings mixing ‚ÄúColorado‚Äù, ‚Äúvideo‚Äù, ‚Äúnegative‚Äù, and ‚Äú2019‚Äù. Your RAG pipeline is doomed before it starts. Fix tokenization or waste money on larger models trying to compensate.\nThe Embedding Murder Scene # Watch tokenization destroy semantic similarity: def embedding_quality_test(tokenizer, embedding_model): \"\"\"See how tokenization ruins your embeddings\"\"\" # Same concept, different tokenizations test_pairs = [ (\"Apple Inc\", \"$AAPL\"), # Company and its ticker (\"COVID-19\", \"coronavirus\"), # Same disease (\"e-commerce\", \"ecommerce\"), # Same concept, different style (\"Ph.D.\", \"PhD\"), # Same degree (\"U.S.A.\", \"USA\"), # Same country ] for term1, term2 in test_pairs: # Tokenize tokens1 = tokenizer.encode(term1) tokens2 = tokenizer.encode(term2) # Get embeddings emb1 = embedding_model.embed(term1) emb2 = embedding_model.embed(term2) # Calculate similarity similarity = cosine_similarity(emb1, emb2) print(f\"{term1} ‚Üí {[tokenizer.decode([t]) for t in tokens1]}\") print(f\"{term2} ‚Üí {[tokenizer.decode([t]) for t in tokens2]}\") print(f\"Similarity: {similarity:.3f}\") if len(tokens1) \u003e 2 or len(tokens2) \u003e 2: print(\"‚ö†Ô∏è FRAGMENTED - Embeddings are NOISE\") print(\"-\" * 50) # Results: # \"$AAPL\" as [\"$\", \"AA\", \"PL\"] ‚Üí similarity with \"Apple\" = 0.15 # \"$AAPL\" as [\"$AAPL\"] ‚Üí similarity with \"Apple\" = 0.75 # 5X DIFFERENCE just from tokenization! The Semantic Catastrophe # How fragmented tokens create nonsense embeddings: def fragmentation_semantic_disaster(): \"\"\"When tokens split, meaning dies\"\"\" # \"$AAPL\" tokenized as [\"$\", \"AA\", \"PL\"] # The embedding becomes: embedding_components = { \"$\": { \"learned_from\": [\"$100\", \"$50\", \"money\", \"currency\", \"price\"], \"semantic_meaning\": \"Money/cost concept\" }, \"AA\": { \"learned_from\": [\"AA batteries\", \"Alcoholics Anonymous\", \"American Airlines\"], \"semantic_meaning\": \"Batteries/support groups/airlines\" }, \"PL\": { \"learned_from\": [\"Poland\", \"PL/SQL\", \".pl domain\"], \"semantic_meaning\": \"Poland/programming/perl\" } } # The final \"$AAPL\" embedding is: # 33% money + 33% batteries/airlines + 33% Poland # NOTHING about Apple Inc! # Meanwhile \"Apple\" embedding is: # 100% technology company / fruit # No wonder similarity is garbage! return \"Your stock ticker has the semantics of Polish batteries\" The Domain-Specific Disaster # Medical/scientific terms getting destroyed: medical_tokenization_disasters = { \"acetaminophen\": [\"acet\", \"amino\", \"phen\"], # Embedding: vinegar + amino acids + phenol # NOT pain reliever! \"COVID-19\": [\"CO\", \"VID\", \"-\", \"19\"], # Embedding: Colorado + video + negative + year # NOT pandemic disease! \"SARS-CoV-2\": [\"SAR\", \"S\", \"-\", \"Co\", \"V\", \"-\", \"2\"], # Embedding: search and rescue + sulfur + cobalt + volt # Complete nonsense! \"methylprednisolone\": [\"methyl\", \"pred\", \"nis\", \"ol\", \"one\"], # Embedding: chemistry + prediction + ? + alcohol + single # Lost all medical meaning! } # Your medical search engine: # Query: \"COVID-19 treatment\" # Returns: \"Colorado video production 2019 tips\" # Because the embeddings are BROKEN Why Fragmented Tokens Create Poor Embeddings def why_fragmentation_destroys_meaning(): \"\"\"Even with attention, fragments can't capture domain meaning\"\"\" # When \"$AAPL\" becomes [\"$\", \"AA\", \"PL\"]: problems = { \"Never learned as unit\": \"Model never saw '$AAPL' together during training\", \"No semantic link\": \"Can't learn these tokens together = Apple stock\", \"Attention can't help\": \"Attention helps context but can't create missing knowledge\", \"Search fails\": \"Looking for 'Apple stock' won't find '$' + 'AA' + 'PL'\" } # The fundamental problem: # The model learned: # \"$\" appears with prices, money, costs # \"AA\" appears with batteries, ratings, airlines # \"PL\" appears with Poland, programming, domains # # It NEVER learned: # \"$AAPL\" = Apple Inc's stock ticker # So when you search for \"Apple stock price\" # The embedding for \"$AAPL\" (as fragments) has NO semantic connection return \"Transformers are powerful, but they can't learn what was never there\" The RAG Pipeline Destruction class RAGDisasterAnalysis: \"\"\"How bad tokenization destroys your entire RAG system\"\"\" def __init__(self, tokenizer, bad_tokenizer): self.good = tokenizer # Domain-specific self.bad = bad_tokenizer # Generic def analyze_retrieval_quality(self, query, documents): \"\"\"Compare retrieval with good vs bad tokenization\"\"\" # Good tokenizer: \"$NVDA\" ‚Üí [\"$NVDA\"] good_tokens = self.good.encode(query) good_embedding = embed_with_tokenizer(query, self.good) good_results = retrieve_similar(good_embedding, documents) # Bad tokenizer: \"$NVDA\" ‚Üí [\"$\", \"N\", \"V\", \"DA\"] bad_tokens = self.bad.encode(query) bad_embedding = embed_with_tokenizer(query, self.bad) bad_results = retrieve_similar(bad_embedding, documents) print(f\"Query: {query}\") print(f\"Good tokenization: {good_tokens}\") print(f\"Top result: {good_results[0]}\") # \"NVIDIA earnings report\" print(f\"Bad tokenization: {bad_tokens}\") print(f\"Top result: {bad_results[0]}\") # \"Nevada state budget\" # Bad tokenization turns NVIDIA into Nevada! return \"Your RAG is only as good as your tokenizer\" The Benchmark Fraud def why_benchmarks_hide_this(): \"\"\"MTEB and other benchmarks use common terms that tokenize well\"\"\" benchmark_terms = [ \"computer\", # Always 1 token \"science\", # Always 1 token \"artificial\", # Usually 1 token \"intelligence\", # Usually 1 token ] real_world_terms = [ \"GPT-4\", # [\"GPT\", \"-\", \"4\"] \"COVID-19\", # [\"CO\", \"VID\", \"-\", \"19\"] \"e-commerce\", # [\"e\", \"-\", \"commerce\"] \"$TSLA\", # [\"$\", \"TS\", \"LA\"] ] print(\"Benchmark terms: Perfect tokenization\") for term in benchmark_terms: print(f\" {term} ‚Üí {len(tokenizer.encode(term))} token\") print(\"\\nReal-world terms: Tokenization disasters\") for term in real_world_terms: tokens = tokenizer.encode(term) print(f\" {term} ‚Üí {len(tokens)} tokens: {[tokenizer.decode([t]) for t in tokens]}\") return \"Models ace benchmarks, fail in production\" The Fix: Domain-Aligned Tokenization class DomainAlignedEmbeddings: \"\"\"How to fix your embeddings before it's too late\"\"\" def __init__(self, domain): self.domain = domain def identify_problem_terms(self, corpus): \"\"\"Find terms that tokenize badly\"\"\" problem_terms = [] for doc in corpus: terms = extract_domain_terms(doc) # Your NER/term extraction for term in terms: tokens = tokenizer.encode(term) # Red flags: if len(tokens) \u003e 3: problem_terms.append({ 'term': term, 'tokens': tokens, 'fragmentation': len(tokens), 'problem': 'Over-fragmented' }) # Check if fragments are meaningful for token in tokens: decoded = tokenizer.decode([token]) if len(decoded) \u003c= 2 and decoded not in [\".\", \"-\", \"_\"]: problem_terms.append({ 'term': term, 'issue': f'Meaningless fragment: {decoded}' }) return problem_terms def fix_tokenization(self, problem_terms): \"\"\"Three strategies to fix broken embeddings\"\"\" # Strategy 1: Add to vocabulary (if you control training) custom_tokens = [term['term'] for term in problem_terms[:1000]] # tokenizer.add_tokens(custom_tokens) # Strategy 2: Pre-compute embeddings for problem terms term_embeddings = {} for term in problem_terms: # Don't use default tokenization # Use character-level or custom embedding term_embeddings[term] = compute_custom_embedding(term) # Strategy 3: Preprocessing substitution substitutions = { \"$AAPL\": \"AAPL_STOCK\", \"COVID-19\": \"COVID_NINETEEN\", \"e-commerce\": \"ecommerce\", } return \"Fixed embeddings through tokenization alignment\" The Cost of Ignoring This The financial and operational costs of bad tokenization compound exponentially through your entire ML pipeline. When your RAG system returns wrong documents, you‚Äôre looking at a 30-50% accuracy drop that teams often try to fix by throwing money at larger models typically needing 3x the compute to compensate for what proper tokenization would have solved for free.\nYour semantic search becomes a user experience nightmare. When searches don‚Äôt return relevant content, users can‚Äôt find the information they need. This translates directly to lost customers, increased support tickets, and engineers spending weeks debugging ‚Äúsearch relevance issues‚Äù that are actually tokenization problems.\nThe clustering and topic modeling failures are equally devastating. When similar concepts don‚Äôt cluster together because their tokenizations differ, your entire data organization breaks down. Teams resort to manual categorization, burning through data science hours on what should be automated.\nPerhaps most expensive is the fine-tuning inefficiency. When your model can‚Äôt learn domain concepts due to fragmented tokens, you need 10x more training data to achieve marginal improvements. That‚Äôs easily $50,000 in additional compute costs, not to mention the opportunity cost of delayed deployments.\nThe cascade is predictable: Bad tokenization ‚Üí Bad embeddings ‚Üí Failed retrieval ‚Üí Larger models ‚Üí Higher costs ‚Üí Still bad results ‚Üí Engineers quit ‚Üí Project fails. And it all started with a tokenizer that couldn‚Äôt handle ‚Äú$AAPL‚Äù properly.\nThe Embedding Quality Test Suite def test_your_embedding_quality(tokenizer, test_pairs): \"\"\"Run this before deploying ANY embedding system\"\"\" critical_tests = [ # Domain terms (\"your_product\", \"YourProduct\"), (\"your-product\", \"your product\"), # Technical terms (\"API_KEY\", \"API KEY\"), (\"OAuth2.0\", \"OAuth 2.0\"), # Financial (\"P/E ratio\", \"PE ratio\"), (\"52-week high\", \"52 week high\"), ] failures = [] for term1, term2 in critical_tests: tokens1 = tokenizer.encode(term1) tokens2 = tokenizer.encode(term2) # These should be similar but tokenize differently if len(tokens1) \u003e 2 or len(tokens2) \u003e 2: similarity = calculate_similarity(term1, term2) if similarity \u003c 0.7: failures.append({ 'pair': (term1, term2), 'similarity': similarity, 'tokens': (tokens1, tokens2) }) if failures: print(\"üö® YOUR EMBEDDINGS WILL FAIL IN PRODUCTION\") return failures return \"Embeddings look good (for now)\" üí° Critical Insight: Your $50K vector database is only as good as your $0 tokenizer. Before you scale compute, buy GPUs, or hire ML engineers, run the embedding quality test. If domain terms fragment into 3+ tokens, your embeddings are noise. Fix tokenization first or everything downstream fails.\nTakeaway: Embeddings aren‚Äôt learned from concepts, they‚Äôre learned from tokens. When ‚Äú$AAPL‚Äù becomes [\"$\", ‚ÄúAA‚Äù, ‚ÄúPL‚Äù], your model learns the embedding of ‚Äúmoney + batteries + Poland‚Äù, not ‚ÄúApple stock‚Äù. Your semantic search, RAG, and clustering all fail because tokenization shattered meaning into nonsense fragments. The fix isn‚Äôt more data or bigger models, it‚Äôs aligning tokenization with your domain FIRST.\n",
  "wordCount" : "1467",
  "inLanguage": "en",
  "datePublished": "2025-09-06T00:10:39+05:30",
  "dateModified": "2025-09-06T00:10:39+05:30",
  "author":{
    "@type": "Person",
    "name": "Lakshay Chhabra"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lucven.com/posts/tokenization/tokenisation_limits/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Lucven AI",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lucven.com/favicon_io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lucven.com/" accesskey="h" title="Lucven AI (Alt + H)">Lucven AI</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://lucven.com/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://lucven.com/">Home</a>&nbsp;¬ª&nbsp;<a href="https://lucven.com/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Why Your Vector Database Thinks $AAPL Means Polish Batteries
    </h1>
    <div class="post-meta"><span title='2025-09-06 00:10:39 +0530 IST'>September 6, 2025</span>&nbsp;¬∑&nbsp;7 min&nbsp;¬∑&nbsp;Lakshay Chhabra

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#why-your-vector-database-thinks-aapl-means-polish-batteries" aria-label="Why Your Vector Database Thinks $AAPL Means Polish Batteries">Why Your Vector Database Thinks $AAPL Means Polish Batteries</a><ul>
                        
                <li>
                    <a href="#the-embedding-murder-scene" aria-label="The Embedding Murder Scene">The Embedding Murder Scene</a></li>
                <li>
                    <a href="#the-semantic-catastrophe" aria-label="The Semantic Catastrophe">The Semantic Catastrophe</a></li>
                <li>
                    <a href="#the-domain-specific-disaster" aria-label="The Domain-Specific Disaster">The Domain-Specific Disaster</a></li>
                <li>
                    <a href="#why-fragmented-tokens-create-poor-embeddings" aria-label="Why Fragmented Tokens Create Poor Embeddings">Why Fragmented Tokens Create Poor Embeddings</a></li>
                <li>
                    <a href="#the-rag-pipeline-destruction" aria-label="The RAG Pipeline Destruction">The RAG Pipeline Destruction</a></li>
                <li>
                    <a href="#the-benchmark-fraud" aria-label="The Benchmark Fraud">The Benchmark Fraud</a></li>
                <li>
                    <a href="#the-fix-domain-aligned-tokenization" aria-label="The Fix: Domain-Aligned Tokenization">The Fix: Domain-Aligned Tokenization</a></li>
                <li>
                    <a href="#the-cost-of-ignoring-this" aria-label="The Cost of Ignoring This">The Cost of Ignoring This</a></li>
                <li>
                    <a href="#the-embedding-quality-test-suite" aria-label="The Embedding Quality Test Suite">The Embedding Quality Test Suite</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="why-your-vector-database-thinks-aapl-means-polish-batteries">Why Your Vector Database Thinks $AAPL Means Polish Batteries<a hidden class="anchor" aria-hidden="true" href="#why-your-vector-database-thinks-aapl-means-polish-batteries">#</a></h1>
<p>Your $50K vector database is returning garbage results because &ldquo;$AAPL&rdquo; tokenizes as [&quot;$&quot;, &ldquo;AA&rdquo;, &ldquo;PL&rdquo;] and now has the embedding of &ldquo;dollar + AA batteries + Poland&rdquo;. Your semantic search for &ldquo;Apple stock&rdquo; returns articles about Polish currency. This isn&rsquo;t a retrieval problem, it&rsquo;s tokenization murdering your embeddings.</p>
<blockquote>
<p><strong>TL;DR</strong>: Bad tokenization creates noisy embeddings. &ldquo;COVID-19&rdquo; split into [&ldquo;CO&rdquo;, &ldquo;VID&rdquo;, &ldquo;-&rdquo;, &ldquo;19&rdquo;] has embeddings mixing &ldquo;Colorado&rdquo;, &ldquo;video&rdquo;, &ldquo;negative&rdquo;, and &ldquo;2019&rdquo;. Your RAG pipeline is doomed before it starts. Fix tokenization or waste money on larger models trying to compensate.</p></blockquote>
<h2 id="the-embedding-murder-scene">The Embedding Murder Scene<a hidden class="anchor" aria-hidden="true" href="#the-embedding-murder-scene">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Watch tokenization destroy semantic similarity:</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">embedding_quality_test</span>(tokenizer, embedding_model):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;See how tokenization ruins your embeddings&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Same concept, different tokenizations</span>
</span></span><span style="display:flex;"><span>    test_pairs <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;Apple Inc&#34;</span>, <span style="color:#e6db74">&#34;$AAPL&#34;</span>),      <span style="color:#75715e"># Company and its ticker</span>
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;COVID-19&#34;</span>, <span style="color:#e6db74">&#34;coronavirus&#34;</span>), <span style="color:#75715e"># Same disease</span>
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;e-commerce&#34;</span>, <span style="color:#e6db74">&#34;ecommerce&#34;</span>),  <span style="color:#75715e"># Same concept, different style</span>
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;Ph.D.&#34;</span>, <span style="color:#e6db74">&#34;PhD&#34;</span>),             <span style="color:#75715e"># Same degree</span>
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;U.S.A.&#34;</span>, <span style="color:#e6db74">&#34;USA&#34;</span>),           <span style="color:#75715e"># Same country</span>
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> term1, term2 <span style="color:#f92672">in</span> test_pairs:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Tokenize</span>
</span></span><span style="display:flex;"><span>        tokens1 <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(term1)
</span></span><span style="display:flex;"><span>        tokens2 <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(term2)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Get embeddings</span>
</span></span><span style="display:flex;"><span>        emb1 <span style="color:#f92672">=</span> embedding_model<span style="color:#f92672">.</span>embed(term1)
</span></span><span style="display:flex;"><span>        emb2 <span style="color:#f92672">=</span> embedding_model<span style="color:#f92672">.</span>embed(term2)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Calculate similarity</span>
</span></span><span style="display:flex;"><span>        similarity <span style="color:#f92672">=</span> cosine_similarity(emb1, emb2)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>term1<span style="color:#e6db74">}</span><span style="color:#e6db74"> ‚Üí </span><span style="color:#e6db74">{</span>[tokenizer<span style="color:#f92672">.</span>decode([t]) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tokens1]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>term2<span style="color:#e6db74">}</span><span style="color:#e6db74"> ‚Üí </span><span style="color:#e6db74">{</span>[tokenizer<span style="color:#f92672">.</span>decode([t]) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tokens2]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Similarity: </span><span style="color:#e6db74">{</span>similarity<span style="color:#e6db74">:</span><span style="color:#e6db74">.3f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> len(tokens1) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">or</span> len(tokens2) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">2</span>:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">&#34;‚ö†Ô∏è FRAGMENTED - Embeddings are NOISE&#34;</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;-&#34;</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">50</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Results:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># &#34;$AAPL&#34; as [&#34;$&#34;, &#34;AA&#34;, &#34;PL&#34;] ‚Üí similarity with &#34;Apple&#34; = 0.15</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># &#34;$AAPL&#34; as [&#34;$AAPL&#34;] ‚Üí similarity with &#34;Apple&#34; = 0.75</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 5X DIFFERENCE just from tokenization!</span>
</span></span></code></pre></div><h2 id="the-semantic-catastrophe">The Semantic Catastrophe<a hidden class="anchor" aria-hidden="true" href="#the-semantic-catastrophe">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># How fragmented tokens create nonsense embeddings:</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fragmentation_semantic_disaster</span>():
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;When tokens split, meaning dies&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># &#34;$AAPL&#34; tokenized as [&#34;$&#34;, &#34;AA&#34;, &#34;PL&#34;]</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># The embedding becomes:</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    embedding_components <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;$&#34;</span>: {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;learned_from&#34;</span>: [<span style="color:#e6db74">&#34;$100&#34;</span>, <span style="color:#e6db74">&#34;$50&#34;</span>, <span style="color:#e6db74">&#34;money&#34;</span>, <span style="color:#e6db74">&#34;currency&#34;</span>, <span style="color:#e6db74">&#34;price&#34;</span>],
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;semantic_meaning&#34;</span>: <span style="color:#e6db74">&#34;Money/cost concept&#34;</span>
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;AA&#34;</span>: {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;learned_from&#34;</span>: [<span style="color:#e6db74">&#34;AA batteries&#34;</span>, <span style="color:#e6db74">&#34;Alcoholics Anonymous&#34;</span>, <span style="color:#e6db74">&#34;American Airlines&#34;</span>],
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;semantic_meaning&#34;</span>: <span style="color:#e6db74">&#34;Batteries/support groups/airlines&#34;</span>
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;PL&#34;</span>: {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;learned_from&#34;</span>: [<span style="color:#e6db74">&#34;Poland&#34;</span>, <span style="color:#e6db74">&#34;PL/SQL&#34;</span>, <span style="color:#e6db74">&#34;.pl domain&#34;</span>],
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;semantic_meaning&#34;</span>: <span style="color:#e6db74">&#34;Poland/programming/perl&#34;</span>
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># The final &#34;$AAPL&#34; embedding is:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 33% money + 33% batteries/airlines + 33% Poland</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># NOTHING about Apple Inc!</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Meanwhile &#34;Apple&#34; embedding is:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 100% technology company / fruit</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># No wonder similarity is garbage!</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;Your stock ticker has the semantics of Polish batteries&#34;</span>
</span></span></code></pre></div><h2 id="the-domain-specific-disaster">The Domain-Specific Disaster<a hidden class="anchor" aria-hidden="true" href="#the-domain-specific-disaster">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Medical/scientific terms getting destroyed:</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>medical_tokenization_disasters <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;acetaminophen&#34;</span>: [<span style="color:#e6db74">&#34;acet&#34;</span>, <span style="color:#e6db74">&#34;amino&#34;</span>, <span style="color:#e6db74">&#34;phen&#34;</span>],
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Embedding: vinegar + amino acids + phenol</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># NOT pain reliever!</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;COVID-19&#34;</span>: [<span style="color:#e6db74">&#34;CO&#34;</span>, <span style="color:#e6db74">&#34;VID&#34;</span>, <span style="color:#e6db74">&#34;-&#34;</span>, <span style="color:#e6db74">&#34;19&#34;</span>],
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Embedding: Colorado + video + negative + year</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># NOT pandemic disease!</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;SARS-CoV-2&#34;</span>: [<span style="color:#e6db74">&#34;SAR&#34;</span>, <span style="color:#e6db74">&#34;S&#34;</span>, <span style="color:#e6db74">&#34;-&#34;</span>, <span style="color:#e6db74">&#34;Co&#34;</span>, <span style="color:#e6db74">&#34;V&#34;</span>, <span style="color:#e6db74">&#34;-&#34;</span>, <span style="color:#e6db74">&#34;2&#34;</span>],
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Embedding: search and rescue + sulfur + cobalt + volt</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Complete nonsense!</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;methylprednisolone&#34;</span>: [<span style="color:#e6db74">&#34;methyl&#34;</span>, <span style="color:#e6db74">&#34;pred&#34;</span>, <span style="color:#e6db74">&#34;nis&#34;</span>, <span style="color:#e6db74">&#34;ol&#34;</span>, <span style="color:#e6db74">&#34;one&#34;</span>],
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Embedding: chemistry + prediction + ? + alcohol + single</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Lost all medical meaning!</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Your medical search engine:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Query: &#34;COVID-19 treatment&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Returns: &#34;Colorado video production 2019 tips&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Because the embeddings are BROKEN</span>
</span></span></code></pre></div><h2 id="why-fragmented-tokens-create-poor-embeddings">Why Fragmented Tokens Create Poor Embeddings<a hidden class="anchor" aria-hidden="true" href="#why-fragmented-tokens-create-poor-embeddings">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">why_fragmentation_destroys_meaning</span>():
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Even with attention, fragments can&#39;t capture domain meaning&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># When &#34;$AAPL&#34; becomes [&#34;$&#34;, &#34;AA&#34;, &#34;PL&#34;]:</span>
</span></span><span style="display:flex;"><span>    problems <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Never learned as unit&#34;</span>: <span style="color:#e6db74">&#34;Model never saw &#39;$AAPL&#39; together during training&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;No semantic link&#34;</span>: <span style="color:#e6db74">&#34;Can&#39;t learn these tokens together = Apple stock&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Attention can&#39;t help&#34;</span>: <span style="color:#e6db74">&#34;Attention helps context but can&#39;t create missing knowledge&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Search fails&#34;</span>: <span style="color:#e6db74">&#34;Looking for &#39;Apple stock&#39; won&#39;t find &#39;$&#39; + &#39;AA&#39; + &#39;PL&#39;&#34;</span>
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># The fundamental problem:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># The model learned:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#   &#34;$&#34; appears with prices, money, costs</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#   &#34;AA&#34; appears with batteries, ratings, airlines</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#   &#34;PL&#34; appears with Poland, programming, domains</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># </span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># It NEVER learned:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#   &#34;$AAPL&#34; = Apple Inc&#39;s stock ticker</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># So when you search for &#34;Apple stock price&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># The embedding for &#34;$AAPL&#34; (as fragments) has NO semantic connection</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;Transformers are powerful, but they can&#39;t learn what was never there&#34;</span>
</span></span></code></pre></div><h2 id="the-rag-pipeline-destruction">The RAG Pipeline Destruction<a hidden class="anchor" aria-hidden="true" href="#the-rag-pipeline-destruction">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">RAGDisasterAnalysis</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;How bad tokenization destroys your entire RAG system&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, tokenizer, bad_tokenizer):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>good <span style="color:#f92672">=</span> tokenizer      <span style="color:#75715e"># Domain-specific</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bad <span style="color:#f92672">=</span> bad_tokenizer   <span style="color:#75715e"># Generic</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">analyze_retrieval_quality</span>(self, query, documents):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Compare retrieval with good vs bad tokenization&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Good tokenizer: &#34;$NVDA&#34; ‚Üí [&#34;$NVDA&#34;]</span>
</span></span><span style="display:flex;"><span>        good_tokens <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>good<span style="color:#f92672">.</span>encode(query)
</span></span><span style="display:flex;"><span>        good_embedding <span style="color:#f92672">=</span> embed_with_tokenizer(query, self<span style="color:#f92672">.</span>good)
</span></span><span style="display:flex;"><span>        good_results <span style="color:#f92672">=</span> retrieve_similar(good_embedding, documents)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Bad tokenizer: &#34;$NVDA&#34; ‚Üí [&#34;$&#34;, &#34;N&#34;, &#34;V&#34;, &#34;DA&#34;]</span>
</span></span><span style="display:flex;"><span>        bad_tokens <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bad<span style="color:#f92672">.</span>encode(query)
</span></span><span style="display:flex;"><span>        bad_embedding <span style="color:#f92672">=</span> embed_with_tokenizer(query, self<span style="color:#f92672">.</span>bad)
</span></span><span style="display:flex;"><span>        bad_results <span style="color:#f92672">=</span> retrieve_similar(bad_embedding, documents)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Query: </span><span style="color:#e6db74">{</span>query<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Good tokenization: </span><span style="color:#e6db74">{</span>good_tokens<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Top result: </span><span style="color:#e6db74">{</span>good_results[<span style="color:#ae81ff">0</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)  <span style="color:#75715e"># &#34;NVIDIA earnings report&#34;</span>
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Bad tokenization: </span><span style="color:#e6db74">{</span>bad_tokens<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Top result: </span><span style="color:#e6db74">{</span>bad_results[<span style="color:#ae81ff">0</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)   <span style="color:#75715e"># &#34;Nevada state budget&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Bad tokenization turns NVIDIA into Nevada!</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;Your RAG is only as good as your tokenizer&#34;</span>
</span></span></code></pre></div><h2 id="the-benchmark-fraud">The Benchmark Fraud<a hidden class="anchor" aria-hidden="true" href="#the-benchmark-fraud">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">why_benchmarks_hide_this</span>():
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;MTEB and other benchmarks use common terms that tokenize well&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    benchmark_terms <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;computer&#34;</span>,      <span style="color:#75715e"># Always 1 token</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;science&#34;</span>,       <span style="color:#75715e"># Always 1 token</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;artificial&#34;</span>,    <span style="color:#75715e"># Usually 1 token</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;intelligence&#34;</span>,  <span style="color:#75715e"># Usually 1 token</span>
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    real_world_terms <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;GPT-4&#34;</span>,         <span style="color:#75715e"># [&#34;GPT&#34;, &#34;-&#34;, &#34;4&#34;] </span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;COVID-19&#34;</span>,      <span style="color:#75715e"># [&#34;CO&#34;, &#34;VID&#34;, &#34;-&#34;, &#34;19&#34;]</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;e-commerce&#34;</span>,    <span style="color:#75715e"># [&#34;e&#34;, &#34;-&#34;, &#34;commerce&#34;]</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;$TSLA&#34;</span>,         <span style="color:#75715e"># [&#34;$&#34;, &#34;TS&#34;, &#34;LA&#34;]</span>
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Benchmark terms: Perfect tokenization&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> term <span style="color:#f92672">in</span> benchmark_terms:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;  </span><span style="color:#e6db74">{</span>term<span style="color:#e6db74">}</span><span style="color:#e6db74"> ‚Üí </span><span style="color:#e6db74">{</span>len(tokenizer<span style="color:#f92672">.</span>encode(term))<span style="color:#e6db74">}</span><span style="color:#e6db74"> token&#34;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Real-world terms: Tokenization disasters&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> term <span style="color:#f92672">in</span> real_world_terms:
</span></span><span style="display:flex;"><span>        tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(term)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;  </span><span style="color:#e6db74">{</span>term<span style="color:#e6db74">}</span><span style="color:#e6db74"> ‚Üí </span><span style="color:#e6db74">{</span>len(tokens)<span style="color:#e6db74">}</span><span style="color:#e6db74"> tokens: </span><span style="color:#e6db74">{</span>[tokenizer<span style="color:#f92672">.</span>decode([t]) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tokens]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;Models ace benchmarks, fail in production&#34;</span>
</span></span></code></pre></div><h2 id="the-fix-domain-aligned-tokenization">The Fix: Domain-Aligned Tokenization<a hidden class="anchor" aria-hidden="true" href="#the-fix-domain-aligned-tokenization">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DomainAlignedEmbeddings</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;How to fix your embeddings before it&#39;s too late&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, domain):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>domain <span style="color:#f92672">=</span> domain
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">identify_problem_terms</span>(self, corpus):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Find terms that tokenize badly&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        problem_terms <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> doc <span style="color:#f92672">in</span> corpus:
</span></span><span style="display:flex;"><span>            terms <span style="color:#f92672">=</span> extract_domain_terms(doc)  <span style="color:#75715e"># Your NER/term extraction</span>
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> term <span style="color:#f92672">in</span> terms:
</span></span><span style="display:flex;"><span>                tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(term)
</span></span><span style="display:flex;"><span>                
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># Red flags:</span>
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">if</span> len(tokens) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">3</span>:
</span></span><span style="display:flex;"><span>                    problem_terms<span style="color:#f92672">.</span>append({
</span></span><span style="display:flex;"><span>                        <span style="color:#e6db74">&#39;term&#39;</span>: term,
</span></span><span style="display:flex;"><span>                        <span style="color:#e6db74">&#39;tokens&#39;</span>: tokens,
</span></span><span style="display:flex;"><span>                        <span style="color:#e6db74">&#39;fragmentation&#39;</span>: len(tokens),
</span></span><span style="display:flex;"><span>                        <span style="color:#e6db74">&#39;problem&#39;</span>: <span style="color:#e6db74">&#39;Over-fragmented&#39;</span>
</span></span><span style="display:flex;"><span>                    })
</span></span><span style="display:flex;"><span>                
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># Check if fragments are meaningful</span>
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> tokens:
</span></span><span style="display:flex;"><span>                    decoded <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>decode([token])
</span></span><span style="display:flex;"><span>                    <span style="color:#66d9ef">if</span> len(decoded) <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">and</span> decoded <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> [<span style="color:#e6db74">&#34;.&#34;</span>, <span style="color:#e6db74">&#34;-&#34;</span>, <span style="color:#e6db74">&#34;_&#34;</span>]:
</span></span><span style="display:flex;"><span>                        problem_terms<span style="color:#f92672">.</span>append({
</span></span><span style="display:flex;"><span>                            <span style="color:#e6db74">&#39;term&#39;</span>: term,
</span></span><span style="display:flex;"><span>                            <span style="color:#e6db74">&#39;issue&#39;</span>: <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;Meaningless fragment: </span><span style="color:#e6db74">{</span>decoded<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>
</span></span><span style="display:flex;"><span>                        })
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> problem_terms
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fix_tokenization</span>(self, problem_terms):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Three strategies to fix broken embeddings&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Strategy 1: Add to vocabulary (if you control training)</span>
</span></span><span style="display:flex;"><span>        custom_tokens <span style="color:#f92672">=</span> [term[<span style="color:#e6db74">&#39;term&#39;</span>] <span style="color:#66d9ef">for</span> term <span style="color:#f92672">in</span> problem_terms[:<span style="color:#ae81ff">1000</span>]]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># tokenizer.add_tokens(custom_tokens)</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Strategy 2: Pre-compute embeddings for problem terms</span>
</span></span><span style="display:flex;"><span>        term_embeddings <span style="color:#f92672">=</span> {}
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> term <span style="color:#f92672">in</span> problem_terms:
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Don&#39;t use default tokenization</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Use character-level or custom embedding</span>
</span></span><span style="display:flex;"><span>            term_embeddings[term] <span style="color:#f92672">=</span> compute_custom_embedding(term)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Strategy 3: Preprocessing substitution</span>
</span></span><span style="display:flex;"><span>        substitutions <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;$AAPL&#34;</span>: <span style="color:#e6db74">&#34;AAPL_STOCK&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;COVID-19&#34;</span>: <span style="color:#e6db74">&#34;COVID_NINETEEN&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;e-commerce&#34;</span>: <span style="color:#e6db74">&#34;ecommerce&#34;</span>,
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;Fixed embeddings through tokenization alignment&#34;</span>
</span></span></code></pre></div><h2 id="the-cost-of-ignoring-this">The Cost of Ignoring This<a hidden class="anchor" aria-hidden="true" href="#the-cost-of-ignoring-this">#</a></h2>
<p>The financial and operational costs of bad tokenization compound exponentially through your entire ML pipeline. When your RAG system returns wrong documents, you&rsquo;re looking at a 30-50% accuracy drop that teams often try to fix by throwing money at larger models typically needing 3x the compute to compensate for what proper tokenization would have solved for free.</p>
<p>Your semantic search becomes a user experience nightmare. When searches don&rsquo;t return relevant content, users can&rsquo;t find the information they need. This translates directly to lost customers, increased support tickets, and engineers spending weeks debugging &ldquo;search relevance issues&rdquo; that are actually tokenization problems.</p>
<p>The clustering and topic modeling failures are equally devastating. When similar concepts don&rsquo;t cluster together because their tokenizations differ, your entire data organization breaks down. Teams resort to manual categorization, burning through data science hours on what should be automated.</p>
<p>Perhaps most expensive is the fine-tuning inefficiency. When your model can&rsquo;t learn domain concepts due to fragmented tokens, you need 10x more training data to achieve marginal improvements. That&rsquo;s easily $50,000 in additional compute costs, not to mention the opportunity cost of delayed deployments.</p>
<p>The cascade is predictable: Bad tokenization ‚Üí Bad embeddings ‚Üí Failed retrieval ‚Üí Larger models ‚Üí Higher costs ‚Üí Still bad results ‚Üí Engineers quit ‚Üí Project fails. And it all started with a tokenizer that couldn&rsquo;t handle &ldquo;$AAPL&rdquo; properly.</p>
<h2 id="the-embedding-quality-test-suite">The Embedding Quality Test Suite<a hidden class="anchor" aria-hidden="true" href="#the-embedding-quality-test-suite">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test_your_embedding_quality</span>(tokenizer, test_pairs):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Run this before deploying ANY embedding system&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    critical_tests <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Domain terms</span>
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;your_product&#34;</span>, <span style="color:#e6db74">&#34;YourProduct&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;your-product&#34;</span>, <span style="color:#e6db74">&#34;your product&#34;</span>),
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Technical terms</span>
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;API_KEY&#34;</span>, <span style="color:#e6db74">&#34;API KEY&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;OAuth2.0&#34;</span>, <span style="color:#e6db74">&#34;OAuth 2.0&#34;</span>),
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Financial</span>
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;P/E ratio&#34;</span>, <span style="color:#e6db74">&#34;PE ratio&#34;</span>),
</span></span><span style="display:flex;"><span>        (<span style="color:#e6db74">&#34;52-week high&#34;</span>, <span style="color:#e6db74">&#34;52 week high&#34;</span>),
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    failures <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> term1, term2 <span style="color:#f92672">in</span> critical_tests:
</span></span><span style="display:flex;"><span>        tokens1 <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(term1)
</span></span><span style="display:flex;"><span>        tokens2 <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(term2)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># These should be similar but tokenize differently</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> len(tokens1) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">or</span> len(tokens2) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">2</span>:
</span></span><span style="display:flex;"><span>            similarity <span style="color:#f92672">=</span> calculate_similarity(term1, term2)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> similarity <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">0.7</span>:
</span></span><span style="display:flex;"><span>                failures<span style="color:#f92672">.</span>append({
</span></span><span style="display:flex;"><span>                    <span style="color:#e6db74">&#39;pair&#39;</span>: (term1, term2),
</span></span><span style="display:flex;"><span>                    <span style="color:#e6db74">&#39;similarity&#39;</span>: similarity,
</span></span><span style="display:flex;"><span>                    <span style="color:#e6db74">&#39;tokens&#39;</span>: (tokens1, tokens2)
</span></span><span style="display:flex;"><span>                })
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> failures:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;üö® YOUR EMBEDDINGS WILL FAIL IN PRODUCTION&#34;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> failures
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;Embeddings look good (for now)&#34;</span>
</span></span></code></pre></div><hr>
<blockquote>
<p><strong>üí° Critical Insight</strong>: Your $50K vector database is only as good as your $0 tokenizer. Before you scale compute, buy GPUs, or hire ML engineers, run the embedding quality test. If domain terms fragment into 3+ tokens, your embeddings are noise. Fix tokenization first or everything downstream fails.</p></blockquote>
<hr>
<p><strong>Takeaway:</strong> Embeddings aren&rsquo;t learned from concepts, they&rsquo;re learned from tokens. When &ldquo;$AAPL&rdquo; becomes [&quot;$&quot;, &ldquo;AA&rdquo;, &ldquo;PL&rdquo;], your model learns the embedding of &ldquo;money + batteries + Poland&rdquo;, not &ldquo;Apple stock&rdquo;. Your semantic search, RAG, and clustering all fail because tokenization shattered meaning into nonsense fragments. The fix isn&rsquo;t more data or bigger models, it&rsquo;s aligning tokenization with your domain FIRST.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lucven.com/tags/tokenisation/">Tokenisation</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://lucven.com/posts/tokenization/tokenization_math/">
    <span class="title">¬´ Prev</span>
    <br>
    <span>How Tokenization Murders Your Model&#39;s Ability to Do Basic Math</span>
  </a>
  <a class="next" href="https://lucven.com/posts/tokenization/forensics/">
    <span class="title">Next ¬ª</span>
    <br>
    <span>Tokenization Forensics about Leaks</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Why Your Vector Database Thinks $AAPL Means Polish Batteries on x"
            href="https://x.com/intent/tweet/?text=Why%20Your%20Vector%20Database%20Thinks%20%24AAPL%20Means%20Polish%20Batteries&amp;url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2ftokenisation_limits%2f&amp;hashtags=Tokenisation">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Why Your Vector Database Thinks $AAPL Means Polish Batteries on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2ftokenisation_limits%2f&amp;title=Why%20Your%20Vector%20Database%20Thinks%20%24AAPL%20Means%20Polish%20Batteries&amp;summary=Why%20Your%20Vector%20Database%20Thinks%20%24AAPL%20Means%20Polish%20Batteries&amp;source=https%3a%2f%2flucven.com%2fposts%2ftokenization%2ftokenisation_limits%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Why Your Vector Database Thinks $AAPL Means Polish Batteries on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2ftokenisation_limits%2f&title=Why%20Your%20Vector%20Database%20Thinks%20%24AAPL%20Means%20Polish%20Batteries">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Why Your Vector Database Thinks $AAPL Means Polish Batteries on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flucven.com%2fposts%2ftokenization%2ftokenisation_limits%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Why Your Vector Database Thinks $AAPL Means Polish Batteries on whatsapp"
            href="https://api.whatsapp.com/send?text=Why%20Your%20Vector%20Database%20Thinks%20%24AAPL%20Means%20Polish%20Batteries%20-%20https%3a%2f%2flucven.com%2fposts%2ftokenization%2ftokenisation_limits%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Why Your Vector Database Thinks $AAPL Means Polish Batteries on telegram"
            href="https://telegram.me/share/url?text=Why%20Your%20Vector%20Database%20Thinks%20%24AAPL%20Means%20Polish%20Batteries&amp;url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2ftokenisation_limits%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Why Your Vector Database Thinks $AAPL Means Polish Batteries on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Why%20Your%20Vector%20Database%20Thinks%20%24AAPL%20Means%20Polish%20Batteries&u=https%3a%2f%2flucven.com%2fposts%2ftokenization%2ftokenisation_limits%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer><script src="https://giscus.app/client.js"
  data-repo="lakshaychhabra/lucven-comments"
  data-repo-id="R_kgDOPYOEYw"
  data-category="Q&A"
  data-category-id="DIC_kwDOPYOEY84CtxWt"
  data-mapping="pathname"
  data-strict="0"
  data-reactions-enabled="1"
  data-emit-metadata="0"
  data-input-position="top"
  data-theme="preferred_color_scheme"
  data-lang="en"
  data-loading="lazy"
  crossorigin="anonymous"
  async>
</script>


</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://lucven.com/">Lucven AI</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
