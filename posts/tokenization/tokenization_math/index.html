<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>How Tokenization Murders Your Model&#39;s Ability to Do Basic Math | Lucven AI</title>
<meta name="keywords" content="Tokenisation">
<meta name="description" content="How Tokenization Murders Your Model&rsquo;s Ability to Do Basic Math
GPT-4o can write Shakespeare but struggles with 4-digit multiplication. It&rsquo;s not stupid, it literally can&rsquo;t see numbers the way you do. &ldquo;12345&rdquo; might be [&ldquo;123&rdquo;, &ldquo;45&rdquo;] while &ldquo;12346&rdquo; is [&ldquo;1&rdquo;, &ldquo;2346&rdquo;]. Try doing math when numbers randomly shatter into chunks.

TL;DR: Tokenizers split numbers inconsistently, making arithmetic nearly impossible. &ldquo;9.11&rdquo; &gt; &ldquo;9.9&rdquo; according to many models because &ldquo;.11&rdquo; and &ldquo;.9&rdquo; are different tokens. Your calculator app works. Your $100B language model doesn&rsquo;t. This is why.">
<meta name="author" content="Lakshay Chhabra">
<link rel="canonical" href="https://lucven.com/posts/tokenization/tokenization_math/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://lucven.com/favicon_io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lucven.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lucven.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lucven.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://lucven.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://lucven.com/posts/tokenization/tokenization_math/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:url" content="https://lucven.com/posts/tokenization/tokenization_math/">
  <meta property="og:site_name" content="Lucven AI">
  <meta property="og:title" content="How Tokenization Murders Your Model&#39;s Ability to Do Basic Math">
  <meta property="og:description" content="How Tokenization Murders Your Model‚Äôs Ability to Do Basic Math GPT-4o can write Shakespeare but struggles with 4-digit multiplication. It‚Äôs not stupid, it literally can‚Äôt see numbers the way you do. ‚Äú12345‚Äù might be [‚Äú123‚Äù, ‚Äú45‚Äù] while ‚Äú12346‚Äù is [‚Äú1‚Äù, ‚Äú2346‚Äù]. Try doing math when numbers randomly shatter into chunks.
TL;DR: Tokenizers split numbers inconsistently, making arithmetic nearly impossible. ‚Äú9.11‚Äù &gt; ‚Äú9.9‚Äù according to many models because ‚Äú.11‚Äù and ‚Äú.9‚Äù are different tokens. Your calculator app works. Your $100B language model doesn‚Äôt. This is why.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-09-07T00:10:39+05:30">
    <meta property="article:modified_time" content="2025-09-07T00:10:39+05:30">
    <meta property="article:tag" content="Tokenisation">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="How Tokenization Murders Your Model&#39;s Ability to Do Basic Math">
<meta name="twitter:description" content="How Tokenization Murders Your Model&rsquo;s Ability to Do Basic Math
GPT-4o can write Shakespeare but struggles with 4-digit multiplication. It&rsquo;s not stupid, it literally can&rsquo;t see numbers the way you do. &ldquo;12345&rdquo; might be [&ldquo;123&rdquo;, &ldquo;45&rdquo;] while &ldquo;12346&rdquo; is [&ldquo;1&rdquo;, &ldquo;2346&rdquo;]. Try doing math when numbers randomly shatter into chunks.

TL;DR: Tokenizers split numbers inconsistently, making arithmetic nearly impossible. &ldquo;9.11&rdquo; &gt; &ldquo;9.9&rdquo; according to many models because &ldquo;.11&rdquo; and &ldquo;.9&rdquo; are different tokens. Your calculator app works. Your $100B language model doesn&rsquo;t. This is why.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://lucven.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "How Tokenization Murders Your Model's Ability to Do Basic Math",
      "item": "https://lucven.com/posts/tokenization/tokenization_math/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "How Tokenization Murders Your Model's Ability to Do Basic Math",
  "name": "How Tokenization Murders Your Model\u0027s Ability to Do Basic Math",
  "description": "How Tokenization Murders Your Model\u0026rsquo;s Ability to Do Basic Math GPT-4o can write Shakespeare but struggles with 4-digit multiplication. It\u0026rsquo;s not stupid, it literally can\u0026rsquo;t see numbers the way you do. \u0026ldquo;12345\u0026rdquo; might be [\u0026ldquo;123\u0026rdquo;, \u0026ldquo;45\u0026rdquo;] while \u0026ldquo;12346\u0026rdquo; is [\u0026ldquo;1\u0026rdquo;, \u0026ldquo;2346\u0026rdquo;]. Try doing math when numbers randomly shatter into chunks.\nTL;DR: Tokenizers split numbers inconsistently, making arithmetic nearly impossible. \u0026ldquo;9.11\u0026rdquo; \u0026gt; \u0026ldquo;9.9\u0026rdquo; according to many models because \u0026ldquo;.11\u0026rdquo; and \u0026ldquo;.9\u0026rdquo; are different tokens. Your calculator app works. Your $100B language model doesn\u0026rsquo;t. This is why.\n",
  "keywords": [
    "Tokenisation"
  ],
  "articleBody": "How Tokenization Murders Your Model‚Äôs Ability to Do Basic Math GPT-4o can write Shakespeare but struggles with 4-digit multiplication. It‚Äôs not stupid, it literally can‚Äôt see numbers the way you do. ‚Äú12345‚Äù might be [‚Äú123‚Äù, ‚Äú45‚Äù] while ‚Äú12346‚Äù is [‚Äú1‚Äù, ‚Äú2346‚Äù]. Try doing math when numbers randomly shatter into chunks.\nTL;DR: Tokenizers split numbers inconsistently, making arithmetic nearly impossible. ‚Äú9.11‚Äù \u003e ‚Äú9.9‚Äù according to many models because ‚Äú.11‚Äù and ‚Äú.9‚Äù are different tokens. Your calculator app works. Your $100B language model doesn‚Äôt. This is why.\nThe Crime Scene: Test This Right Now # The murder weapon: inconsistent number tokenization def number_tokenization_horror(tokenizer): \"\"\"Watch tokenization destroy math ability\"\"\" numbers = [ \"1\", \"12\", \"123\", \"1234\", \"12345\", \"123456\", \"42\", \"420\", \"4200\", \"42000\", \"9.9\", \"9.11\", \"9.111\", \"2023\", \"2024\", \"2025\", \"1000000\", \"1,000,000\", \"1e6\" ] print(\"Number ‚Üí Tokens (How the model 'sees' it)\") print(\"-\" * 50) for num in numbers: tokens = tokenizer.encode(num) decoded = [tokenizer.decode([t]) for t in tokens] # The horror reveal if len(decoded) \u003e 1: print(f\"{num:10} ‚Üí {decoded} üî™ (MURDERED)\") else: print(f\"{num:10} ‚Üí {decoded}\") return \"Your model can't do math because it can't even see numbers\" # Run this and watch the chaos The ‚Äú9.11 \u003e 9.9‚Äù Disaster (Yes, Really) # This actually happens in production: comparisons = [ (\"9.9\", \"9.11\"), # 9.11 is SMALLER but models think it's bigger (\"2.8\", \"2.80\"), # Same number, different tokens (\"1000\", \"1,000\"), # Same number, different tokens (\"3.14\", \"3.141\"), # œÄ gets progressively worse ] for a, b in comparisons: tokens_a = tokenizer.encode(a) tokens_b = tokenizer.encode(b) print(f\"{a} ‚Üí {tokens_a}\") print(f\"{b} ‚Üí {tokens_b}\") # Models compare TOKEN VALUES, not numerical values # \"11\" \u003e \"9\" as a token, so 9.11 \u003e 9.9 # This is why your chatbot says 9.11 is bigger than 9.9 # THE KILLER: Version numbers # \"Python 3.9\" vs \"Python 3.11\" # Models think 3.11 \u003c 3.9 because \".11\" \u003c \".9\" as text Why Your Model Can‚Äôt Count # The counting disaster: def why_counting_fails(tokenizer): \"\"\"Models can't count because they can't see sequences\"\"\" # Try to count from 1 to 20 sequence = \"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\" tokens = tokenizer.encode(sequence) decoded = [tokenizer.decode([t]) for t in tokens] print(\"You see: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\") print(f\"Model sees: {decoded}\") # Reality: ['1', ' 2', ' 3', ..., ' 10', ' 11', ' 12', ' 13', ' 14', ' 15', ' 16', ' 17', ' 18', ' 19', ' 20'] # Some are single tokens, some are split # Patterns are DESTROYED # Now try counting by 10s: by_tens = \"10 20 30 40 50 60 70 80 90 100\" tokens = tokenizer.encode(by_tens) decoded = [tokenizer.decode([t]) for t in tokens] print(f\"Counting by 10s: {decoded}\") # \"10\" might be 1 token, \"20\" might be 1 token, but \"30\" might be [\"3\", \"0\"] # No wonder it can't learn the pattern! # This is why models fail at: # - \"Continue the sequence: 2, 4, 6, 8, ...\" # - \"What comes after 99?\" # - \"Count backwards from 10\" The Arithmetic Apocalypse # Why GPT can't do basic math: def arithmetic_tokenization_study(tokenizer): \"\"\"See why 2+2 works but 1234+5678 doesn't\"\"\" problems = [ \"2+2\", # Perfect: [\"2\", \"+\", \"2\"] \"10+10\", # Still OK: [\"10\", \"+\", \"10\"] \"123+456\", # Getting bad: [\"123\", \"+\", \"456\"] or [\"12\", \"3\", \"+\", \"45\", \"6\"] \"1234+5678\", # Disaster: [\"123\", \"4\", \"+\", \"567\", \"8\"] \"12345+67890\", # Apocalypse: [\"1\", \"234\", \"5\", \"+\", \"678\", \"90\"] ] for problem in problems: tokens = tokenizer.encode(problem) decoded = [tokenizer.decode([t]) for t in tokens] # Calculate token chaos score expected_tokens = 3 # number, operator, number chaos_score = len(tokens) - expected_tokens if chaos_score == 0: status = \"‚úÖ Can solve\" elif chaos_score \u003c= 2: status = \"‚ö†Ô∏è Might solve\" else: status = \"üíÄ Will fail\" print(f\"{problem:15} ‚Üí {decoded:30} {status}\") return \"This is why calculators still exist\" # The bitter truth: # o can write poetry about quantum physics # But fails at 4-digit multiplication # Because it literally cannot see \"1234\" as 1234 The Decimal Disaster # Decimals are even worse: decimal_nightmares = { \"0.1\": [\"0\", \".\", \"1\"], # 3 tokens for a simple decimal \"0.01\": [\"0\", \".\", \"01\"], # Inconsistent! \"0.001\": [\"0\", \".\", \"001\"], # Getting worse \"3.14159\": [\"3\", \".\", \"14\", \"159\"], # œÄ is shattered \"2.718\": [\"2\", \".\", \"7\", \"18\"], # e is broken \"$19.99\": [\"$\", \"19\", \".\", \"99\"], # Prices are chaos \"0.999...\": [\"0\", \".\", \"999\", \"...\"], # Math notation destroyed } # This is why: # - Models think 0.9 \u003e 0.11 (string comparison) # - Can't properly handle financial calculations # - Fail at scientific notation # - Think $19.99 and $20.00 are fundamentally different concepts The Date/Time Tokenization Massacre def datetime_tokenization_chaos(tokenizer): \"\"\"Why models are terrible with dates and times\"\"\" dates = [ \"2023\", # Likely 1 token (common year) \"2024\", # Might be 2 tokens \"2025\", # Probably 2 tokens \"1999\", # 1-2 tokens (Y2K made it common) \"2000\", # 1 token (millennium) \"1823\", # 2-3 tokens (random year) \"2024-01-01\", # 5-7 tokens \"12/25/2023\", # 6-8 tokens \"3:14 PM\", # 4-5 tokens \"15:30:45\", # 5-7 tokens ] for date in dates: tokens = tokenizer.encode(date) if len(tokens) == 1: print(f\"{date} ‚Üí MEMORIZED (seen thousands of times)\") else: decoded = [tokenizer.decode([t]) for t in tokens] print(f\"{date} ‚Üí {decoded} (fragmented perception)\") # This explains why models: # - Can't calculate date differences # - Fail at \"what day is 30 days from today?\" # - Think 12/25 comes before 12/3 (string order) # - Can't handle timezone conversions The Phone Number Privacy Leak # Some phone numbers are single tokens (!!!) def phone_number_investigation(tokenizer): \"\"\"Some numbers are suspiciously well-tokenized\"\"\" numbers = [ \"911\", # Emergency (1 token) \"1-800-273-8255\", # Suicide hotline (might be few tokens) \"867-5309\", # Jenny's number (cultural reference) \"(555) 555-5555\", # Movie/TV placeholder \"+1234567890\", # Random number ] for number in numbers: tokens = len(tokenizer.encode(number)) if tokens \u003c= 3: print(f\"üö® {number} is {tokens} tokens - MEMORIZED IN TRAINING\") else: print(f\"{number} is {tokens} tokens\") # If a phone number is \u003c5 tokens, it appeared in training data # This is a privacy nightmare The Solution No One Wants to Hear class MathTokenizationWorkaround: \"\"\"How to make models not suck at math\"\"\" def fix_arithmetic(self, expression): \"\"\"Pre-tokenize numbers properly\"\"\" # Step 1: Space out everything # \"1234+5678\" ‚Üí \"1 2 3 4 + 5 6 7 8\" spaced = ' '.join(expression) # Step 2: Use chain-of-thought prompt = f\"\"\" Solve step by step: {expression} First, identify the numbers: - First number: {expression.split('+')[0]} - Second number: {expression.split('+')[1]} Now add digit by digit... \"\"\" # Step 3: Or just give up and use a calculator import re if re.match(r'^[\\d\\+\\-\\*/\\.\\s]+$', expression): result = eval(expression) # Don't do this in production! return f\"The answer is {result}\" return \"This is why we still need calculators\" def fix_comparison(self, num1, num2): \"\"\"Fix number comparison\"\"\" # Convert to actual numbers first prompt = f\"\"\" Compare these as decimal numbers: A = {num1} (decimal value: {float(num1)}) B = {num2} (decimal value: {float(num2)}) Therefore {float(num1)} {'\u003e' if float(num1) \u003e float(num2) else '\u003c'} {float(num2)} \"\"\" return prompt # The harsh reality: # We're using $100B language models # But need to PRE-CALCULATE math for them # Because they can't see numbers properly The Benchmarks That Lie # Why math benchmarks are misleading: def benchmark_tokenization_bias(): \"\"\"GSM8K and other benchmarks use 'nice' numbers\"\"\" # Benchmark problems use: nice_numbers = [\"2\", \"5\", \"10\", \"100\", \"1000\"] # All single tokens! # Real-world uses: real_numbers = [\"1847\", \"3.14159\", \"$24.99\", \"2024-01-15\"] # All fragmented! print(\"Benchmark numbers (what models are tested on):\") for n in nice_numbers: print(f\" {n} ‚Üí {len(tokenizer.encode(n))} token(s)\") print(\"\\nReal-world numbers (what you actually need):\") for n in real_numbers: print(f\" {n} ‚Üí {len(tokenizer.encode(n))} token(s)\") return \"Models ace benchmarks, fail at your invoice calculations\" The Uncomfortable Truth About Quantitative AI The paradox: We‚Äôre using language models for quantitative analysis, but they literally cannot perceive quantities consistently.\nWhat this means:\nFinancial models that can‚Äôt compare prices Scientific models that fail at measurements Data analysis tools that can‚Äôt count Coding assistants that mess up array indices The industry‚Äôs dirty secret: Everyone knows this, but we pretend it‚Äôs fine because:\nThe models are ‚Äúgood enough‚Äù for text We can work around it with prompting Fixing it would require rebuilding everything üí° Immediate Action: Never trust a language model with math. Always verify numerical outputs. If you‚Äôre building a financial or scientific application, pre-process all numbers into consistent tokens or use specialized numerical encodings.\nTakeaway: Your $100B language model can‚Äôt do 4th grade math because tokenization shatters numbers into random chunks. It‚Äôs not learning arithmetic, it‚Äôs pattern matching fragments. This is why ‚Äú9.11 \u003e 9.9‚Äù and why GPT-4o needs a calculator plugin. Until we fix tokenization, language models will remain quantitatively illiterate.\n",
  "wordCount" : "1460",
  "inLanguage": "en",
  "datePublished": "2025-09-07T00:10:39+05:30",
  "dateModified": "2025-09-07T00:10:39+05:30",
  "author":{
    "@type": "Person",
    "name": "Lakshay Chhabra"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lucven.com/posts/tokenization/tokenization_math/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Lucven AI",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lucven.com/favicon_io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lucven.com/" accesskey="h" title="Lucven AI (Alt + H)">Lucven AI</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://lucven.com/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://lucven.com/">Home</a>&nbsp;¬ª&nbsp;<a href="https://lucven.com/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      How Tokenization Murders Your Model&#39;s Ability to Do Basic Math
    </h1>
    <div class="post-meta"><span title='2025-09-07 00:10:39 +0530 IST'>September 7, 2025</span>&nbsp;¬∑&nbsp;7 min&nbsp;¬∑&nbsp;Lakshay Chhabra

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#how-tokenization-murders-your-models-ability-to-do-basic-math" aria-label="How Tokenization Murders Your Model&rsquo;s Ability to Do Basic Math">How Tokenization Murders Your Model&rsquo;s Ability to Do Basic Math</a><ul>
                        
                <li>
                    <a href="#the-crime-scene-test-this-right-now" aria-label="The Crime Scene: Test This Right Now">The Crime Scene: Test This Right Now</a></li>
                <li>
                    <a href="#the-911--99-disaster-yes-really" aria-label="The &ldquo;9.11 &gt; 9.9&rdquo; Disaster (Yes, Really)">The &ldquo;9.11 &gt; 9.9&rdquo; Disaster (Yes, Really)</a></li>
                <li>
                    <a href="#why-your-model-cant-count" aria-label="Why Your Model Can&rsquo;t Count">Why Your Model Can&rsquo;t Count</a></li>
                <li>
                    <a href="#the-arithmetic-apocalypse" aria-label="The Arithmetic Apocalypse">The Arithmetic Apocalypse</a></li>
                <li>
                    <a href="#the-decimal-disaster" aria-label="The Decimal Disaster">The Decimal Disaster</a></li>
                <li>
                    <a href="#the-datetime-tokenization-massacre" aria-label="The Date/Time Tokenization Massacre">The Date/Time Tokenization Massacre</a></li>
                <li>
                    <a href="#the-phone-number-privacy-leak" aria-label="The Phone Number Privacy Leak">The Phone Number Privacy Leak</a></li>
                <li>
                    <a href="#the-solution-no-one-wants-to-hear" aria-label="The Solution No One Wants to Hear">The Solution No One Wants to Hear</a></li>
                <li>
                    <a href="#the-benchmarks-that-lie" aria-label="The Benchmarks That Lie">The Benchmarks That Lie</a></li>
                <li>
                    <a href="#the-uncomfortable-truth-about-quantitative-ai" aria-label="The Uncomfortable Truth About Quantitative AI">The Uncomfortable Truth About Quantitative AI</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="how-tokenization-murders-your-models-ability-to-do-basic-math">How Tokenization Murders Your Model&rsquo;s Ability to Do Basic Math<a hidden class="anchor" aria-hidden="true" href="#how-tokenization-murders-your-models-ability-to-do-basic-math">#</a></h1>
<p>GPT-4o can write Shakespeare but struggles with 4-digit multiplication. It&rsquo;s not stupid, it literally can&rsquo;t see numbers the way you do. &ldquo;12345&rdquo; might be [&ldquo;123&rdquo;, &ldquo;45&rdquo;] while &ldquo;12346&rdquo; is [&ldquo;1&rdquo;, &ldquo;2346&rdquo;]. Try doing math when numbers randomly shatter into chunks.</p>
<blockquote>
<p><strong>TL;DR</strong>: Tokenizers split numbers inconsistently, making arithmetic nearly impossible. &ldquo;9.11&rdquo; &gt; &ldquo;9.9&rdquo; according to many models because &ldquo;.11&rdquo; and &ldquo;.9&rdquo; are different tokens. Your calculator app works. Your $100B language model doesn&rsquo;t. This is why.</p></blockquote>
<h2 id="the-crime-scene-test-this-right-now">The Crime Scene: Test This Right Now<a hidden class="anchor" aria-hidden="true" href="#the-crime-scene-test-this-right-now">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># The murder weapon: inconsistent number tokenization</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">number_tokenization_horror</span>(tokenizer):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Watch tokenization destroy math ability&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    numbers <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;1&#34;</span>, <span style="color:#e6db74">&#34;12&#34;</span>, <span style="color:#e6db74">&#34;123&#34;</span>, <span style="color:#e6db74">&#34;1234&#34;</span>, <span style="color:#e6db74">&#34;12345&#34;</span>, <span style="color:#e6db74">&#34;123456&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;42&#34;</span>, <span style="color:#e6db74">&#34;420&#34;</span>, <span style="color:#e6db74">&#34;4200&#34;</span>, <span style="color:#e6db74">&#34;42000&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;9.9&#34;</span>, <span style="color:#e6db74">&#34;9.11&#34;</span>, <span style="color:#e6db74">&#34;9.111&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;2023&#34;</span>, <span style="color:#e6db74">&#34;2024&#34;</span>, <span style="color:#e6db74">&#34;2025&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;1000000&#34;</span>, <span style="color:#e6db74">&#34;1,000,000&#34;</span>, <span style="color:#e6db74">&#34;1e6&#34;</span>
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Number ‚Üí Tokens (How the model &#39;sees&#39; it)&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;-&#34;</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">50</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> num <span style="color:#f92672">in</span> numbers:
</span></span><span style="display:flex;"><span>        tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(num)
</span></span><span style="display:flex;"><span>        decoded <span style="color:#f92672">=</span> [tokenizer<span style="color:#f92672">.</span>decode([t]) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tokens]
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># The horror reveal</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> len(decoded) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>num<span style="color:#e6db74">:</span><span style="color:#e6db74">10</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> ‚Üí </span><span style="color:#e6db74">{</span>decoded<span style="color:#e6db74">}</span><span style="color:#e6db74"> üî™ (MURDERED)&#34;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>num<span style="color:#e6db74">:</span><span style="color:#e6db74">10</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> ‚Üí </span><span style="color:#e6db74">{</span>decoded<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;Your model can&#39;t do math because it can&#39;t even see numbers&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run this and watch the chaos</span>
</span></span></code></pre></div><h2 id="the-911--99-disaster-yes-really">The &ldquo;9.11 &gt; 9.9&rdquo; Disaster (Yes, Really)<a hidden class="anchor" aria-hidden="true" href="#the-911--99-disaster-yes-really">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># This actually happens in production:</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>comparisons <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;9.9&#34;</span>, <span style="color:#e6db74">&#34;9.11&#34;</span>),    <span style="color:#75715e"># 9.11 is SMALLER but models think it&#39;s bigger</span>
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;2.8&#34;</span>, <span style="color:#e6db74">&#34;2.80&#34;</span>),    <span style="color:#75715e"># Same number, different tokens</span>
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;1000&#34;</span>, <span style="color:#e6db74">&#34;1,000&#34;</span>),  <span style="color:#75715e"># Same number, different tokens</span>
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;3.14&#34;</span>, <span style="color:#e6db74">&#34;3.141&#34;</span>),  <span style="color:#75715e"># œÄ gets progressively worse</span>
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> a, b <span style="color:#f92672">in</span> comparisons:
</span></span><span style="display:flex;"><span>    tokens_a <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(a)
</span></span><span style="display:flex;"><span>    tokens_b <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(b)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>a<span style="color:#e6db74">}</span><span style="color:#e6db74"> ‚Üí </span><span style="color:#e6db74">{</span>tokens_a<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>b<span style="color:#e6db74">}</span><span style="color:#e6db74"> ‚Üí </span><span style="color:#e6db74">{</span>tokens_b<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Models compare TOKEN VALUES, not numerical values</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># &#34;11&#34; &gt; &#34;9&#34; as a token, so 9.11 &gt; 9.9</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># This is why your chatbot says 9.11 is bigger than 9.9</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># THE KILLER: Version numbers</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># &#34;Python 3.9&#34; vs &#34;Python 3.11&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Models think 3.11 &lt; 3.9 because &#34;.11&#34; &lt; &#34;.9&#34; as text</span>
</span></span></code></pre></div><h2 id="why-your-model-cant-count">Why Your Model Can&rsquo;t Count<a hidden class="anchor" aria-hidden="true" href="#why-your-model-cant-count">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># The counting disaster:</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">why_counting_fails</span>(tokenizer):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Models can&#39;t count because they can&#39;t see sequences&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Try to count from 1 to 20</span>
</span></span><span style="display:flex;"><span>    sequence <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20&#34;</span>
</span></span><span style="display:flex;"><span>    tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(sequence)
</span></span><span style="display:flex;"><span>    decoded <span style="color:#f92672">=</span> [tokenizer<span style="color:#f92672">.</span>decode([t]) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tokens]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;You see: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Model sees: </span><span style="color:#e6db74">{</span>decoded<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Reality: [&#39;1&#39;, &#39; 2&#39;, &#39; 3&#39;, ..., &#39; 10&#39;, &#39; 11&#39;, &#39; 12&#39;, &#39; 13&#39;, &#39; 14&#39;, &#39; 15&#39;, &#39; 16&#39;, &#39; 17&#39;, &#39; 18&#39;, &#39; 19&#39;, &#39; 20&#39;]</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Some are single tokens, some are split</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Patterns are DESTROYED</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Now try counting by 10s:</span>
</span></span><span style="display:flex;"><span>    by_tens <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;10 20 30 40 50 60 70 80 90 100&#34;</span>
</span></span><span style="display:flex;"><span>    tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(by_tens)
</span></span><span style="display:flex;"><span>    decoded <span style="color:#f92672">=</span> [tokenizer<span style="color:#f92672">.</span>decode([t]) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tokens]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Counting by 10s: </span><span style="color:#e6db74">{</span>decoded<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># &#34;10&#34; might be 1 token, &#34;20&#34; might be 1 token, but &#34;30&#34; might be [&#34;3&#34;, &#34;0&#34;]</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># No wonder it can&#39;t learn the pattern!</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This is why models fail at:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># - &#34;Continue the sequence: 2, 4, 6, 8, ...&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># - &#34;What comes after 99?&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># - &#34;Count backwards from 10&#34;</span>
</span></span></code></pre></div><h2 id="the-arithmetic-apocalypse">The Arithmetic Apocalypse<a hidden class="anchor" aria-hidden="true" href="#the-arithmetic-apocalypse">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Why GPT can&#39;t do basic math:</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">arithmetic_tokenization_study</span>(tokenizer):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;See why 2+2 works but 1234+5678 doesn&#39;t&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    problems <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;2+2&#34;</span>,          <span style="color:#75715e"># Perfect: [&#34;2&#34;, &#34;+&#34;, &#34;2&#34;]</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;10+10&#34;</span>,        <span style="color:#75715e"># Still OK: [&#34;10&#34;, &#34;+&#34;, &#34;10&#34;]</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;123+456&#34;</span>,      <span style="color:#75715e"># Getting bad: [&#34;123&#34;, &#34;+&#34;, &#34;456&#34;] or [&#34;12&#34;, &#34;3&#34;, &#34;+&#34;, &#34;45&#34;, &#34;6&#34;]</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;1234+5678&#34;</span>,    <span style="color:#75715e"># Disaster: [&#34;123&#34;, &#34;4&#34;, &#34;+&#34;, &#34;567&#34;, &#34;8&#34;]</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;12345+67890&#34;</span>,  <span style="color:#75715e"># Apocalypse: [&#34;1&#34;, &#34;234&#34;, &#34;5&#34;, &#34;+&#34;, &#34;678&#34;, &#34;90&#34;]</span>
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> problem <span style="color:#f92672">in</span> problems:
</span></span><span style="display:flex;"><span>        tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(problem)
</span></span><span style="display:flex;"><span>        decoded <span style="color:#f92672">=</span> [tokenizer<span style="color:#f92672">.</span>decode([t]) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tokens]
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Calculate token chaos score</span>
</span></span><span style="display:flex;"><span>        expected_tokens <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>  <span style="color:#75715e"># number, operator, number</span>
</span></span><span style="display:flex;"><span>        chaos_score <span style="color:#f92672">=</span> len(tokens) <span style="color:#f92672">-</span> expected_tokens
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> chaos_score <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            status <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;‚úÖ Can solve&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">elif</span> chaos_score <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">2</span>:
</span></span><span style="display:flex;"><span>            status <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;‚ö†Ô∏è Might solve&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            status <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;üíÄ Will fail&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>problem<span style="color:#e6db74">:</span><span style="color:#e6db74">15</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> ‚Üí </span><span style="color:#e6db74">{</span>decoded<span style="color:#e6db74">:</span><span style="color:#e6db74">30</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> </span><span style="color:#e6db74">{</span>status<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;This is why calculators still exist&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The bitter truth:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># o can write poetry about quantum physics</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># But fails at 4-digit multiplication</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Because it literally cannot see &#34;1234&#34; as 1234</span>
</span></span></code></pre></div><h2 id="the-decimal-disaster">The Decimal Disaster<a hidden class="anchor" aria-hidden="true" href="#the-decimal-disaster">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Decimals are even worse:</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>decimal_nightmares <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;0.1&#34;</span>: [<span style="color:#e6db74">&#34;0&#34;</span>, <span style="color:#e6db74">&#34;.&#34;</span>, <span style="color:#e6db74">&#34;1&#34;</span>],           <span style="color:#75715e"># 3 tokens for a simple decimal</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;0.01&#34;</span>: [<span style="color:#e6db74">&#34;0&#34;</span>, <span style="color:#e6db74">&#34;.&#34;</span>, <span style="color:#e6db74">&#34;01&#34;</span>],         <span style="color:#75715e"># Inconsistent!</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;0.001&#34;</span>: [<span style="color:#e6db74">&#34;0&#34;</span>, <span style="color:#e6db74">&#34;.&#34;</span>, <span style="color:#e6db74">&#34;001&#34;</span>],       <span style="color:#75715e"># Getting worse</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;3.14159&#34;</span>: [<span style="color:#e6db74">&#34;3&#34;</span>, <span style="color:#e6db74">&#34;.&#34;</span>, <span style="color:#e6db74">&#34;14&#34;</span>, <span style="color:#e6db74">&#34;159&#34;</span>], <span style="color:#75715e"># œÄ is shattered</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;2.718&#34;</span>: [<span style="color:#e6db74">&#34;2&#34;</span>, <span style="color:#e6db74">&#34;.&#34;</span>, <span style="color:#e6db74">&#34;7&#34;</span>, <span style="color:#e6db74">&#34;18&#34;</span>],   <span style="color:#75715e"># e is broken</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;$19.99&#34;</span>: [<span style="color:#e6db74">&#34;$&#34;</span>, <span style="color:#e6db74">&#34;19&#34;</span>, <span style="color:#e6db74">&#34;.&#34;</span>, <span style="color:#e6db74">&#34;99&#34;</span>], <span style="color:#75715e"># Prices are chaos</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;0.999...&#34;</span>: [<span style="color:#e6db74">&#34;0&#34;</span>, <span style="color:#e6db74">&#34;.&#34;</span>, <span style="color:#e6db74">&#34;999&#34;</span>, <span style="color:#e6db74">&#34;...&#34;</span>], <span style="color:#75715e"># Math notation destroyed</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This is why:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># - Models think 0.9 &gt; 0.11 (string comparison)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># - Can&#39;t properly handle financial calculations  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># - Fail at scientific notation</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># - Think $19.99 and $20.00 are fundamentally different concepts</span>
</span></span></code></pre></div><h2 id="the-datetime-tokenization-massacre">The Date/Time Tokenization Massacre<a hidden class="anchor" aria-hidden="true" href="#the-datetime-tokenization-massacre">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">datetime_tokenization_chaos</span>(tokenizer):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Why models are terrible with dates and times&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    dates <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;2023&#34;</span>,           <span style="color:#75715e"># Likely 1 token (common year)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;2024&#34;</span>,           <span style="color:#75715e"># Might be 2 tokens</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;2025&#34;</span>,           <span style="color:#75715e"># Probably 2 tokens  </span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;1999&#34;</span>,           <span style="color:#75715e"># 1-2 tokens (Y2K made it common)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;2000&#34;</span>,           <span style="color:#75715e"># 1 token (millennium)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;1823&#34;</span>,           <span style="color:#75715e"># 2-3 tokens (random year)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;2024-01-01&#34;</span>,     <span style="color:#75715e"># 5-7 tokens</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;12/25/2023&#34;</span>,     <span style="color:#75715e"># 6-8 tokens</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;3:14 PM&#34;</span>,        <span style="color:#75715e"># 4-5 tokens</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;15:30:45&#34;</span>,       <span style="color:#75715e"># 5-7 tokens</span>
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> date <span style="color:#f92672">in</span> dates:
</span></span><span style="display:flex;"><span>        tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(date)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> len(tokens) <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>date<span style="color:#e6db74">}</span><span style="color:#e6db74"> ‚Üí MEMORIZED (seen thousands of times)&#34;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            decoded <span style="color:#f92672">=</span> [tokenizer<span style="color:#f92672">.</span>decode([t]) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tokens]
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>date<span style="color:#e6db74">}</span><span style="color:#e6db74"> ‚Üí </span><span style="color:#e6db74">{</span>decoded<span style="color:#e6db74">}</span><span style="color:#e6db74"> (fragmented perception)&#34;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># This explains why models:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># - Can&#39;t calculate date differences</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># - Fail at &#34;what day is 30 days from today?&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># - Think 12/25 comes before 12/3 (string order)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># - Can&#39;t handle timezone conversions</span>
</span></span></code></pre></div><h2 id="the-phone-number-privacy-leak">The Phone Number Privacy Leak<a hidden class="anchor" aria-hidden="true" href="#the-phone-number-privacy-leak">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Some phone numbers are single tokens (!!!)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">phone_number_investigation</span>(tokenizer):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Some numbers are suspiciously well-tokenized&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    numbers <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;911&#34;</span>,            <span style="color:#75715e"># Emergency (1 token)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;1-800-273-8255&#34;</span>, <span style="color:#75715e"># Suicide hotline (might be few tokens)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;867-5309&#34;</span>,       <span style="color:#75715e"># Jenny&#39;s number (cultural reference)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;(555) 555-5555&#34;</span>, <span style="color:#75715e"># Movie/TV placeholder</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;+1234567890&#34;</span>,    <span style="color:#75715e"># Random number</span>
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> number <span style="color:#f92672">in</span> numbers:
</span></span><span style="display:flex;"><span>        tokens <span style="color:#f92672">=</span> len(tokenizer<span style="color:#f92672">.</span>encode(number))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> tokens <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">3</span>:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;üö® </span><span style="color:#e6db74">{</span>number<span style="color:#e6db74">}</span><span style="color:#e6db74"> is </span><span style="color:#e6db74">{</span>tokens<span style="color:#e6db74">}</span><span style="color:#e6db74"> tokens - MEMORIZED IN TRAINING&#34;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>number<span style="color:#e6db74">}</span><span style="color:#e6db74"> is </span><span style="color:#e6db74">{</span>tokens<span style="color:#e6db74">}</span><span style="color:#e6db74"> tokens&#34;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># If a phone number is &lt;5 tokens, it appeared in training data</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># This is a privacy nightmare</span>
</span></span></code></pre></div><h2 id="the-solution-no-one-wants-to-hear">The Solution No One Wants to Hear<a hidden class="anchor" aria-hidden="true" href="#the-solution-no-one-wants-to-hear">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MathTokenizationWorkaround</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;How to make models not suck at math&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fix_arithmetic</span>(self, expression):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Pre-tokenize numbers properly&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Step 1: Space out everything</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># &#34;1234+5678&#34; ‚Üí &#34;1 2 3 4 + 5 6 7 8&#34;</span>
</span></span><span style="display:flex;"><span>        spaced <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39; &#39;</span><span style="color:#f92672">.</span>join(expression)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Step 2: Use chain-of-thought</span>
</span></span><span style="display:flex;"><span>        prompt <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Solve step by step:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        </span><span style="color:#e6db74">{</span>expression<span style="color:#e6db74">}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        First, identify the numbers:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        - First number: </span><span style="color:#e6db74">{</span>expression<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;+&#39;</span>)[<span style="color:#ae81ff">0</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        - Second number: </span><span style="color:#e6db74">{</span>expression<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;+&#39;</span>)[<span style="color:#ae81ff">1</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Now add digit by digit...
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Step 3: Or just give up and use a calculator</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">import</span> re
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> re<span style="color:#f92672">.</span><span style="color:#66d9ef">match</span>(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;^[\d\+\-\*/\.\s]+$&#39;</span>, expression):
</span></span><span style="display:flex;"><span>            result <span style="color:#f92672">=</span> eval(expression)  <span style="color:#75715e"># Don&#39;t do this in production!</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;The answer is </span><span style="color:#e6db74">{</span>result<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;This is why we still need calculators&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fix_comparison</span>(self, num1, num2):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Fix number comparison&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Convert to actual numbers first</span>
</span></span><span style="display:flex;"><span>        prompt <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Compare these as decimal numbers:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        A = </span><span style="color:#e6db74">{</span>num1<span style="color:#e6db74">}</span><span style="color:#e6db74"> (decimal value: </span><span style="color:#e6db74">{</span>float(num1)<span style="color:#e6db74">}</span><span style="color:#e6db74">)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        B = </span><span style="color:#e6db74">{</span>num2<span style="color:#e6db74">}</span><span style="color:#e6db74"> (decimal value: </span><span style="color:#e6db74">{</span>float(num2)<span style="color:#e6db74">}</span><span style="color:#e6db74">)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Therefore </span><span style="color:#e6db74">{</span>float(num1)<span style="color:#e6db74">}</span><span style="color:#e6db74"> </span><span style="color:#e6db74">{</span><span style="color:#e6db74">&#39;&gt;&#39;</span> <span style="color:#66d9ef">if</span> float(num1) <span style="color:#f92672">&gt;</span> float(num2) <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#39;&lt;&#39;</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> </span><span style="color:#e6db74">{</span>float(num2)<span style="color:#e6db74">}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> prompt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The harsh reality:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># We&#39;re using $100B language models</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># But need to PRE-CALCULATE math for them</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Because they can&#39;t see numbers properly</span>
</span></span></code></pre></div><h2 id="the-benchmarks-that-lie">The Benchmarks That Lie<a hidden class="anchor" aria-hidden="true" href="#the-benchmarks-that-lie">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Why math benchmarks are misleading:</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">benchmark_tokenization_bias</span>():
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;GSM8K and other benchmarks use &#39;nice&#39; numbers&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Benchmark problems use:</span>
</span></span><span style="display:flex;"><span>    nice_numbers <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;2&#34;</span>, <span style="color:#e6db74">&#34;5&#34;</span>, <span style="color:#e6db74">&#34;10&#34;</span>, <span style="color:#e6db74">&#34;100&#34;</span>, <span style="color:#e6db74">&#34;1000&#34;</span>]  <span style="color:#75715e"># All single tokens!</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Real-world uses:</span>
</span></span><span style="display:flex;"><span>    real_numbers <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;1847&#34;</span>, <span style="color:#e6db74">&#34;3.14159&#34;</span>, <span style="color:#e6db74">&#34;$24.99&#34;</span>, <span style="color:#e6db74">&#34;2024-01-15&#34;</span>]  <span style="color:#75715e"># All fragmented!</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Benchmark numbers (what models are tested on):&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> n <span style="color:#f92672">in</span> nice_numbers:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;  </span><span style="color:#e6db74">{</span>n<span style="color:#e6db74">}</span><span style="color:#e6db74"> ‚Üí </span><span style="color:#e6db74">{</span>len(tokenizer<span style="color:#f92672">.</span>encode(n))<span style="color:#e6db74">}</span><span style="color:#e6db74"> token(s)&#34;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Real-world numbers (what you actually need):&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> n <span style="color:#f92672">in</span> real_numbers:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;  </span><span style="color:#e6db74">{</span>n<span style="color:#e6db74">}</span><span style="color:#e6db74"> ‚Üí </span><span style="color:#e6db74">{</span>len(tokenizer<span style="color:#f92672">.</span>encode(n))<span style="color:#e6db74">}</span><span style="color:#e6db74"> token(s)&#34;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;Models ace benchmarks, fail at your invoice calculations&#34;</span>
</span></span></code></pre></div><h2 id="the-uncomfortable-truth-about-quantitative-ai">The Uncomfortable Truth About Quantitative AI<a hidden class="anchor" aria-hidden="true" href="#the-uncomfortable-truth-about-quantitative-ai">#</a></h2>
<p><strong>The paradox</strong>: We&rsquo;re using language models for quantitative analysis, but they literally cannot perceive quantities consistently.</p>
<p><strong>What this means</strong>:</p>
<ul>
<li>Financial models that can&rsquo;t compare prices</li>
<li>Scientific models that fail at measurements</li>
<li>Data analysis tools that can&rsquo;t count</li>
<li>Coding assistants that mess up array indices</li>
</ul>
<p><strong>The industry&rsquo;s dirty secret</strong>: Everyone knows this, but we pretend it&rsquo;s fine because:</p>
<ol>
<li>The models are &ldquo;good enough&rdquo; for text</li>
<li>We can work around it with prompting</li>
<li>Fixing it would require rebuilding everything</li>
</ol>
<hr>
<blockquote>
<p><strong>üí° Immediate Action</strong>: Never trust a language model with math. Always verify numerical outputs. If you&rsquo;re building a financial or scientific application, pre-process all numbers into consistent tokens or use specialized numerical encodings.</p></blockquote>
<hr>
<p><strong>Takeaway:</strong> Your $100B language model can&rsquo;t do 4th grade math because tokenization shatters numbers into random chunks. It&rsquo;s not learning arithmetic, it&rsquo;s pattern matching fragments. This is why &ldquo;9.11 &gt; 9.9&rdquo; and why GPT-4o needs a calculator plugin. Until we fix tokenization, language models will remain quantitatively illiterate.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lucven.com/tags/tokenisation/">Tokenisation</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://lucven.com/posts/tokenization/tokenization_personality_trigger/">
    <span class="title">¬´ Prev</span>
    <br>
    <span>How Spacing and Capitalization Randomly Change Your Model&#39;s Entire Personality</span>
  </a>
  <a class="next" href="https://lucven.com/posts/tokenization/tokenisation_limits/">
    <span class="title">Next ¬ª</span>
    <br>
    <span>Why Your Vector Database Thinks $AAPL Means Polish Batteries</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share How Tokenization Murders Your Model&#39;s Ability to Do Basic Math on x"
            href="https://x.com/intent/tweet/?text=How%20Tokenization%20Murders%20Your%20Model%27s%20Ability%20to%20Do%20Basic%20Math&amp;url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2ftokenization_math%2f&amp;hashtags=Tokenisation">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share How Tokenization Murders Your Model&#39;s Ability to Do Basic Math on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2ftokenization_math%2f&amp;title=How%20Tokenization%20Murders%20Your%20Model%27s%20Ability%20to%20Do%20Basic%20Math&amp;summary=How%20Tokenization%20Murders%20Your%20Model%27s%20Ability%20to%20Do%20Basic%20Math&amp;source=https%3a%2f%2flucven.com%2fposts%2ftokenization%2ftokenization_math%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share How Tokenization Murders Your Model&#39;s Ability to Do Basic Math on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2ftokenization_math%2f&title=How%20Tokenization%20Murders%20Your%20Model%27s%20Ability%20to%20Do%20Basic%20Math">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share How Tokenization Murders Your Model&#39;s Ability to Do Basic Math on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flucven.com%2fposts%2ftokenization%2ftokenization_math%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share How Tokenization Murders Your Model&#39;s Ability to Do Basic Math on whatsapp"
            href="https://api.whatsapp.com/send?text=How%20Tokenization%20Murders%20Your%20Model%27s%20Ability%20to%20Do%20Basic%20Math%20-%20https%3a%2f%2flucven.com%2fposts%2ftokenization%2ftokenization_math%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share How Tokenization Murders Your Model&#39;s Ability to Do Basic Math on telegram"
            href="https://telegram.me/share/url?text=How%20Tokenization%20Murders%20Your%20Model%27s%20Ability%20to%20Do%20Basic%20Math&amp;url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2ftokenization_math%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share How Tokenization Murders Your Model&#39;s Ability to Do Basic Math on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=How%20Tokenization%20Murders%20Your%20Model%27s%20Ability%20to%20Do%20Basic%20Math&u=https%3a%2f%2flucven.com%2fposts%2ftokenization%2ftokenization_math%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer><script src="https://giscus.app/client.js"
  data-repo="lakshaychhabra/lucven-comments"
  data-repo-id="R_kgDOPYOEYw"
  data-category="Q&A"
  data-category-id="DIC_kwDOPYOEY84CtxWt"
  data-mapping="pathname"
  data-strict="0"
  data-reactions-enabled="1"
  data-emit-metadata="0"
  data-input-position="top"
  data-theme="preferred_color_scheme"
  data-lang="en"
  data-loading="lazy"
  crossorigin="anonymous"
  async>
</script>


</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://lucven.com/">Lucven AI</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
