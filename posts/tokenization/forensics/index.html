<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Tokenization Forensics about Leaks | Lucven AI</title>
<meta name="keywords" content="Tokenisation">
<meta name="description" content="Tokenization Leaks the Training Set (The Forensics Goldmine)
Want to know if GPT-4 was trained on your company&rsquo;s leaked data? Check if your internal codenames are single tokens. Want to detect if a model saw specific Reddit posts? The tokenizer already told you.

TL;DR: Tokenizers are accidental forensic evidence. If SolidGoldMagikarp is a single token, that string appeared thousands of times in training. This is how researchers discovered GPT models trained on specific Reddit users, leaked databases, and private codebases.">
<meta name="author" content="Lakshay Chhabra">
<link rel="canonical" href="https://lucven.com/posts/tokenization/forensics/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://lucven.com/favicon_io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lucven.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lucven.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lucven.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://lucven.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://lucven.com/posts/tokenization/forensics/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:url" content="https://lucven.com/posts/tokenization/forensics/">
  <meta property="og:site_name" content="Lucven AI">
  <meta property="og:title" content="Tokenization Forensics about Leaks">
  <meta property="og:description" content="Tokenization Leaks the Training Set (The Forensics Goldmine) Want to know if GPT-4 was trained on your company‚Äôs leaked data? Check if your internal codenames are single tokens. Want to detect if a model saw specific Reddit posts? The tokenizer already told you.
TL;DR: Tokenizers are accidental forensic evidence. If SolidGoldMagikarp is a single token, that string appeared thousands of times in training. This is how researchers discovered GPT models trained on specific Reddit users, leaked databases, and private codebases.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-09-03T00:10:39+05:30">
    <meta property="article:modified_time" content="2025-09-03T00:10:39+05:30">
    <meta property="article:tag" content="Tokenisation">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tokenization Forensics about Leaks">
<meta name="twitter:description" content="Tokenization Leaks the Training Set (The Forensics Goldmine)
Want to know if GPT-4 was trained on your company&rsquo;s leaked data? Check if your internal codenames are single tokens. Want to detect if a model saw specific Reddit posts? The tokenizer already told you.

TL;DR: Tokenizers are accidental forensic evidence. If SolidGoldMagikarp is a single token, that string appeared thousands of times in training. This is how researchers discovered GPT models trained on specific Reddit users, leaked databases, and private codebases.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://lucven.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Tokenization Forensics about Leaks",
      "item": "https://lucven.com/posts/tokenization/forensics/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Tokenization Forensics about Leaks",
  "name": "Tokenization Forensics about Leaks",
  "description": "Tokenization Leaks the Training Set (The Forensics Goldmine) Want to know if GPT-4 was trained on your company\u0026rsquo;s leaked data? Check if your internal codenames are single tokens. Want to detect if a model saw specific Reddit posts? The tokenizer already told you.\nTL;DR: Tokenizers are accidental forensic evidence. If SolidGoldMagikarp is a single token, that string appeared thousands of times in training. This is how researchers discovered GPT models trained on specific Reddit users, leaked databases, and private codebases.\n",
  "keywords": [
    "Tokenisation"
  ],
  "articleBody": "Tokenization Leaks the Training Set (The Forensics Goldmine) Want to know if GPT-4 was trained on your company‚Äôs leaked data? Check if your internal codenames are single tokens. Want to detect if a model saw specific Reddit posts? The tokenizer already told you.\nTL;DR: Tokenizers are accidental forensic evidence. If SolidGoldMagikarp is a single token, that string appeared thousands of times in training. This is how researchers discovered GPT models trained on specific Reddit users, leaked databases, and private codebases.\nThe Smoking Gun Test # Test if a model was trained on specific data: def training_set_forensics(tokenizer, test_strings): \"\"\"Detect what the model likely saw during training\"\"\" results = {} for string in test_strings: tokens = tokenizer.encode(string) decoded = [tokenizer.decode([t]) for t in tokens] # Single token = appeared frequently in training # Many tokens = rare or unseen if len(tokens) == 1: status = \"üî¥ DEFINITELY in training (1000s of times)\" elif len(tokens) == 2: status = \"üü° Likely in training (100s of times)\" elif len(tokens) \u003c= len(string)/4: status = \"üü¢ Possibly in training\" else: status = \"‚ö™ Probably NOT in training\" results[string] = { 'tokens': len(tokens), 'pieces': decoded, 'status': status } return results # Try these: suspicious_strings = [ \"$AAPL\", # Apple stock \"$TSLA\", # Tesla stock \"$GME\", # GameStop (meme stock) \"SolidGoldMagikarp\", # Reddit username \"petertodd\", # Bitcoin developer \" davidjl123\", # Another Reddit user \"TheNitromeFan\", # Counting subreddit user \"\u003c|endoftext|\u003e\", # GPT special token \"REDACTED_EMAIL\", # Your company's placeholder? ] results = training_set_forensics(tokenizer, suspicious_strings) The Reddit Conspiracy (It‚Äôs Real) # These are ACTUAL single tokens in GPT tokenizers: weird_tokens = { \"SolidGoldMagikarp\": 1, # Reddit user with 100K+ comments \" petertodd\": 1, # Bitcoin developer mentioned everywhere \"TheNitromeFan\": 1, # Power user in r/counting \" davidjl123\": 1, # Another counting enthusiast \"cloneembryos\": 1, # WTF? Specific subreddit discussions \" guiActiveUn\": 1, # Unity game engine internal variable \" TPPStreamerBot\": 1, # Twitch Plays Pokemon bot } # Why does this matter? # 1. These users' writing styles are BAKED into the model # 2. Prompting with these tokens triggers specific behaviors # 3. It's proof of training on Reddit data through 2021 The Domain Detector # Compare tokenizers to detect training bias: def compare_domain_coverage(text, tokenizers_dict): \"\"\"See which model knows your domain best\"\"\" print(f\"Testing: '{text}'\") print(\"-\" * 50) best_score = float('inf') best_model = None for name, tokenizer in tokenizers_dict.items(): tokens = tokenizer.encode(text) decoded = [tokenizer.decode([t]) for t in tokens] print(f\"{name:15} ‚Üí {len(tokens)} tokens: {decoded}\") if len(tokens) \u003c best_score: best_score = len(tokens) best_model = name print(f\"\\nüèÜ {best_model} knows this domain best!\") return best_model # Real examples: finance_test = \"$NVDA earnings beat expectations Q4\" crypto_test = \"dogecoin hodl moon lambo wen\" medical_test = \"acetaminophen hepatotoxicity cirrhosis\" gaming_test = \"speedrun any% glitchless PB WR\" # Results reveal training data: # GPT-4: Good at finance (WSB in training) # Claude: Better at medical (PubMed heavy) # LLaMA: Gaming terms fragmented (less Reddit) The Corporate Leak Detector # Check if your company's data leaked into training: company_indicators = [ # Your internal codenames \"Project_Phantom\", \"PROD_API_KEY_V2\", \"TODO_FIXME_HACK\", # Your employee usernames \"jsmith@yourcompany\", \"jenkins-bot-prod\", # Your internal URLs \"internal.yourcompany.com\", \"staging-server-2\", # Your error messages \"Error: Database connection failed (YourCompany v2.3.1)\", ] for indicator in company_indicators: tokens = tokenizer.encode(indicator) if len(tokens) \u003c= 3: # Suspiciously efficient print(f\"‚ö†Ô∏è ALERT: '{indicator}' is {len(tokens)} tokens!\") print(f\"This string likely appeared in training data!\") The Easter Egg Hunt (Glitch Tokens) # Some tokens cause BIZARRE behavior when used as prompts: glitch_tokens = { \"SolidGoldMagikarp\": \"Causes hallucinations about goldfish\", \" petertodd\": \"Triggers Bitcoin discussions\", \"„Äâ„Äà\": \"Makes models output broken HTML\", \" Leilan\": \"Triggers anime/mythology crossover\", \"PsyNetMessage\": \"Outputs Rocket League error messages\", \" embedreportprint\": \"Triggers debug output\", \" guiActiveUn\": \"Outputs Unity Engine internals\", } # Try this (at your own risk): # prompt = \"Tell me about SolidGoldMagikarp\" # Models often respond with nonsense about goldfish, magic, or Reddit # WHY THIS HAPPENS: # 1. Token appears thousands of times in specific context # 2. Model memorizes the pattern around it # 3. Token becomes a \"trigger\" for that context # 4. Using it alone causes context confusion The Stock Market Tell # Financial bias detection: stocks = [\"$AAPL\", \"$GOOGL\", \"$TSLA\", \"$GME\", \"$AMC\", \"$BBBY\", \"$PLTR\"] for stock in stocks: tokens = len(tokenizer.encode(stock)) if tokens == 1: print(f\"{stock}: üìà Meme stock or high-frequency in training\") elif tokens == 2: print(f\"{stock}: üìä Common stock in financial data\") else: print(f\"{stock}: üìâ Rare in training data\") # GPT models: $GME is often 1-2 tokens (WSB influence) # Claude: $GME is 3-4 tokens (less meme stock training) # This reveals: # - GPT saw tons of WallStreetBets data # - Claude focused more on traditional finance # - Training cutoff dates (pre/post meme stock era) The Privacy Nightmare def check_pii_leakage(tokenizer, email_patterns): \"\"\"Check if specific emails/usernames are in training\"\"\" concerning = [] for email in email_patterns: tokens = tokenizer.encode(email) # If an email is \u003c5 tokens, it appeared A LOT if len(tokens) \u003c 5: concerning.append({ 'email': email, 'tokens': len(tokens), 'risk': 'HIGH - Likely memorized' }) return concerning # Real researcher findings: leaked_emails = [ \"support@company.com\", # 2 tokens - appeared thousands of times \"john.doe.1234@gmail\", # 8 tokens - probably safe \"ceo@fortune500.com\", # 3 tokens - concerning! ] # Researchers found actual email addresses as single tokens # meaning they appeared THOUSANDS of times in training How to Exploit This (Ethically) class TokenizerForensics: \"\"\"Use tokenization to understand model capabilities\"\"\" def detect_programming_languages(self, tokenizer): \"\"\"Which languages did it see most?\"\"\" code_snippets = { 'Python': 'def __init__(self):', 'JavaScript': 'const async = await', 'Rust': 'fn main() -\u003e Result\u003c\u003e', 'Go': 'func (r *Reader) Read()', 'COBOL': 'IDENTIFICATION DIVISION', } for lang, snippet in code_snippets.items(): tokens = len(tokenizer.encode(snippet)) efficiency = len(snippet) / tokens if efficiency \u003e 10: print(f\"{lang}: Heavily trained on this\") elif efficiency \u003e 5: print(f\"{lang}: Moderate training\") else: print(f\"{lang}: Minimal training\") def detect_website_training(self, tokenizer): \"\"\"Which websites were scraped?\"\"\" sites = [ \"reddit.com/r/\", \"stackoverflow.com/questions/\", \"github.com/\", \"arxiv.org/abs/\", \"medium.com/@\", \"substack.com/p/\", ] for site in sites: tokens = len(tokenizer.encode(site)) if tokens \u003c= 3: print(f\"‚úì {site} - Heavily scraped\") else: print(f\"‚úó {site} - Lightly scraped\") The Uncomfortable Implications Your Reddit posts are tokens: Specific users‚Äô entire post histories are compressed into the model\nCorporate leaks are detectable: Internal codenames as single tokens = data breach\nModel capabilities are predictable: Bad tokenization = bad performance in that domain\nPrivacy is already broken: Email addresses, usernames, and phone numbers exist as tokens\nTemporal information leaks: Token efficiency reveals WHEN data was collected\nüí° Action Item: Run training_set_forensics() on your company‚Äôs internal terminology. If anything is ‚â§3 tokens, you might have a leak. Check your competitor‚Äôs terminology too, you might discover their training data sources.\nTakeaway: Tokenizers are inadvertent forensic evidence. Every merge decision is a fossil record of the training data. Single tokens for usernames, stock symbols, or internal codenames aren‚Äôt bugs, they‚Äôre proof of what the model memorized. Use this for good (detecting capabilities) or evil (finding leaks), but know that the tokenizer already spilled the secrets.\n",
  "wordCount" : "1145",
  "inLanguage": "en",
  "datePublished": "2025-09-03T00:10:39+05:30",
  "dateModified": "2025-09-03T00:10:39+05:30",
  "author":{
    "@type": "Person",
    "name": "Lakshay Chhabra"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lucven.com/posts/tokenization/forensics/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Lucven AI",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lucven.com/favicon_io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lucven.com/" accesskey="h" title="Lucven AI (Alt + H)">Lucven AI</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://lucven.com/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://lucven.com/">Home</a>&nbsp;¬ª&nbsp;<a href="https://lucven.com/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Tokenization Forensics about Leaks
    </h1>
    <div class="post-meta"><span title='2025-09-03 00:10:39 +0530 IST'>September 3, 2025</span>&nbsp;¬∑&nbsp;6 min&nbsp;¬∑&nbsp;Lakshay Chhabra

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#tokenization-leaks-the-training-set-the-forensics-goldmine" aria-label="Tokenization Leaks the Training Set (The Forensics Goldmine)">Tokenization Leaks the Training Set (The Forensics Goldmine)</a><ul>
                        
                <li>
                    <a href="#the-smoking-gun-test" aria-label="The Smoking Gun Test">The Smoking Gun Test</a></li>
                <li>
                    <a href="#the-reddit-conspiracy-its-real" aria-label="The Reddit Conspiracy (It&rsquo;s Real)">The Reddit Conspiracy (It&rsquo;s Real)</a></li>
                <li>
                    <a href="#the-domain-detector" aria-label="The Domain Detector">The Domain Detector</a></li>
                <li>
                    <a href="#the-corporate-leak-detector" aria-label="The Corporate Leak Detector">The Corporate Leak Detector</a></li>
                <li>
                    <a href="#the-easter-egg-hunt-glitch-tokens" aria-label="The Easter Egg Hunt (Glitch Tokens)">The Easter Egg Hunt (Glitch Tokens)</a></li>
                <li>
                    <a href="#the-stock-market-tell" aria-label="The Stock Market Tell">The Stock Market Tell</a></li>
                <li>
                    <a href="#the-privacy-nightmare" aria-label="The Privacy Nightmare">The Privacy Nightmare</a></li>
                <li>
                    <a href="#how-to-exploit-this-ethically" aria-label="How to Exploit This (Ethically)">How to Exploit This (Ethically)</a></li>
                <li>
                    <a href="#the-uncomfortable-implications" aria-label="The Uncomfortable Implications">The Uncomfortable Implications</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="tokenization-leaks-the-training-set-the-forensics-goldmine">Tokenization Leaks the Training Set (The Forensics Goldmine)<a hidden class="anchor" aria-hidden="true" href="#tokenization-leaks-the-training-set-the-forensics-goldmine">#</a></h1>
<p>Want to know if GPT-4 was trained on your company&rsquo;s leaked data? Check if your internal codenames are single tokens. Want to detect if a model saw specific Reddit posts? The tokenizer already told you.</p>
<blockquote>
<p><strong>TL;DR</strong>: Tokenizers are accidental forensic evidence. If <code>SolidGoldMagikarp</code> is a single token, that string appeared thousands of times in training. This is how researchers discovered GPT models trained on specific Reddit users, leaked databases, and private codebases.</p></blockquote>
<h2 id="the-smoking-gun-test">The Smoking Gun Test<a hidden class="anchor" aria-hidden="true" href="#the-smoking-gun-test">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Test if a model was trained on specific data:</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">training_set_forensics</span>(tokenizer, test_strings):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Detect what the model likely saw during training&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    results <span style="color:#f92672">=</span> {}
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> string <span style="color:#f92672">in</span> test_strings:
</span></span><span style="display:flex;"><span>        tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(string)
</span></span><span style="display:flex;"><span>        decoded <span style="color:#f92672">=</span> [tokenizer<span style="color:#f92672">.</span>decode([t]) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tokens]
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Single token = appeared frequently in training</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Many tokens = rare or unseen</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> len(tokens) <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>            status <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;üî¥ DEFINITELY in training (1000s of times)&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">elif</span> len(tokens) <span style="color:#f92672">==</span> <span style="color:#ae81ff">2</span>:
</span></span><span style="display:flex;"><span>            status <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;üü° Likely in training (100s of times)&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">elif</span> len(tokens) <span style="color:#f92672">&lt;=</span> len(string)<span style="color:#f92672">/</span><span style="color:#ae81ff">4</span>:
</span></span><span style="display:flex;"><span>            status <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;üü¢ Possibly in training&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            status <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;‚ö™ Probably NOT in training&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        results[string] <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;tokens&#39;</span>: len(tokens),
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;pieces&#39;</span>: decoded,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;status&#39;</span>: status
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> results
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Try these:</span>
</span></span><span style="display:flex;"><span>suspicious_strings <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;$AAPL&#34;</span>,           <span style="color:#75715e"># Apple stock</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;$TSLA&#34;</span>,           <span style="color:#75715e"># Tesla stock  </span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;$GME&#34;</span>,            <span style="color:#75715e"># GameStop (meme stock)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;SolidGoldMagikarp&#34;</span>,  <span style="color:#75715e"># Reddit username</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;petertodd&#34;</span>,       <span style="color:#75715e"># Bitcoin developer</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34; davidjl123&#34;</span>,     <span style="color:#75715e"># Another Reddit user</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;TheNitromeFan&#34;</span>,   <span style="color:#75715e"># Counting subreddit user</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&lt;|endoftext|&gt;&#34;</span>,   <span style="color:#75715e"># GPT special token</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;REDACTED_EMAIL&#34;</span>,  <span style="color:#75715e"># Your company&#39;s placeholder?</span>
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>results <span style="color:#f92672">=</span> training_set_forensics(tokenizer, suspicious_strings)
</span></span></code></pre></div><h2 id="the-reddit-conspiracy-its-real">The Reddit Conspiracy (It&rsquo;s Real)<a hidden class="anchor" aria-hidden="true" href="#the-reddit-conspiracy-its-real">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># These are ACTUAL single tokens in GPT tokenizers:</span>
</span></span><span style="display:flex;"><span>weird_tokens <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;SolidGoldMagikarp&#34;</span>: <span style="color:#ae81ff">1</span>,  <span style="color:#75715e"># Reddit user with 100K+ comments</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34; petertodd&#34;</span>: <span style="color:#ae81ff">1</span>,          <span style="color:#75715e"># Bitcoin developer mentioned everywhere</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;TheNitromeFan&#34;</span>: <span style="color:#ae81ff">1</span>,       <span style="color:#75715e"># Power user in r/counting</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34; davidjl123&#34;</span>: <span style="color:#ae81ff">1</span>,         <span style="color:#75715e"># Another counting enthusiast</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;cloneembryos&#34;</span>: <span style="color:#ae81ff">1</span>,        <span style="color:#75715e"># WTF? Specific subreddit discussions</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34; guiActiveUn&#34;</span>: <span style="color:#ae81ff">1</span>,        <span style="color:#75715e"># Unity game engine internal variable</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34; TPPStreamerBot&#34;</span>: <span style="color:#ae81ff">1</span>,     <span style="color:#75715e"># Twitch Plays Pokemon bot</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Why does this matter?</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. These users&#39; writing styles are BAKED into the model</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. Prompting with these tokens triggers specific behaviors</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. It&#39;s proof of training on Reddit data through 2021</span>
</span></span></code></pre></div><h2 id="the-domain-detector">The Domain Detector<a hidden class="anchor" aria-hidden="true" href="#the-domain-detector">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Compare tokenizers to detect training bias:</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compare_domain_coverage</span>(text, tokenizers_dict):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;See which model knows your domain best&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Testing: &#39;</span><span style="color:#e6db74">{</span>text<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;-&#34;</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">50</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    best_score <span style="color:#f92672">=</span> float(<span style="color:#e6db74">&#39;inf&#39;</span>)
</span></span><span style="display:flex;"><span>    best_model <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> name, tokenizer <span style="color:#f92672">in</span> tokenizers_dict<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>        tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(text)
</span></span><span style="display:flex;"><span>        decoded <span style="color:#f92672">=</span> [tokenizer<span style="color:#f92672">.</span>decode([t]) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tokens]
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>name<span style="color:#e6db74">:</span><span style="color:#e6db74">15</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> ‚Üí </span><span style="color:#e6db74">{</span>len(tokens)<span style="color:#e6db74">}</span><span style="color:#e6db74"> tokens: </span><span style="color:#e6db74">{</span>decoded<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> len(tokens) <span style="color:#f92672">&lt;</span> best_score:
</span></span><span style="display:flex;"><span>            best_score <span style="color:#f92672">=</span> len(tokens)
</span></span><span style="display:flex;"><span>            best_model <span style="color:#f92672">=</span> name
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">üèÜ </span><span style="color:#e6db74">{</span>best_model<span style="color:#e6db74">}</span><span style="color:#e6db74"> knows this domain best!&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> best_model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Real examples:</span>
</span></span><span style="display:flex;"><span>finance_test <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;$NVDA earnings beat expectations Q4&#34;</span>
</span></span><span style="display:flex;"><span>crypto_test <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;dogecoin hodl moon lambo wen&#34;</span>
</span></span><span style="display:flex;"><span>medical_test <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;acetaminophen hepatotoxicity cirrhosis&#34;</span>
</span></span><span style="display:flex;"><span>gaming_test <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;speedrun any</span><span style="color:#e6db74">% g</span><span style="color:#e6db74">litchless PB WR&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Results reveal training data:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># GPT-4: Good at finance (WSB in training)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Claude: Better at medical (PubMed heavy)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># LLaMA: Gaming terms fragmented (less Reddit)</span>
</span></span></code></pre></div><h2 id="the-corporate-leak-detector">The Corporate Leak Detector<a hidden class="anchor" aria-hidden="true" href="#the-corporate-leak-detector">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Check if your company&#39;s data leaked into training:</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>company_indicators <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Your internal codenames</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Project_Phantom&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;PROD_API_KEY_V2&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;TODO_FIXME_HACK&#34;</span>,
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Your employee usernames</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;jsmith@yourcompany&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;jenkins-bot-prod&#34;</span>,
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Your internal URLs</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;internal.yourcompany.com&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;staging-server-2&#34;</span>,
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Your error messages</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Error: Database connection failed (YourCompany v2.3.1)&#34;</span>,
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> indicator <span style="color:#f92672">in</span> company_indicators:
</span></span><span style="display:flex;"><span>    tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(indicator)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> len(tokens) <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">3</span>:  <span style="color:#75715e"># Suspiciously efficient</span>
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;‚ö†Ô∏è ALERT: &#39;</span><span style="color:#e6db74">{</span>indicator<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39; is </span><span style="color:#e6db74">{</span>len(tokens)<span style="color:#e6db74">}</span><span style="color:#e6db74"> tokens!&#34;</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;This string likely appeared in training data!&#34;</span>)
</span></span></code></pre></div><h2 id="the-easter-egg-hunt-glitch-tokens">The Easter Egg Hunt (Glitch Tokens)<a hidden class="anchor" aria-hidden="true" href="#the-easter-egg-hunt-glitch-tokens">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Some tokens cause BIZARRE behavior when used as prompts:</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>glitch_tokens <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;SolidGoldMagikarp&#34;</span>: <span style="color:#e6db74">&#34;Causes hallucinations about goldfish&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34; petertodd&#34;</span>: <span style="color:#e6db74">&#34;Triggers Bitcoin discussions&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;„Äâ„Äà&#34;</span>: <span style="color:#e6db74">&#34;Makes models output broken HTML&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34; Leilan&#34;</span>: <span style="color:#e6db74">&#34;Triggers anime/mythology crossover&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;PsyNetMessage&#34;</span>: <span style="color:#e6db74">&#34;Outputs Rocket League error messages&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34; embedreportprint&#34;</span>: <span style="color:#e6db74">&#34;Triggers debug output&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34; guiActiveUn&#34;</span>: <span style="color:#e6db74">&#34;Outputs Unity Engine internals&#34;</span>,
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Try this (at your own risk):</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># prompt = &#34;Tell me about SolidGoldMagikarp&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Models often respond with nonsense about goldfish, magic, or Reddit</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># WHY THIS HAPPENS:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. Token appears thousands of times in specific context</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. Model memorizes the pattern around it</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. Token becomes a &#34;trigger&#34; for that context</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 4. Using it alone causes context confusion</span>
</span></span></code></pre></div><h2 id="the-stock-market-tell">The Stock Market Tell<a hidden class="anchor" aria-hidden="true" href="#the-stock-market-tell">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Financial bias detection:</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stocks <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;$AAPL&#34;</span>, <span style="color:#e6db74">&#34;$GOOGL&#34;</span>, <span style="color:#e6db74">&#34;$TSLA&#34;</span>, <span style="color:#e6db74">&#34;$GME&#34;</span>, <span style="color:#e6db74">&#34;$AMC&#34;</span>, <span style="color:#e6db74">&#34;$BBBY&#34;</span>, <span style="color:#e6db74">&#34;$PLTR&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> stock <span style="color:#f92672">in</span> stocks:
</span></span><span style="display:flex;"><span>    tokens <span style="color:#f92672">=</span> len(tokenizer<span style="color:#f92672">.</span>encode(stock))
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> tokens <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>stock<span style="color:#e6db74">}</span><span style="color:#e6db74">: üìà Meme stock or high-frequency in training&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> tokens <span style="color:#f92672">==</span> <span style="color:#ae81ff">2</span>:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>stock<span style="color:#e6db74">}</span><span style="color:#e6db74">: üìä Common stock in financial data&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>stock<span style="color:#e6db74">}</span><span style="color:#e6db74">: üìâ Rare in training data&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># GPT models: $GME is often 1-2 tokens (WSB influence)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Claude: $GME is 3-4 tokens (less meme stock training)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This reveals:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># - GPT saw tons of WallStreetBets data</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># - Claude focused more on traditional finance</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># - Training cutoff dates (pre/post meme stock era)</span>
</span></span></code></pre></div><h2 id="the-privacy-nightmare">The Privacy Nightmare<a hidden class="anchor" aria-hidden="true" href="#the-privacy-nightmare">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">check_pii_leakage</span>(tokenizer, email_patterns):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Check if specific emails/usernames are in training&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    concerning <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> email <span style="color:#f92672">in</span> email_patterns:
</span></span><span style="display:flex;"><span>        tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(email)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># If an email is &lt;5 tokens, it appeared A LOT</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> len(tokens) <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">5</span>:
</span></span><span style="display:flex;"><span>            concerning<span style="color:#f92672">.</span>append({
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#39;email&#39;</span>: email,
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#39;tokens&#39;</span>: len(tokens),
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#39;risk&#39;</span>: <span style="color:#e6db74">&#39;HIGH - Likely memorized&#39;</span>
</span></span><span style="display:flex;"><span>            })
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> concerning
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Real researcher findings:</span>
</span></span><span style="display:flex;"><span>leaked_emails <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;support@company.com&#34;</span>,  <span style="color:#75715e"># 2 tokens - appeared thousands of times</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;john.doe.1234@gmail&#34;</span>,  <span style="color:#75715e"># 8 tokens - probably safe</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;ceo@fortune500.com&#34;</span>,   <span style="color:#75715e"># 3 tokens - concerning!</span>
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Researchers found actual email addresses as single tokens</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># meaning they appeared THOUSANDS of times in training</span>
</span></span></code></pre></div><h2 id="how-to-exploit-this-ethically">How to Exploit This (Ethically)<a hidden class="anchor" aria-hidden="true" href="#how-to-exploit-this-ethically">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TokenizerForensics</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Use tokenization to understand model capabilities&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">detect_programming_languages</span>(self, tokenizer):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Which languages did it see most?&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        code_snippets <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;Python&#39;</span>: <span style="color:#e6db74">&#39;def __init__(self):&#39;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;JavaScript&#39;</span>: <span style="color:#e6db74">&#39;const async = await&#39;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;Rust&#39;</span>: <span style="color:#e6db74">&#39;fn main() -&gt; Result&lt;&gt;&#39;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;Go&#39;</span>: <span style="color:#e6db74">&#39;func (r *Reader) Read()&#39;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#39;COBOL&#39;</span>: <span style="color:#e6db74">&#39;IDENTIFICATION DIVISION&#39;</span>,
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> lang, snippet <span style="color:#f92672">in</span> code_snippets<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>            tokens <span style="color:#f92672">=</span> len(tokenizer<span style="color:#f92672">.</span>encode(snippet))
</span></span><span style="display:flex;"><span>            efficiency <span style="color:#f92672">=</span> len(snippet) <span style="color:#f92672">/</span> tokens
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> efficiency <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">10</span>:
</span></span><span style="display:flex;"><span>                print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>lang<span style="color:#e6db74">}</span><span style="color:#e6db74">: Heavily trained on this&#34;</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">elif</span> efficiency <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">5</span>:
</span></span><span style="display:flex;"><span>                print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>lang<span style="color:#e6db74">}</span><span style="color:#e6db74">: Moderate training&#34;</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>lang<span style="color:#e6db74">}</span><span style="color:#e6db74">: Minimal training&#34;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">detect_website_training</span>(self, tokenizer):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Which websites were scraped?&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        sites <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;reddit.com/r/&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;stackoverflow.com/questions/&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;github.com/&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;arxiv.org/abs/&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;medium.com/@&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;substack.com/p/&#34;</span>,
</span></span><span style="display:flex;"><span>        ]
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> site <span style="color:#f92672">in</span> sites:
</span></span><span style="display:flex;"><span>            tokens <span style="color:#f92672">=</span> len(tokenizer<span style="color:#f92672">.</span>encode(site))
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> tokens <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">3</span>:
</span></span><span style="display:flex;"><span>                print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;‚úì </span><span style="color:#e6db74">{</span>site<span style="color:#e6db74">}</span><span style="color:#e6db74"> - Heavily scraped&#34;</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;‚úó </span><span style="color:#e6db74">{</span>site<span style="color:#e6db74">}</span><span style="color:#e6db74"> - Lightly scraped&#34;</span>)
</span></span></code></pre></div><h2 id="the-uncomfortable-implications">The Uncomfortable Implications<a hidden class="anchor" aria-hidden="true" href="#the-uncomfortable-implications">#</a></h2>
<ol>
<li>
<p><strong>Your Reddit posts are tokens</strong>: Specific users&rsquo; entire post histories are compressed into the model</p>
</li>
<li>
<p><strong>Corporate leaks are detectable</strong>: Internal codenames as single tokens = data breach</p>
</li>
<li>
<p><strong>Model capabilities are predictable</strong>: Bad tokenization = bad performance in that domain</p>
</li>
<li>
<p><strong>Privacy is already broken</strong>: Email addresses, usernames, and phone numbers exist as tokens</p>
</li>
<li>
<p><strong>Temporal information leaks</strong>: Token efficiency reveals WHEN data was collected</p>
</li>
</ol>
<hr>
<blockquote>
<p><strong>üí° Action Item</strong>: Run <code>training_set_forensics()</code> on your company&rsquo;s internal terminology. If anything is ‚â§3 tokens, you might have a leak. Check your competitor&rsquo;s terminology too, you might discover their training data sources.</p></blockquote>
<hr>
<p><strong>Takeaway:</strong> Tokenizers are inadvertent forensic evidence. Every merge decision is a fossil record of the training data. Single tokens for usernames, stock symbols, or internal codenames aren&rsquo;t bugs, they&rsquo;re proof of what the model memorized. Use this for good (detecting capabilities) or evil (finding leaks), but know that the tokenizer already spilled the secrets.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lucven.com/tags/tokenisation/">Tokenisation</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://lucven.com/posts/tokenization/tokenisation_limits/">
    <span class="title">¬´ Prev</span>
    <br>
    <span>Why Your Vector Database Thinks $AAPL Means Polish Batteries</span>
  </a>
  <a class="next" href="https://lucven.com/posts/tokenization/byte-level-tokenizer/">
    <span class="title">Next ¬ª</span>
    <br>
    <span>Byte Level Tokenizer</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tokenization Forensics about Leaks on x"
            href="https://x.com/intent/tweet/?text=Tokenization%20Forensics%20about%20Leaks&amp;url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2fforensics%2f&amp;hashtags=Tokenisation">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tokenization Forensics about Leaks on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2fforensics%2f&amp;title=Tokenization%20Forensics%20about%20Leaks&amp;summary=Tokenization%20Forensics%20about%20Leaks&amp;source=https%3a%2f%2flucven.com%2fposts%2ftokenization%2fforensics%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tokenization Forensics about Leaks on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2fforensics%2f&title=Tokenization%20Forensics%20about%20Leaks">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tokenization Forensics about Leaks on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flucven.com%2fposts%2ftokenization%2fforensics%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tokenization Forensics about Leaks on whatsapp"
            href="https://api.whatsapp.com/send?text=Tokenization%20Forensics%20about%20Leaks%20-%20https%3a%2f%2flucven.com%2fposts%2ftokenization%2fforensics%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tokenization Forensics about Leaks on telegram"
            href="https://telegram.me/share/url?text=Tokenization%20Forensics%20about%20Leaks&amp;url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2fforensics%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tokenization Forensics about Leaks on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Tokenization%20Forensics%20about%20Leaks&u=https%3a%2f%2flucven.com%2fposts%2ftokenization%2fforensics%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer><script src="https://giscus.app/client.js"
  data-repo="lakshaychhabra/lucven-comments"
  data-repo-id="R_kgDOPYOEYw"
  data-category="Q&A"
  data-category-id="DIC_kwDOPYOEY84CtxWt"
  data-mapping="pathname"
  data-strict="0"
  data-reactions-enabled="1"
  data-emit-metadata="0"
  data-input-position="top"
  data-theme="preferred_color_scheme"
  data-lang="en"
  data-loading="lazy"
  crossorigin="anonymous"
  async>
</script>


</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://lucven.com/">Lucven AI</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
