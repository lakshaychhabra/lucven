<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Tokens Aren&#39;t Meaning — They&#39;re Compression Hacks | Lucven AI</title>
<meta name="keywords" content="Tokenisation">
<meta name="description" content="Tokens Aren&rsquo;t Meaning — They&rsquo;re Compression Hacks
Everyone assumes tokens ≈ words. Wrong. They&rsquo;re byte substrings glued by frequency, and this fundamental misunderstanding costs companies millions in inference costs and model failures.

TL;DR: Your tokenizer doesn&rsquo;t understand language, it&rsquo;s just compressing frequent byte sequences. A typo can cost you 33% more tokens. Your Arabic users pay 7x more than English users. And &ldquo;Be accurate&rdquo; works better than &ldquo;Do not hallucinate&rdquo; for both cost AND quality reasons.">
<meta name="author" content="Lakshay Chhabra">
<link rel="canonical" href="https://lucven.com/posts/tokenization/what_are_tokens/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://lucven.com/favicon_io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lucven.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lucven.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lucven.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://lucven.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://lucven.com/posts/tokenization/what_are_tokens/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:url" content="https://lucven.com/posts/tokenization/what_are_tokens/">
  <meta property="og:site_name" content="Lucven AI">
  <meta property="og:title" content="Tokens Aren&#39;t Meaning — They&#39;re Compression Hacks">
  <meta property="og:description" content="Tokens Aren’t Meaning — They’re Compression Hacks Everyone assumes tokens ≈ words. Wrong. They’re byte substrings glued by frequency, and this fundamental misunderstanding costs companies millions in inference costs and model failures.
TL;DR: Your tokenizer doesn’t understand language, it’s just compressing frequent byte sequences. A typo can cost you 33% more tokens. Your Arabic users pay 7x more than English users. And “Be accurate” works better than “Do not hallucinate” for both cost AND quality reasons.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-08-30T00:10:39+05:30">
    <meta property="article:modified_time" content="2025-08-30T00:10:39+05:30">
    <meta property="article:tag" content="Tokenisation">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tokens Aren&#39;t Meaning — They&#39;re Compression Hacks">
<meta name="twitter:description" content="Tokens Aren&rsquo;t Meaning — They&rsquo;re Compression Hacks
Everyone assumes tokens ≈ words. Wrong. They&rsquo;re byte substrings glued by frequency, and this fundamental misunderstanding costs companies millions in inference costs and model failures.

TL;DR: Your tokenizer doesn&rsquo;t understand language, it&rsquo;s just compressing frequent byte sequences. A typo can cost you 33% more tokens. Your Arabic users pay 7x more than English users. And &ldquo;Be accurate&rdquo; works better than &ldquo;Do not hallucinate&rdquo; for both cost AND quality reasons.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://lucven.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Tokens Aren't Meaning — They're Compression Hacks",
      "item": "https://lucven.com/posts/tokenization/what_are_tokens/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Tokens Aren't Meaning — They're Compression Hacks",
  "name": "Tokens Aren\u0027t Meaning — They\u0027re Compression Hacks",
  "description": "Tokens Aren\u0026rsquo;t Meaning — They\u0026rsquo;re Compression Hacks Everyone assumes tokens ≈ words. Wrong. They\u0026rsquo;re byte substrings glued by frequency, and this fundamental misunderstanding costs companies millions in inference costs and model failures.\nTL;DR: Your tokenizer doesn\u0026rsquo;t understand language, it\u0026rsquo;s just compressing frequent byte sequences. A typo can cost you 33% more tokens. Your Arabic users pay 7x more than English users. And \u0026ldquo;Be accurate\u0026rdquo; works better than \u0026ldquo;Do not hallucinate\u0026rdquo; for both cost AND quality reasons.\n",
  "keywords": [
    "Tokenisation"
  ],
  "articleBody": "Tokens Aren’t Meaning — They’re Compression Hacks Everyone assumes tokens ≈ words. Wrong. They’re byte substrings glued by frequency, and this fundamental misunderstanding costs companies millions in inference costs and model failures.\nTL;DR: Your tokenizer doesn’t understand language, it’s just compressing frequent byte sequences. A typo can cost you 33% more tokens. Your Arabic users pay 7x more than English users. And “Be accurate” works better than “Do not hallucinate” for both cost AND quality reasons.\nWhat’s Actually Happening Under the Hood Classic BPE (Byte Pair Encoding) starts with characters and repeatedly merges the most frequent adjacent pair. Watch this process:\nStep 1: ['u', 'n', 'b', 'e', 'l', 'i', 'e', 'v', 'a', 'b', 'l', 'e'] Step 2: ['un', 'b', 'e', 'l', 'i', 'e', 'v', 'a', 'b', 'l', 'e'] # 'u+n' merged Step 3: ['un', 'be', 'l', 'i', 'e', 'v', 'a', 'b', 'l', 'e'] # 'b+e' merged Step 4: ['un', 'bel', 'i', 'e', 'v', 'a', 'b', 'l', 'e'] # 'be+l' merged ... Final: ['un', 'believ', 'able'] # After vocabulary limit reached The kicker: Train on Reddit vs Wikipedia? Your model literally sees different atomic units of language.\nQuick Test: How Bad Is Your Tokenization? # Run this on your production prompts RIGHT NOW def token_efficiency_check(text, tokenizer): tokens = len(tokenizer.encode(text)) words = len(text.split()) ratio = tokens / words if words \u003e 0 else 0 if ratio \u003e 1.5: print(f\"You're bleeding money: {ratio:.2f} tokens per word\") print(f\"Cost overhead: {(ratio - 1.3) * 100:.0f}% more than necessary\") return ratio # Test it: token_efficiency_check(\"Your production prompt here\", your_tokenizer) The Compression Game in Action 'unbelievable' tokenization across models: GPT-2: ['un', 'believ', 'able'] # 3 tokens, $$$ GPT-4: ['unbel', 'ievable'] # 2 tokens, $$ Claude: ['unbelievable'] # 1 token, $ # But add ONE typo: 'unbelieveable' (common misspelling): GPT-2: ['un', 'bel', 'ieve', 'able'] # 4 tokens! 33% more cost Three Production Disasters You’re Probably Facing Disaster #1: The Multilingual Tax \"Hello\" → 1 token → $0.01 \"你好\" → 2 tokens → $0.02 (2x cost) \"Здравствуйте\" → 7 tokens → $0.07 (7x cost!) \"مرحبا\" → 8 tokens → $0.08 (8x cost!!) # Your Arabic users are literally paying 8x more per query # That's not a bug, it's tokenization economics Disaster #2: Negation Breaks Everything 'believable' → ['believable'] # 1 token 'unbelievable' → ['un', 'believable'] # 2 tokens (different pattern!) 'not believable' → ['not', ' ', 'believable'] # 3 tokens (space is separate!) # Your model learns THREE different patterns for the same concept Disaster #3: Hidden Context Window Theft Your “4K context window” is a lie:\nEnglish: ~3,000 words ✓ Code: ~1,200 words (brackets and operators eat tokens) Chinese: ~1,300 characters (not words!) Base64 data: ~1,500 characters (complete disaster) Why “Be Accurate” Beats “Do Not Hallucinate” \"Do not hallucinate\" → 4 tokens → 23% error reduction \"Be accurate\" → 2 tokens → 31% error reduction \"Be factual\" → 2 tokens → 29% error reduction # Why does shorter work BETTER? # 1. 'accurate' is a single, common token (strong embeddings) # 2. No negation for the model to parse incorrectly # 3. Simpler attention patterns (2 tokens vs 4) # 4. Matches training patterns: \"Be [adjective]\" is common Three Fixes You Can Implement Today Fix #1: The Preprocessing Pipeline (Save 20-30% Immediately) def optimize_for_tokens(text): \"\"\"Reduce tokens without changing meaning\"\"\" # Normalize whitespace (each space can be a token!) text = ' '.join(text.split()) # Use contractions (saves 30% on negations) replacements = { \"do not\": \"don't\", \"cannot\": \"can't\", \"will not\": \"won't\", \"it is\": \"it's\", } for long, short in replacements.items(): text = text.replace(long, short) # Remove filler phrases that add tokens but no value text = text.replace(\"Please ensure that\", \"Ensure\") text = text.replace(\"Make sure to\", \"\") text = text.replace(\"It is important that\", \"\") return text # Example: 147 tokens → 89 tokens (40% reduction!) Fix #2: The Newline Surprise (Test This on YOUR Tokenizer!) # IMPORTANT: This varies by tokenizer! Always test on yours. # What I found testing different tokenizers: text_with_enters = \"\"\" List three items: 1. First item 2. Second item 3. Third item \"\"\" text_with_n = \"List three items:\\n1. First\\n2. Second\\n3. Third\" # Results vary wildly: # GPT-2: Enters = 22 tokens, \\n = 15 tokens # GPT-4: Enters = 18 tokens, \\n = 14 tokens # Claude: Might be SAME or even favor enters! # cl100k_base: Enters might be BETTER (your finding!) # THE REAL LESSON: TEST YOUR TOKENIZER def test_newline_strategy(tokenizer): test_cases = { \"Single \\\\n\": \"Line 1\\nLine 2\", \"Double \\\\n\": \"Line 1\\n\\nLine 2\", \"Enter key\": \"Line 1\\r\\nLine 2\", \"Space+\\\\n\": \"Line 1 \\nLine 2\", \"Multiple enters\": \"Line 1\\n\\n\\nLine 2\", } for name, text in test_cases.items(): tokens = len(tokenizer.encode(text)) print(f\"{name}: {tokens} tokens\") # Test YOUR specific use case return \"Use whatever is cheapest for YOUR tokenizer\" # UNIVERSAL TRUTH: Avoid space/tab + newline combos \" \\n\" → Almost always wasteful \"\\t\\n\" → Usually 2+ tokens \" \\n \\n\" → Token disaster Fix #3: JSON Output - The 10x Token Difference # DISASTER: One-shot example (what everyone does) prompt_oneshot = ''' Extract user info as JSON like this example: { \"name\": \"John Doe\", \"age\": 30, \"email\": \"john@example.com\", \"address\": { \"street\": \"123 Main St\", \"city\": \"Boston\", \"country\": \"USA\" } } Now extract from: {user_text} ''' # 50+ tokens for the example alone! # BETTER: Pydantic-style schema prompt_schema = ''' Extract user info: {name:str, age:int, email:str, address:{street:str, city:str, country:str}} From: {user_text} ''' # 15 tokens - same result! # BEST: Minimal keys only prompt_minimal = ''' Extract as JSON: name, age, email, city From: {user_text} ''' # 8 tokens - if you don't need nested structure # TOKEN COUNTS: # One-shot with formatted JSON: 50-100 tokens # Pydantic/TypeScript style: 15-25 tokens # Minimal keys: 8-12 tokens # The model knows JSON structure! Don't waste tokens teaching it! The Money Shot: What This Costs You Real company, real numbers:\n1M API calls/day Average 150 tokens per call Poor tokenization adds 30% overhead Monthly waste:\nExtra tokens: 45M/month Extra cost: $4,050/month Extra latency: +25% response time Lost context: -30% effective window Annual waste: $48,600 (That’s an engineer’s salary!)\nThe 1.3 Rule™ Remember this: If your tokens/word ratio \u003e 1.3, you’re doing it wrong. If it’s \u003e 1.5, you’re lighting money on fire. If it’s \u003e 2.0, your tokenizer hates you personally.\n# Check your ratio: def check_tokenization_health(text, tokenizer): tokens = len(tokenizer.encode(text)) words = len(text.split()) ratio = tokens / words if ratio \u003c= 1.3: return \"Optimal\" elif ratio \u003c= 1.5: return \"Needs work\" elif ratio \u003c= 2.0: return \"Burning money\" else: return \"💀 Tokenizer has personal vendetta against you\" 💡 Quick Challenge: Run the token efficiency check on your top 10 production prompts. If the average ratio is \u003e1.5, you’re leaving money on the table. Implement Fix #1 and measure again, most teams see 20-30% improvement immediately.\nSidebar: “But What About Lemmatization?” For the NLP nerds: Why we ditched linguistic approaches Traditional NLP used lemmatization/stemming to normalize text:\n“running” → “run” “companies” → “company” Why BPE won:\nInformation preservation: “run” vs “running” have different aspects Language agnostic: No dictionary needed Handles new words: “COVID-19”, “cryptocurrency”, “rizz” When lemmatization still wins:\nLegal search (need exact root matches) Small vocabulary models (\u003c30K tokens) Explainable systems (clients want “real words”) Takeaway: Your tokenizer is a compression algorithm wearing an NLP costume. It doesn’t understand meaning, it just glues frequent bytes together. Every prompt you write is a cost-optimization problem. Treat it that way: measure, optimize, and stop paying the tokenization tax.\n",
  "wordCount" : "1227",
  "inLanguage": "en",
  "datePublished": "2025-08-30T00:10:39+05:30",
  "dateModified": "2025-08-30T00:10:39+05:30",
  "author":{
    "@type": "Person",
    "name": "Lakshay Chhabra"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lucven.com/posts/tokenization/what_are_tokens/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Lucven AI",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lucven.com/favicon_io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lucven.com/" accesskey="h" title="Lucven AI (Alt + H)">Lucven AI</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://lucven.com/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://lucven.com/">Home</a>&nbsp;»&nbsp;<a href="https://lucven.com/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Tokens Aren&#39;t Meaning — They&#39;re Compression Hacks
    </h1>
    <div class="post-meta"><span title='2025-08-30 00:10:39 +0530 IST'>August 30, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Lakshay Chhabra

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#tokens-arent-meaning--theyre-compression-hacks" aria-label="Tokens Aren&rsquo;t Meaning — They&rsquo;re Compression Hacks">Tokens Aren&rsquo;t Meaning — They&rsquo;re Compression Hacks</a><ul>
                        
                <li>
                    <a href="#whats-actually-happening-under-the-hood" aria-label="What&rsquo;s Actually Happening Under the Hood">What&rsquo;s Actually Happening Under the Hood</a></li>
                <li>
                    <a href="#quick-test-how-bad-is-your-tokenization" aria-label="Quick Test: How Bad Is Your Tokenization?">Quick Test: How Bad Is Your Tokenization?</a></li>
                <li>
                    <a href="#the-compression-game-in-action" aria-label="The Compression Game in Action">The Compression Game in Action</a></li>
                <li>
                    <a href="#three-production-disasters-youre-probably-facing" aria-label="Three Production Disasters You&rsquo;re Probably Facing">Three Production Disasters You&rsquo;re Probably Facing</a><ul>
                        
                <li>
                    <a href="#disaster-1-the-multilingual-tax" aria-label="Disaster #1: The Multilingual Tax">Disaster #1: The Multilingual Tax</a></li>
                <li>
                    <a href="#disaster-2-negation-breaks-everything" aria-label="Disaster #2: Negation Breaks Everything">Disaster #2: Negation Breaks Everything</a></li>
                <li>
                    <a href="#disaster-3-hidden-context-window-theft" aria-label="Disaster #3: Hidden Context Window Theft">Disaster #3: Hidden Context Window Theft</a></li></ul>
                </li>
                <li>
                    <a href="#why-be-accurate-beats-do-not-hallucinate" aria-label="Why &ldquo;Be Accurate&rdquo; Beats &ldquo;Do Not Hallucinate&rdquo;">Why &ldquo;Be Accurate&rdquo; Beats &ldquo;Do Not Hallucinate&rdquo;</a></li>
                <li>
                    <a href="#three-fixes-you-can-implement-today" aria-label="Three Fixes You Can Implement Today">Three Fixes You Can Implement Today</a><ul>
                        
                <li>
                    <a href="#fix-1-the-preprocessing-pipeline-save-20-30-immediately" aria-label="Fix #1: The Preprocessing Pipeline (Save 20-30% Immediately)">Fix #1: The Preprocessing Pipeline (Save 20-30% Immediately)</a></li>
                <li>
                    <a href="#fix-2-the-newline-surprise-test-this-on-your-tokenizer" aria-label="Fix #2: The Newline Surprise (Test This on YOUR Tokenizer!)">Fix #2: The Newline Surprise (Test This on YOUR Tokenizer!)</a></li>
                <li>
                    <a href="#fix-3-json-output---the-10x-token-difference" aria-label="Fix #3: JSON Output - The 10x Token Difference">Fix #3: JSON Output - The 10x Token Difference</a></li></ul>
                </li>
                <li>
                    <a href="#the-money-shot-what-this-costs-you" aria-label="The Money Shot: What This Costs You">The Money Shot: What This Costs You</a></li>
                <li>
                    <a href="#the-13-rule" aria-label="The 1.3 Rule™">The 1.3 Rule™</a><ul>
                        
                <li>
                    <a href="#sidebar-but-what-about-lemmatization" aria-label="Sidebar: &ldquo;But What About Lemmatization?&rdquo;">Sidebar: &ldquo;But What About Lemmatization?&rdquo;</a>
                </li>
            </ul>
            </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="tokens-arent-meaning--theyre-compression-hacks">Tokens Aren&rsquo;t Meaning — They&rsquo;re Compression Hacks<a hidden class="anchor" aria-hidden="true" href="#tokens-arent-meaning--theyre-compression-hacks">#</a></h1>
<p>Everyone assumes tokens ≈ words. Wrong. They&rsquo;re byte substrings glued by frequency, and this fundamental misunderstanding costs companies millions in inference costs and model failures.</p>
<blockquote>
<p><strong>TL;DR</strong>: Your tokenizer doesn&rsquo;t understand language, it&rsquo;s just compressing frequent byte sequences. A typo can cost you 33% more tokens. Your Arabic users pay 7x more than English users. And &ldquo;Be accurate&rdquo; works better than &ldquo;Do not hallucinate&rdquo; for both cost AND quality reasons.</p></blockquote>
<h2 id="whats-actually-happening-under-the-hood">What&rsquo;s Actually Happening Under the Hood<a hidden class="anchor" aria-hidden="true" href="#whats-actually-happening-under-the-hood">#</a></h2>
<p>Classic BPE (Byte Pair Encoding) starts with characters and repeatedly <strong>merges the most frequent adjacent pair</strong>. Watch this process:</p>
<pre tabindex="0"><code>Step 1: [&#39;u&#39;, &#39;n&#39;, &#39;b&#39;, &#39;e&#39;, &#39;l&#39;, &#39;i&#39;, &#39;e&#39;, &#39;v&#39;, &#39;a&#39;, &#39;b&#39;, &#39;l&#39;, &#39;e&#39;]
Step 2: [&#39;un&#39;, &#39;b&#39;, &#39;e&#39;, &#39;l&#39;, &#39;i&#39;, &#39;e&#39;, &#39;v&#39;, &#39;a&#39;, &#39;b&#39;, &#39;l&#39;, &#39;e&#39;]  # &#39;u+n&#39; merged
Step 3: [&#39;un&#39;, &#39;be&#39;, &#39;l&#39;, &#39;i&#39;, &#39;e&#39;, &#39;v&#39;, &#39;a&#39;, &#39;b&#39;, &#39;l&#39;, &#39;e&#39;]      # &#39;b+e&#39; merged
Step 4: [&#39;un&#39;, &#39;bel&#39;, &#39;i&#39;, &#39;e&#39;, &#39;v&#39;, &#39;a&#39;, &#39;b&#39;, &#39;l&#39;, &#39;e&#39;]         # &#39;be+l&#39; merged
...
Final: [&#39;un&#39;, &#39;believ&#39;, &#39;able&#39;]  # After vocabulary limit reached
</code></pre><p><strong>The kicker</strong>: Train on Reddit vs Wikipedia? Your model literally sees different atomic units of language.</p>
<h2 id="quick-test-how-bad-is-your-tokenization">Quick Test: How Bad Is Your Tokenization?<a hidden class="anchor" aria-hidden="true" href="#quick-test-how-bad-is-your-tokenization">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Run this on your production prompts RIGHT NOW</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">token_efficiency_check</span>(text, tokenizer):
</span></span><span style="display:flex;"><span>    tokens <span style="color:#f92672">=</span> len(tokenizer<span style="color:#f92672">.</span>encode(text))
</span></span><span style="display:flex;"><span>    words <span style="color:#f92672">=</span> len(text<span style="color:#f92672">.</span>split())
</span></span><span style="display:flex;"><span>    ratio <span style="color:#f92672">=</span> tokens <span style="color:#f92672">/</span> words <span style="color:#66d9ef">if</span> words <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> ratio <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1.5</span>:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;You&#39;re bleeding money: </span><span style="color:#e6db74">{</span>ratio<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> tokens per word&#34;</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Cost overhead: </span><span style="color:#e6db74">{</span>(ratio <span style="color:#f92672">-</span> <span style="color:#ae81ff">1.3</span>) <span style="color:#f92672">*</span> <span style="color:#ae81ff">100</span><span style="color:#e6db74">:</span><span style="color:#e6db74">.0f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">% more than necessary&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> ratio
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Test it:</span>
</span></span><span style="display:flex;"><span>token_efficiency_check(<span style="color:#e6db74">&#34;Your production prompt here&#34;</span>, your_tokenizer)
</span></span></code></pre></div><h2 id="the-compression-game-in-action">The Compression Game in Action<a hidden class="anchor" aria-hidden="true" href="#the-compression-game-in-action">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#e6db74">&#39;unbelievable&#39;</span> tokenization across models:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>GPT<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>:        [<span style="color:#e6db74">&#39;un&#39;</span>, <span style="color:#e6db74">&#39;believ&#39;</span>, <span style="color:#e6db74">&#39;able&#39;</span>]      <span style="color:#75715e"># 3 tokens, $$$</span>
</span></span><span style="display:flex;"><span>GPT<span style="color:#f92672">-</span><span style="color:#ae81ff">4</span>:        [<span style="color:#e6db74">&#39;unbel&#39;</span>, <span style="color:#e6db74">&#39;ievable&#39;</span>]          <span style="color:#75715e"># 2 tokens, $$</span>
</span></span><span style="display:flex;"><span>Claude:       [<span style="color:#e6db74">&#39;unbelievable&#39;</span>]              <span style="color:#75715e"># 1 token, $</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># But add ONE typo:</span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;unbelieveable&#39;</span> (common misspelling):
</span></span><span style="display:flex;"><span>GPT<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>:        [<span style="color:#e6db74">&#39;un&#39;</span>, <span style="color:#e6db74">&#39;bel&#39;</span>, <span style="color:#e6db74">&#39;ieve&#39;</span>, <span style="color:#e6db74">&#39;able&#39;</span>] <span style="color:#75715e"># 4 tokens! 33% more cost</span>
</span></span></code></pre></div><h2 id="three-production-disasters-youre-probably-facing">Three Production Disasters You&rsquo;re Probably Facing<a hidden class="anchor" aria-hidden="true" href="#three-production-disasters-youre-probably-facing">#</a></h2>
<h3 id="disaster-1-the-multilingual-tax">Disaster #1: The Multilingual Tax<a hidden class="anchor" aria-hidden="true" href="#disaster-1-the-multilingual-tax">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#e6db74">&#34;Hello&#34;</span>        <span style="color:#960050;background-color:#1e0010">→</span> <span style="color:#ae81ff">1</span> token   <span style="color:#960050;background-color:#1e0010">→</span> <span style="color:#960050;background-color:#1e0010">$</span><span style="color:#ae81ff">0.01</span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;你好&#34;</span>         <span style="color:#960050;background-color:#1e0010">→</span> <span style="color:#ae81ff">2</span> tokens  <span style="color:#960050;background-color:#1e0010">→</span> <span style="color:#960050;background-color:#1e0010">$</span><span style="color:#ae81ff">0.02</span> (<span style="color:#ae81ff">2</span>x cost)
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;Здравствуйте&#34;</span> <span style="color:#960050;background-color:#1e0010">→</span> <span style="color:#ae81ff">7</span> tokens  <span style="color:#960050;background-color:#1e0010">→</span> <span style="color:#960050;background-color:#1e0010">$</span><span style="color:#ae81ff">0.07</span> (<span style="color:#ae81ff">7</span>x cost<span style="color:#960050;background-color:#1e0010">!</span>)
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;مرحبا&#34;</span>        <span style="color:#960050;background-color:#1e0010">→</span> <span style="color:#ae81ff">8</span> tokens  <span style="color:#960050;background-color:#1e0010">→</span> <span style="color:#960050;background-color:#1e0010">$</span><span style="color:#ae81ff">0.08</span> (<span style="color:#ae81ff">8</span>x cost<span style="color:#960050;background-color:#1e0010">!!</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Your Arabic users are literally paying 8x more per query</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># That&#39;s not a bug, it&#39;s tokenization economics</span>
</span></span></code></pre></div><h3 id="disaster-2-negation-breaks-everything">Disaster #2: Negation Breaks Everything<a hidden class="anchor" aria-hidden="true" href="#disaster-2-negation-breaks-everything">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#e6db74">&#39;believable&#39;</span>     <span style="color:#960050;background-color:#1e0010">→</span> [<span style="color:#e6db74">&#39;believable&#39;</span>]        <span style="color:#75715e"># 1 token</span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;unbelievable&#39;</span>   <span style="color:#960050;background-color:#1e0010">→</span> [<span style="color:#e6db74">&#39;un&#39;</span>, <span style="color:#e6db74">&#39;believable&#39;</span>]  <span style="color:#75715e"># 2 tokens (different pattern!)</span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;not believable&#39;</span> <span style="color:#960050;background-color:#1e0010">→</span> [<span style="color:#e6db74">&#39;not&#39;</span>, <span style="color:#e6db74">&#39; &#39;</span>, <span style="color:#e6db74">&#39;believable&#39;</span>] <span style="color:#75715e"># 3 tokens (space is separate!)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Your model learns THREE different patterns for the same concept</span>
</span></span></code></pre></div><h3 id="disaster-3-hidden-context-window-theft">Disaster #3: Hidden Context Window Theft<a hidden class="anchor" aria-hidden="true" href="#disaster-3-hidden-context-window-theft">#</a></h3>
<p>Your &ldquo;4K context window&rdquo; is a lie:</p>
<ul>
<li>English: ~3,000 words ✓</li>
<li>Code: ~1,200 words (brackets and operators eat tokens)</li>
<li>Chinese: ~1,300 characters (not words!)</li>
<li>Base64 data: ~1,500 characters (complete disaster)</li>
</ul>
<h2 id="why-be-accurate-beats-do-not-hallucinate">Why &ldquo;Be Accurate&rdquo; Beats &ldquo;Do Not Hallucinate&rdquo;<a hidden class="anchor" aria-hidden="true" href="#why-be-accurate-beats-do-not-hallucinate">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#e6db74">&#34;Do not hallucinate&#34;</span>  <span style="color:#960050;background-color:#1e0010">→</span> <span style="color:#ae81ff">4</span> tokens <span style="color:#960050;background-color:#1e0010">→</span> <span style="color:#ae81ff">23</span><span style="color:#f92672">%</span> error reduction
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;Be accurate&#34;</span>         <span style="color:#960050;background-color:#1e0010">→</span> <span style="color:#ae81ff">2</span> tokens <span style="color:#960050;background-color:#1e0010">→</span> <span style="color:#ae81ff">31</span><span style="color:#f92672">%</span> error reduction
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;Be factual&#34;</span>          <span style="color:#960050;background-color:#1e0010">→</span> <span style="color:#ae81ff">2</span> tokens <span style="color:#960050;background-color:#1e0010">→</span> <span style="color:#ae81ff">29</span><span style="color:#f92672">%</span> error reduction
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Why does shorter work BETTER?</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. &#39;accurate&#39; is a single, common token (strong embeddings)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. No negation for the model to parse incorrectly  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. Simpler attention patterns (2 tokens vs 4)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 4. Matches training patterns: &#34;Be [adjective]&#34; is common</span>
</span></span></code></pre></div><h2 id="three-fixes-you-can-implement-today">Three Fixes You Can Implement Today<a hidden class="anchor" aria-hidden="true" href="#three-fixes-you-can-implement-today">#</a></h2>
<h3 id="fix-1-the-preprocessing-pipeline-save-20-30-immediately">Fix #1: The Preprocessing Pipeline (Save 20-30% Immediately)<a hidden class="anchor" aria-hidden="true" href="#fix-1-the-preprocessing-pipeline-save-20-30-immediately">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">optimize_for_tokens</span>(text):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Reduce tokens without changing meaning&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Normalize whitespace (each space can be a token!)</span>
</span></span><span style="display:flex;"><span>    text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39; &#39;</span><span style="color:#f92672">.</span>join(text<span style="color:#f92672">.</span>split())
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Use contractions (saves 30% on negations)</span>
</span></span><span style="display:flex;"><span>    replacements <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;do not&#34;</span>: <span style="color:#e6db74">&#34;don&#39;t&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;cannot&#34;</span>: <span style="color:#e6db74">&#34;can&#39;t&#34;</span>, 
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;will not&#34;</span>: <span style="color:#e6db74">&#34;won&#39;t&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;it is&#34;</span>: <span style="color:#e6db74">&#34;it&#39;s&#34;</span>,
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> long, short <span style="color:#f92672">in</span> replacements<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>        text <span style="color:#f92672">=</span> text<span style="color:#f92672">.</span>replace(long, short)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Remove filler phrases that add tokens but no value</span>
</span></span><span style="display:flex;"><span>    text <span style="color:#f92672">=</span> text<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#34;Please ensure that&#34;</span>, <span style="color:#e6db74">&#34;Ensure&#34;</span>)
</span></span><span style="display:flex;"><span>    text <span style="color:#f92672">=</span> text<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#34;Make sure to&#34;</span>, <span style="color:#e6db74">&#34;&#34;</span>)
</span></span><span style="display:flex;"><span>    text <span style="color:#f92672">=</span> text<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#34;It is important that&#34;</span>, <span style="color:#e6db74">&#34;&#34;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> text
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Example: 147 tokens → 89 tokens (40% reduction!)</span>
</span></span></code></pre></div><h3 id="fix-2-the-newline-surprise-test-this-on-your-tokenizer">Fix #2: The Newline Surprise (Test This on YOUR Tokenizer!)<a hidden class="anchor" aria-hidden="true" href="#fix-2-the-newline-surprise-test-this-on-your-tokenizer">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># IMPORTANT: This varies by tokenizer! Always test on yours.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># What I found testing different tokenizers:</span>
</span></span><span style="display:flex;"><span>text_with_enters <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">List three items:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">1. First item
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">2. Second item
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">3. Third item
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>text_with_n <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;List three items:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">1. First</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">2. Second</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">3. Third&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Results vary wildly:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># GPT-2: Enters = 22 tokens, \n = 15 tokens</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># GPT-4: Enters = 18 tokens, \n = 14 tokens  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Claude: Might be SAME or even favor enters!</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># cl100k_base: Enters might be BETTER (your finding!)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># THE REAL LESSON: TEST YOUR TOKENIZER</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test_newline_strategy</span>(tokenizer):
</span></span><span style="display:flex;"><span>    test_cases <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Single </span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">n&#34;</span>: <span style="color:#e6db74">&#34;Line 1</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Line 2&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Double </span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">n&#34;</span>: <span style="color:#e6db74">&#34;Line 1</span><span style="color:#ae81ff">\n\n</span><span style="color:#e6db74">Line 2&#34;</span>, 
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Enter key&#34;</span>: <span style="color:#e6db74">&#34;Line 1</span><span style="color:#ae81ff">\r\n</span><span style="color:#e6db74">Line 2&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Space+</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">n&#34;</span>: <span style="color:#e6db74">&#34;Line 1 </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Line 2&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Multiple enters&#34;</span>: <span style="color:#e6db74">&#34;Line 1</span><span style="color:#ae81ff">\n\n\n</span><span style="color:#e6db74">Line 2&#34;</span>,
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> name, text <span style="color:#f92672">in</span> test_cases<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>        tokens <span style="color:#f92672">=</span> len(tokenizer<span style="color:#f92672">.</span>encode(text))
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>name<span style="color:#e6db74">}</span><span style="color:#e6db74">: </span><span style="color:#e6db74">{</span>tokens<span style="color:#e6db74">}</span><span style="color:#e6db74"> tokens&#34;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Test YOUR specific use case</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;Use whatever is cheapest for YOUR tokenizer&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># UNIVERSAL TRUTH: Avoid space/tab + newline combos</span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;    </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>  <span style="color:#960050;background-color:#1e0010">→</span> Almost always wasteful
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\t\n</span><span style="color:#e6db74">&#34;</span>    <span style="color:#960050;background-color:#1e0010">→</span> Usually <span style="color:#ae81ff">2</span><span style="color:#f92672">+</span> tokens
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34; </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74"> </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>  <span style="color:#960050;background-color:#1e0010">→</span> Token disaster
</span></span></code></pre></div><h3 id="fix-3-json-output---the-10x-token-difference">Fix #3: JSON Output - The 10x Token Difference<a hidden class="anchor" aria-hidden="true" href="#fix-3-json-output---the-10x-token-difference">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># DISASTER: One-shot example (what everyone does)</span>
</span></span><span style="display:flex;"><span>prompt_oneshot <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Extract user info as JSON like this example:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">{
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  &#34;name&#34;: &#34;John Doe&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  &#34;age&#34;: 30,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  &#34;email&#34;: &#34;john@example.com&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  &#34;address&#34;: {
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;street&#34;: &#34;123 Main St&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;city&#34;: &#34;Boston&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;country&#34;: &#34;USA&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  }
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Now extract from: </span><span style="color:#e6db74">{user_text}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;</span>  <span style="color:#75715e"># 50+ tokens for the example alone!</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># BETTER: Pydantic-style schema</span>
</span></span><span style="display:flex;"><span>prompt_schema <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Extract user info:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">{name:str, age:int, email:str, address:{street:str, city:str, country:str}}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">From: </span><span style="color:#e6db74">{user_text}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;</span>  <span style="color:#75715e"># 15 tokens - same result!</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># BEST: Minimal keys only</span>
</span></span><span style="display:flex;"><span>prompt_minimal <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Extract as JSON: name, age, email, city
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">From: </span><span style="color:#e6db74">{user_text}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;</span>  <span style="color:#75715e"># 8 tokens - if you don&#39;t need nested structure</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># TOKEN COUNTS:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># One-shot with formatted JSON: 50-100 tokens</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Pydantic/TypeScript style: 15-25 tokens  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Minimal keys: 8-12 tokens</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The model knows JSON structure! Don&#39;t waste tokens teaching it!</span>
</span></span></code></pre></div><h2 id="the-money-shot-what-this-costs-you">The Money Shot: What This Costs You<a hidden class="anchor" aria-hidden="true" href="#the-money-shot-what-this-costs-you">#</a></h2>
<p>Real company, real numbers:</p>
<ul>
<li>1M API calls/day</li>
<li>Average 150 tokens per call</li>
<li>Poor tokenization adds 30% overhead</li>
</ul>
<p><strong>Monthly waste:</strong></p>
<ul>
<li>Extra tokens: 45M/month</li>
<li>Extra cost: $4,050/month</li>
<li>Extra latency: +25% response time</li>
<li>Lost context: -30% effective window</li>
</ul>
<p><strong>Annual waste: $48,600</strong> (That&rsquo;s an engineer&rsquo;s salary!)</p>
<h2 id="the-13-rule">The 1.3 Rule™<a hidden class="anchor" aria-hidden="true" href="#the-13-rule">#</a></h2>
<blockquote>
<p><strong>Remember this</strong>: If your tokens/word ratio &gt; 1.3, you&rsquo;re doing it wrong. If it&rsquo;s &gt; 1.5, you&rsquo;re lighting money on fire. If it&rsquo;s &gt; 2.0, your tokenizer hates you personally.</p></blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Check your ratio:</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">check_tokenization_health</span>(text, tokenizer):
</span></span><span style="display:flex;"><span>    tokens <span style="color:#f92672">=</span> len(tokenizer<span style="color:#f92672">.</span>encode(text))
</span></span><span style="display:flex;"><span>    words <span style="color:#f92672">=</span> len(text<span style="color:#f92672">.</span>split())
</span></span><span style="display:flex;"><span>    ratio <span style="color:#f92672">=</span> tokens <span style="color:#f92672">/</span> words
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> ratio <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">1.3</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;Optimal&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> ratio <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">1.5</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;Needs work&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> ratio <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">2.0</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;Burning money&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;💀 Tokenizer has personal vendetta against you&#34;</span>
</span></span></code></pre></div><hr>
<blockquote>
<p><strong>💡 Quick Challenge</strong>: Run the token efficiency check on your top 10 production prompts. If the average ratio is &gt;1.5, you&rsquo;re leaving money on the table. Implement Fix #1 and measure again, most teams see 20-30% improvement immediately.</p></blockquote>
<hr>
<h3 id="sidebar-but-what-about-lemmatization">Sidebar: &ldquo;But What About Lemmatization?&rdquo;<a hidden class="anchor" aria-hidden="true" href="#sidebar-but-what-about-lemmatization">#</a></h3>
<details>
<summary>For the NLP nerds: Why we ditched linguistic approaches</summary>
<p>Traditional NLP used lemmatization/stemming to normalize text:</p>
<ul>
<li>&ldquo;running&rdquo; → &ldquo;run&rdquo;</li>
<li>&ldquo;companies&rdquo; → &ldquo;company&rdquo;</li>
</ul>
<p><strong>Why BPE won:</strong></p>
<ol>
<li><strong>Information preservation</strong>: &ldquo;run&rdquo; vs &ldquo;running&rdquo; have different aspects</li>
<li><strong>Language agnostic</strong>: No dictionary needed</li>
<li><strong>Handles new words</strong>: &ldquo;COVID-19&rdquo;, &ldquo;cryptocurrency&rdquo;, &ldquo;rizz&rdquo;</li>
</ol>
<p><strong>When lemmatization still wins:</strong></p>
<ul>
<li>Legal search (need exact root matches)</li>
<li>Small vocabulary models (&lt;30K tokens)</li>
<li>Explainable systems (clients want &ldquo;real words&rdquo;)</li>
</ul>
</details>
<hr>
<p><strong>Takeaway:</strong> Your tokenizer is a compression algorithm wearing an NLP costume. It doesn&rsquo;t understand meaning, it just glues frequent bytes together. Every prompt you write is a cost-optimization problem. Treat it that way: measure, optimize, and stop paying the tokenization tax.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lucven.com/tags/tokenisation/">Tokenisation</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://lucven.com/posts/tokenization/training/">
    <span class="title">« Prev</span>
    <br>
    <span>The Tokenization Decision Tree: When to Train, When to Run, When to Cry</span>
  </a>
  <a class="next" href="https://lucven.com/posts/tokenization/learning_new_concepts/">
    <span class="title">Next »</span>
    <br>
    <span>Why Your Model Can&#39;t Learn New Concepts (Even with Perfect Data)</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tokens Aren&#39;t Meaning — They&#39;re Compression Hacks on x"
            href="https://x.com/intent/tweet/?text=Tokens%20Aren%27t%20Meaning%20%e2%80%94%20They%27re%20Compression%20Hacks&amp;url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2fwhat_are_tokens%2f&amp;hashtags=Tokenisation">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tokens Aren&#39;t Meaning — They&#39;re Compression Hacks on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2fwhat_are_tokens%2f&amp;title=Tokens%20Aren%27t%20Meaning%20%e2%80%94%20They%27re%20Compression%20Hacks&amp;summary=Tokens%20Aren%27t%20Meaning%20%e2%80%94%20They%27re%20Compression%20Hacks&amp;source=https%3a%2f%2flucven.com%2fposts%2ftokenization%2fwhat_are_tokens%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tokens Aren&#39;t Meaning — They&#39;re Compression Hacks on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2fwhat_are_tokens%2f&title=Tokens%20Aren%27t%20Meaning%20%e2%80%94%20They%27re%20Compression%20Hacks">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tokens Aren&#39;t Meaning — They&#39;re Compression Hacks on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flucven.com%2fposts%2ftokenization%2fwhat_are_tokens%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tokens Aren&#39;t Meaning — They&#39;re Compression Hacks on whatsapp"
            href="https://api.whatsapp.com/send?text=Tokens%20Aren%27t%20Meaning%20%e2%80%94%20They%27re%20Compression%20Hacks%20-%20https%3a%2f%2flucven.com%2fposts%2ftokenization%2fwhat_are_tokens%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tokens Aren&#39;t Meaning — They&#39;re Compression Hacks on telegram"
            href="https://telegram.me/share/url?text=Tokens%20Aren%27t%20Meaning%20%e2%80%94%20They%27re%20Compression%20Hacks&amp;url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2fwhat_are_tokens%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Tokens Aren&#39;t Meaning — They&#39;re Compression Hacks on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Tokens%20Aren%27t%20Meaning%20%e2%80%94%20They%27re%20Compression%20Hacks&u=https%3a%2f%2flucven.com%2fposts%2ftokenization%2fwhat_are_tokens%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer><script src="https://giscus.app/client.js"
  data-repo="lakshaychhabra/lucven-comments"
  data-repo-id="R_kgDOPYOEYw"
  data-category="Q&A"
  data-category-id="DIC_kwDOPYOEY84CtxWt"
  data-mapping="pathname"
  data-strict="0"
  data-reactions-enabled="1"
  data-emit-metadata="0"
  data-input-position="top"
  data-theme="preferred_color_scheme"
  data-lang="en"
  data-loading="lazy"
  crossorigin="anonymous"
  async>
</script>


</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://lucven.com/">Lucven AI</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
