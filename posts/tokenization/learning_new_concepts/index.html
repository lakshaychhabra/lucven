<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Why Your Model Can&#39;t Learn New Concepts (Even with Perfect Data) | Lucven AI</title>
<meta name="keywords" content="Tokenisation">
<meta name="description" content="Why Your Model Can&rsquo;t Learn New Concepts (Even with Perfect Data)
You just spent months annotating 50,000 examples of your proprietary concept &ldquo;TurboIN&rdquo; (your new indexing architecture for Indian markets). Your model still thinks it&rsquo;s about turbochargers in Indiana. Not a data quality issue. Not a quantity issue. Your model literally cannot learn concepts that don&rsquo;t exist in its tokenizer embedding space. You&rsquo;re trying to teach calculus to someone who doesn&rsquo;t have numbers.">
<meta name="author" content="Lakshay Chhabra">
<link rel="canonical" href="https://lucven.com/posts/tokenization/learning_new_concepts/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://lucven.com/favicon_io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lucven.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lucven.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lucven.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://lucven.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://lucven.com/posts/tokenization/learning_new_concepts/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:url" content="https://lucven.com/posts/tokenization/learning_new_concepts/">
  <meta property="og:site_name" content="Lucven AI">
  <meta property="og:title" content="Why Your Model Can&#39;t Learn New Concepts (Even with Perfect Data)">
  <meta property="og:description" content="Why Your Model Can’t Learn New Concepts (Even with Perfect Data) You just spent months annotating 50,000 examples of your proprietary concept “TurboIN” (your new indexing architecture for Indian markets). Your model still thinks it’s about turbochargers in Indiana. Not a data quality issue. Not a quantity issue. Your model literally cannot learn concepts that don’t exist in its tokenizer embedding space. You’re trying to teach calculus to someone who doesn’t have numbers.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-08-05T22:09:00+05:30">
    <meta property="article:modified_time" content="2025-08-05T22:09:00+05:30">
    <meta property="article:tag" content="Tokenisation">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Why Your Model Can&#39;t Learn New Concepts (Even with Perfect Data)">
<meta name="twitter:description" content="Why Your Model Can&rsquo;t Learn New Concepts (Even with Perfect Data)
You just spent months annotating 50,000 examples of your proprietary concept &ldquo;TurboIN&rdquo; (your new indexing architecture for Indian markets). Your model still thinks it&rsquo;s about turbochargers in Indiana. Not a data quality issue. Not a quantity issue. Your model literally cannot learn concepts that don&rsquo;t exist in its tokenizer embedding space. You&rsquo;re trying to teach calculus to someone who doesn&rsquo;t have numbers.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://lucven.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Why Your Model Can't Learn New Concepts (Even with Perfect Data)",
      "item": "https://lucven.com/posts/tokenization/learning_new_concepts/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Why Your Model Can't Learn New Concepts (Even with Perfect Data)",
  "name": "Why Your Model Can\u0027t Learn New Concepts (Even with Perfect Data)",
  "description": "Why Your Model Can\u0026rsquo;t Learn New Concepts (Even with Perfect Data) You just spent months annotating 50,000 examples of your proprietary concept \u0026ldquo;TurboIN\u0026rdquo; (your new indexing architecture for Indian markets). Your model still thinks it\u0026rsquo;s about turbochargers in Indiana. Not a data quality issue. Not a quantity issue. Your model literally cannot learn concepts that don\u0026rsquo;t exist in its tokenizer embedding space. You\u0026rsquo;re trying to teach calculus to someone who doesn\u0026rsquo;t have numbers.\n",
  "keywords": [
    "Tokenisation"
  ],
  "articleBody": "Why Your Model Can’t Learn New Concepts (Even with Perfect Data) You just spent months annotating 50,000 examples of your proprietary concept “TurboIN” (your new indexing architecture for Indian markets). Your model still thinks it’s about turbochargers in Indiana. Not a data quality issue. Not a quantity issue. Your model literally cannot learn concepts that don’t exist in its tokenizer embedding space. You’re trying to teach calculus to someone who doesn’t have numbers.\nTL;DR: Models can’t learn genuinely new concepts because their representational capacity was frozen at pretraining. Your “new” concept gets mapped to the nearest existing concepts in embedding space. It’s like trying to explain “purple” using only red and blue pixels you get approximation, never true understanding.\nThe Concept Learning Impossibility Here’s the brutal truth nobody tells you at conferences. When your model sees “TurboIN”, three layers of impossibility hit:\nLayer 1: Tokenization ceiling\nYour term “TurboIN” tokenizes as [“Turbo”, “IN”]. Immediately, your India specific indexing architecture becomes “fast cars” + “Indiana/India country code”. The model never sees “TurboIN” as a unit, it sees car parts and geography.\nLayer 2: Embedding space prison\nEach token has fixed embeddings from pretraining. “Turbo” activates neurons trained on turbochargers, turbines, and speed optimization. “IN” activates neurons for India, Indiana, “inside”, and “input”. There are exactly zero neurons for “Indian market indexing architecture” because that concept didn’t exist during pretraining.\nLayer 3: Attention can’t create meaning\nAttention mechanisms can relate tokens brilliantly, but they can’t create new semantic dimensions. It’s like trying to describe a completely new color using only RGB values, you get approximations, not the actual color.\nThe fundamental problem? Your model’s entire conceptual universe was determined at pretraining. Everything after is just remixing existing concepts.\nThe Embedding Space Tragedy Think about this carefully, it’s worse than you realize.\nWhen your model encounters “TurboIN”, it doesn’t learn a new embedding. Instead, it activates existing neurons. The “Turbo” token fires neurons trained on millions of examples about turbochargers, turbines, “turbo mode”, speed optimization. The “IN” token fires neurons for India (the country), Indiana (the state), “in” (preposition), “input” (programming).\nYour sophisticated indexing architecture? It’s being understood as some Frankenstein combination of these unrelated concepts.\nThe model might learn through fine-tuning that when it sees “TurboIN” in certain contexts, it should talk about indexing. But probe deeper, ask it to reason about TurboIN’s properties or compare it to other architectures and you’ll get nonsense about speed improvements in Indian data centers, because that’s what the embedding space contains.\nThe Fine-tuning Delusion Every team thinks fine-tuning will save them. Here’s why it won’t:\nWhat you think happens: The model learns “TurboIN” as a new concept with its own meaning and properties.\nWhat actually happens: The model learns statistical patterns. When it sees “TurboIN” near words like “indexing”, “architecture”, “performance”, it learns to output appropriate responses. But the internal representation is still “turbo + IN”. It’s behavioral conditioning, not conceptual understanding.\nTest this yourself. Fine-tune on thousands of examples, then ask: “What would happen if we combined TurboIN with quantum computing?” The model will hallucinate based on turbochargers and India, not your indexing architecture, because that’s what lives in the embedding space.\nThe Medical Domain Disaster Here’s what happens when this goes wrong in production. Consider a new cancer drug “Nexletrozole” that gets tokenized as [“Nex”, “let”, “ro”, “zole”].\nThe model’s neurons activate for:\n“Nex” → next, nexus, connection concepts “let” → permission, letter, allowing concepts “ro” → Romanian, rotation, read only concepts “zole” → antifungal drug family (metronidazole, fluconazole) When a patient asks about Nexletrozole’s side effects, the model combines its understanding of antifungal medications with random concepts about rotation and permission. The actual drug causes severe bone density loss. The model suggests watching for fungal resistance and dizziness from rotation.\nThis isn’t hypothetical. Models scoring 94% on medical benchmarks fail catastrophically on drugs introduced after their training cutoff.\nBut What If I Have Petabytes of Data? Good question. If you have a petabyte of data with your new concept, won’t the model eventually learn it?\nNo, and here’s why:\nEven with infinite examples, you’re constrained by the model’s architecture. The embedding layer has fixed dimensions. The attention heads have learned specific patterns. The FFN layers have fixed representations. You can adjust weights, but you can’t create new representational capacity.\nThink of it like a piano with 88 keys. You want to play a note between C and C#. No amount of practice will create that note, it doesn’t exist on the instrument. Your model’s “keys” were set during pretraining. Fine-tuning can only teach new songs with existing notes, not create new notes.\nWith enough data, the model becomes excellent at pattern matching knowing when to output “TurboIN” and what phrases to associate with it. But ask it to innovate or reason deeply about TurboIN, and it fails because it’s really thinking about turbochargers and geographical locations.\nWhy Vocabulary Expansion Usually Fails The obvious solution seems to be adding new tokens to the vocabulary. Here’s why this rarely works in practice:\nWhen you add a new token like “TurboIN” to the vocabulary, you need to initialize its embedding. Random initialization means your token starts with no semantic meaning, it’s noise in a 768 dimensional space. The model needs millions of examples to learn proper associations from scratch.\nSome teams try initializing the new token’s embedding based on existing tokens, but this just hardcodes the same problem. You’re still combining “turbo” and “IN” representations, just at the embedding layer instead of tokenization layer.\nThe only real solution is continued pretraining with massive compute budgets, essentially rebuilding the model. This is why OpenAI and Anthropic retrain from scratch rather than expanding vocabularies incrementally. It’s not laziness, it’s mathematical necessity.\nSolutions That Actually Work Solution 1: Semantic Anchoring Instead of fighting the embedding space, work with it. Don’t introduce “TurboIN” as an opaque term. Use descriptive phrases that leverage existing semantic understanding.\nRather than training on “TurboIN improves performance”, use “TurboIN (Turbo Index for Indian markets) improves performance”. The parenthetical expansion helps the model triangulate meaning from known concepts. Yes, it uses more tokens, but the model actually understands what you’re talking about.\nEven better, establish consistent notation: “The Turbo-Index-India system (TurboIN)” on first use, then “TurboIN” afterwards. The model learns the association between the full semantic description and the shorthand. This isn’t a hack, it’s aligning with how the model actually processes information.\nSolution 2: Compositional Encoding Break your concept into atomic pieces the model already understands, then teach the composition pattern rather than trying to create a new atomic concept.\nInstead of teaching “TurboIN” as a monolithic concept, decompose it:\nComponent 1: “High speed indexing” (model understands this) Component 2: “Geo distributed architecture” (model understands this) Component 3: “Indian market optimization” (model understands this) Composition rule: These three work together in specific ways Now when you finetune, you’re not trying to create new embeddings. You’re teaching the model how existing concepts combine in your specific use case. The model can reason about each component and their interactions, giving you actual understanding rather than pattern matching.\nIn practice, structure your training data to explicitly decompose concepts: “TurboIN uses high-speed indexing with geo-distributed architecture optimized for Indian markets” rather than just “TurboIN is fast”.\nSolution 3: Retrieval-Augmented Concepts (RAC) When you need precise concept understanding and have the infrastructure for RAG, you can bypass the embedding problem entirely by retrieving concept definitions at inference time.\nThis works well when:\nYou have a finite set of proprietary concepts Precision is more important than latency You can maintain a concept knowledge base Your concepts evolve frequently Limitations:\nRequires retrieval infrastructure (vector DB, embedding model, orchestration) Adds 200-500ms latency per request Model still won’t deeply reason about your concept, it’s following retrieved instructions If retrieval fails, the model reverts to hallucinating based on token fragments Solution 4: Prefix Tuning for Concept Injection Prefix tuning learns a set of continuous vectors (think of them as “soft prompts”) that prime the model for your concepts without changing the base model weights. Instead of changing what “TurboIN” means in embedding space, you learn a prefix that shifts the model’s attention and processing to interpret existing embeddings differently.\nIt’s like putting on special glasses that make the model see “turbo + IN” as your indexing system. You’re not fighting the embedding space, you’re learning how to reinterpret it. The model’s weights stay frozen; only the prefix vectors are learned.\nThe limitation is that you need these prefix vectors at inference time (adding overhead), and they’re specific to each concept family. But it’s far more parameter-efficient than full fine-tuning and preserves the model’s general capabilities.\nThe Reality Check After all these solutions, here’s what you’re actually achieving versus what’s impossible:\nWhat’s Possible:\nPattern recognition: The model learns when and how to use your concept correctly Contextual behavior: Given the right context, it produces appropriate outputs Associative learning: It can link your concept to related ideas and outcomes Functional approximation: For most business needs, it works well enough What’s Impossible:\nTrue semantic understanding: The model doesn’t have neurons for your concept Novel reasoning: It can’t derive properties you didn’t explicitly train Creative application: It won’t innovate with your concept in unexpected ways Deep compositionality: Complex reasoning about your concept will fail You’re teaching sophisticated pattern matching, not genuine understanding. For most production systems, that’s actually fine, you need correct behavior, not philosophical understanding.\nThe Production Checklist Before you waste money on fine-tuning:\nTokenization Test: How does your concept tokenize? If it splits into unrelated tokens, you’re starting with garbage representations.\nSemantic Neighbor Test: Use the base model to embed your concept and find nearest neighbors. If they’re semantically unrelated, the model will hallucinate.\nCompositional Test: Can you break your concept into understood components? If yes, use compositional encoding.\nBehavioral Sufficiency: Do you need true understanding or just correct behavior? Most applications only need behavior.\nRAG Feasibility: Can you inject definitions at inference? Often more reliable than fine-tuning.\n💡 The Hard Truth: Your model’s conceptual universe was fixed at pretraining. Every “new” concept is just a projection onto that fixed space. You’re not teaching new concepts, you’re teaching patterns that trigger existing concept combinations. Plan accordingly or watch your project fail when someone asks your model to reason about “TurboIN” and it starts talking about turbochargers in Indianapolis.\nTakeaway: Stop trying to teach genuinely new concepts through fine-tuning alone. Use semantic anchoring to leverage existing understanding, compositional encoding to build from known pieces, or accept that you’re teaching behavior patterns, not conceptual understanding. The model that outputs “TurboIN” correctly doesn’t understand TurboIN, it just knows when to say it.\nNext Up: The context window lie: Why 128K tokens doesn’t mean 128K understanding… →\n",
  "wordCount" : "1787",
  "inLanguage": "en",
  "datePublished": "2025-08-05T22:09:00+05:30",
  "dateModified": "2025-08-05T22:09:00+05:30",
  "author":{
    "@type": "Person",
    "name": "Lakshay Chhabra"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lucven.com/posts/tokenization/learning_new_concepts/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Lucven AI",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lucven.com/favicon_io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lucven.com/" accesskey="h" title="Lucven AI (Alt + H)">Lucven AI</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://lucven.com/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://lucven.com/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://lucven.com/">Home</a>&nbsp;»&nbsp;<a href="https://lucven.com/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Why Your Model Can&#39;t Learn New Concepts (Even with Perfect Data)
    </h1>
    <div class="post-meta"><span title='2025-08-05 22:09:00 +0530 IST'>August 5, 2025</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Lakshay Chhabra

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#why-your-model-cant-learn-new-concepts-even-with-perfect-data" aria-label="Why Your Model Can&rsquo;t Learn New Concepts (Even with Perfect Data)">Why Your Model Can&rsquo;t Learn New Concepts (Even with Perfect Data)</a><ul>
                        
                <li>
                    <a href="#the-concept-learning-impossibility" aria-label="The Concept Learning Impossibility">The Concept Learning Impossibility</a></li>
                <li>
                    <a href="#the-embedding-space-tragedy" aria-label="The Embedding Space Tragedy">The Embedding Space Tragedy</a></li>
                <li>
                    <a href="#the-fine-tuning-delusion" aria-label="The Fine-tuning Delusion">The Fine-tuning Delusion</a></li>
                <li>
                    <a href="#the-medical-domain-disaster" aria-label="The Medical Domain Disaster">The Medical Domain Disaster</a></li>
                <li>
                    <a href="#but-what-if-i-have-petabytes-of-data" aria-label="But What If I Have Petabytes of Data?">But What If I Have Petabytes of Data?</a></li>
                <li>
                    <a href="#why-vocabulary-expansion-usually-fails" aria-label="Why Vocabulary Expansion Usually Fails">Why Vocabulary Expansion Usually Fails</a></li>
                <li>
                    <a href="#solutions-that-actually-work" aria-label="Solutions That Actually Work">Solutions That Actually Work</a><ul>
                        
                <li>
                    <a href="#solution-1-semantic-anchoring" aria-label="Solution 1: Semantic Anchoring">Solution 1: Semantic Anchoring</a></li>
                <li>
                    <a href="#solution-2-compositional-encoding" aria-label="Solution 2: Compositional Encoding">Solution 2: Compositional Encoding</a></li>
                <li>
                    <a href="#solution-3-retrieval-augmented-concepts-rac" aria-label="Solution 3: Retrieval-Augmented Concepts (RAC)">Solution 3: Retrieval-Augmented Concepts (RAC)</a></li>
                <li>
                    <a href="#solution-4-prefix-tuning-for-concept-injection" aria-label="Solution 4: Prefix Tuning for Concept Injection">Solution 4: Prefix Tuning for Concept Injection</a></li></ul>
                </li>
                <li>
                    <a href="#the-reality-check" aria-label="The Reality Check">The Reality Check</a></li>
                <li>
                    <a href="#the-production-checklist" aria-label="The Production Checklist">The Production Checklist</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="why-your-model-cant-learn-new-concepts-even-with-perfect-data">Why Your Model Can&rsquo;t Learn New Concepts (Even with Perfect Data)<a hidden class="anchor" aria-hidden="true" href="#why-your-model-cant-learn-new-concepts-even-with-perfect-data">#</a></h1>
<p>You just spent months annotating 50,000 examples of your proprietary concept &ldquo;TurboIN&rdquo; (your new indexing architecture for Indian markets). Your model still thinks it&rsquo;s about turbochargers in Indiana. Not a data quality issue. Not a quantity issue. Your model literally cannot learn concepts that don&rsquo;t exist in its tokenizer embedding space. You&rsquo;re trying to teach calculus to someone who doesn&rsquo;t have numbers.</p>
<blockquote>
<p><strong>TL;DR</strong>: Models can&rsquo;t learn genuinely new concepts because their representational capacity was frozen at pretraining. Your &ldquo;new&rdquo; concept gets mapped to the nearest existing concepts in embedding space. It&rsquo;s like trying to explain &ldquo;purple&rdquo; using only red and blue pixels you get approximation, never true understanding.</p></blockquote>
<h2 id="the-concept-learning-impossibility">The Concept Learning Impossibility<a hidden class="anchor" aria-hidden="true" href="#the-concept-learning-impossibility">#</a></h2>
<p>Here&rsquo;s the brutal truth nobody tells you at conferences. When your model sees &ldquo;TurboIN&rdquo;, three layers of impossibility hit:</p>
<p><strong>Layer 1: Tokenization ceiling</strong><br>
Your term &ldquo;TurboIN&rdquo; tokenizes as [&ldquo;Turbo&rdquo;, &ldquo;IN&rdquo;]. Immediately, your India specific indexing architecture becomes &ldquo;fast cars&rdquo; + &ldquo;Indiana/India country code&rdquo;. The model never sees &ldquo;TurboIN&rdquo; as a unit, it sees car parts and geography.</p>
<p><strong>Layer 2: Embedding space prison</strong><br>
Each token has fixed embeddings from pretraining. &ldquo;Turbo&rdquo; activates neurons trained on turbochargers, turbines, and speed optimization. &ldquo;IN&rdquo; activates neurons for India, Indiana, &ldquo;inside&rdquo;, and &ldquo;input&rdquo;. There are exactly zero neurons for &ldquo;Indian market indexing architecture&rdquo; because that concept didn&rsquo;t exist during pretraining.</p>
<p><strong>Layer 3: Attention can&rsquo;t create meaning</strong><br>
Attention mechanisms can relate tokens brilliantly, but they can&rsquo;t create new semantic dimensions. It&rsquo;s like trying to describe a completely new color using only RGB values, you get approximations, not the actual color.</p>
<p>The fundamental problem? Your model&rsquo;s entire conceptual universe was determined at pretraining. Everything after is just remixing existing concepts.</p>
<h2 id="the-embedding-space-tragedy">The Embedding Space Tragedy<a hidden class="anchor" aria-hidden="true" href="#the-embedding-space-tragedy">#</a></h2>
<p>Think about this carefully, it&rsquo;s worse than you realize.</p>
<p>When your model encounters &ldquo;TurboIN&rdquo;, it doesn&rsquo;t learn a new embedding. Instead, it activates existing neurons. The &ldquo;Turbo&rdquo; token fires neurons trained on millions of examples about turbochargers, turbines, &ldquo;turbo mode&rdquo;, speed optimization. The &ldquo;IN&rdquo; token fires neurons for India (the country), Indiana (the state), &ldquo;in&rdquo; (preposition), &ldquo;input&rdquo; (programming).</p>
<p>Your sophisticated indexing architecture? It&rsquo;s being understood as some Frankenstein combination of these unrelated concepts.</p>
<p>The model might learn through fine-tuning that when it sees &ldquo;TurboIN&rdquo; in certain contexts, it should talk about indexing. But probe deeper, ask it to reason about TurboIN&rsquo;s properties or compare it to other architectures and you&rsquo;ll get nonsense about speed improvements in Indian data centers, because that&rsquo;s what the embedding space contains.</p>
<h2 id="the-fine-tuning-delusion">The Fine-tuning Delusion<a hidden class="anchor" aria-hidden="true" href="#the-fine-tuning-delusion">#</a></h2>
<p>Every team thinks fine-tuning will save them. Here&rsquo;s why it won&rsquo;t:</p>
<p><strong>What you think happens:</strong> The model learns &ldquo;TurboIN&rdquo; as a new concept with its own meaning and properties.</p>
<p><strong>What actually happens:</strong> The model learns statistical patterns. When it sees &ldquo;TurboIN&rdquo; near words like &ldquo;indexing&rdquo;, &ldquo;architecture&rdquo;, &ldquo;performance&rdquo;, it learns to output appropriate responses. But the internal representation is still &ldquo;turbo + IN&rdquo;. It&rsquo;s behavioral conditioning, not conceptual understanding.</p>
<p>Test this yourself. Fine-tune on thousands of examples, then ask: &ldquo;What would happen if we combined TurboIN with quantum computing?&rdquo; The model will hallucinate based on turbochargers and India, not your indexing architecture, because that&rsquo;s what lives in the embedding space.</p>
<h2 id="the-medical-domain-disaster">The Medical Domain Disaster<a hidden class="anchor" aria-hidden="true" href="#the-medical-domain-disaster">#</a></h2>
<p>Here&rsquo;s what happens when this goes wrong in production. Consider a new cancer drug &ldquo;Nexletrozole&rdquo; that gets tokenized as [&ldquo;Nex&rdquo;, &ldquo;let&rdquo;, &ldquo;ro&rdquo;, &ldquo;zole&rdquo;].</p>
<p>The model&rsquo;s neurons activate for:</p>
<ul>
<li><strong>&ldquo;Nex&rdquo;</strong> → next, nexus, connection concepts</li>
<li><strong>&ldquo;let&rdquo;</strong> → permission, letter, allowing concepts</li>
<li><strong>&ldquo;ro&rdquo;</strong> → Romanian, rotation, read only concepts</li>
<li><strong>&ldquo;zole&rdquo;</strong> → antifungal drug family (metronidazole, fluconazole)</li>
</ul>
<p>When a patient asks about Nexletrozole&rsquo;s side effects, the model combines its understanding of antifungal medications with random concepts about rotation and permission. The actual drug causes severe bone density loss. The model suggests watching for fungal resistance and dizziness from rotation.</p>
<p>This isn&rsquo;t hypothetical. Models scoring 94% on medical benchmarks fail catastrophically on drugs introduced after their training cutoff.</p>
<h2 id="but-what-if-i-have-petabytes-of-data">But What If I Have Petabytes of Data?<a hidden class="anchor" aria-hidden="true" href="#but-what-if-i-have-petabytes-of-data">#</a></h2>
<p>Good question. If you have a petabyte of data with your new concept, won&rsquo;t the model eventually learn it?</p>
<p><strong>No, and here&rsquo;s why:</strong></p>
<p>Even with infinite examples, you&rsquo;re constrained by the model&rsquo;s architecture. The embedding layer has fixed dimensions. The attention heads have learned specific patterns. The FFN layers have fixed representations. You can adjust weights, but you can&rsquo;t create new representational capacity.</p>
<p>Think of it like a piano with 88 keys. You want to play a note between C and C#. No amount of practice will create that note, it doesn&rsquo;t exist on the instrument. Your model&rsquo;s &ldquo;keys&rdquo; were set during pretraining. Fine-tuning can only teach new songs with existing notes, not create new notes.</p>
<p>With enough data, the model becomes excellent at pattern matching knowing when to output &ldquo;TurboIN&rdquo; and what phrases to associate with it. But ask it to innovate or reason deeply about TurboIN, and it fails because it&rsquo;s really thinking about turbochargers and geographical locations.</p>
<h2 id="why-vocabulary-expansion-usually-fails">Why Vocabulary Expansion Usually Fails<a hidden class="anchor" aria-hidden="true" href="#why-vocabulary-expansion-usually-fails">#</a></h2>
<p>The obvious solution seems to be adding new tokens to the vocabulary. Here&rsquo;s why this rarely works in practice:</p>
<p>When you add a new token like &ldquo;TurboIN&rdquo; to the vocabulary, you need to initialize its embedding. Random initialization means your token starts with no semantic meaning, it&rsquo;s noise in a 768 dimensional space. The model needs millions of examples to learn proper associations from scratch.</p>
<p>Some teams try initializing the new token&rsquo;s embedding based on existing tokens, but this just hardcodes the same problem. You&rsquo;re still combining &ldquo;turbo&rdquo; and &ldquo;IN&rdquo; representations, just at the embedding layer instead of tokenization layer.</p>
<p>The only real solution is continued pretraining with massive compute budgets, essentially rebuilding the model. This is why OpenAI and Anthropic retrain from scratch rather than expanding vocabularies incrementally. It&rsquo;s not laziness, it&rsquo;s mathematical necessity.</p>
<h2 id="solutions-that-actually-work">Solutions That Actually Work<a hidden class="anchor" aria-hidden="true" href="#solutions-that-actually-work">#</a></h2>
<h3 id="solution-1-semantic-anchoring">Solution 1: Semantic Anchoring<a hidden class="anchor" aria-hidden="true" href="#solution-1-semantic-anchoring">#</a></h3>
<p>Instead of fighting the embedding space, work with it. Don&rsquo;t introduce &ldquo;TurboIN&rdquo; as an opaque term. Use descriptive phrases that leverage existing semantic understanding.</p>
<p>Rather than training on &ldquo;TurboIN improves performance&rdquo;, use &ldquo;TurboIN (Turbo Index for Indian markets) improves performance&rdquo;. The parenthetical expansion helps the model triangulate meaning from known concepts. Yes, it uses more tokens, but the model actually understands what you&rsquo;re talking about.</p>
<p>Even better, establish consistent notation: &ldquo;The Turbo-Index-India system (TurboIN)&rdquo; on first use, then &ldquo;TurboIN&rdquo; afterwards. The model learns the association between the full semantic description and the shorthand. This isn&rsquo;t a hack, it&rsquo;s aligning with how the model actually processes information.</p>
<h3 id="solution-2-compositional-encoding">Solution 2: Compositional Encoding<a hidden class="anchor" aria-hidden="true" href="#solution-2-compositional-encoding">#</a></h3>
<p>Break your concept into atomic pieces the model already understands, then teach the composition pattern rather than trying to create a new atomic concept.</p>
<p>Instead of teaching &ldquo;TurboIN&rdquo; as a monolithic concept, decompose it:</p>
<ul>
<li><strong>Component 1:</strong> &ldquo;High speed indexing&rdquo; (model understands this)</li>
<li><strong>Component 2:</strong> &ldquo;Geo distributed architecture&rdquo; (model understands this)</li>
<li><strong>Component 3:</strong> &ldquo;Indian market optimization&rdquo; (model understands this)</li>
<li><strong>Composition rule:</strong> These three work together in specific ways</li>
</ul>
<p>Now when you finetune, you&rsquo;re not trying to create new embeddings. You&rsquo;re teaching the model how existing concepts combine in your specific use case. The model can reason about each component and their interactions, giving you actual understanding rather than pattern matching.</p>
<p>In practice, structure your training data to explicitly decompose concepts: &ldquo;TurboIN uses high-speed indexing with geo-distributed architecture optimized for Indian markets&rdquo; rather than just &ldquo;TurboIN is fast&rdquo;.</p>
<h3 id="solution-3-retrieval-augmented-concepts-rac">Solution 3: Retrieval-Augmented Concepts (RAC)<a hidden class="anchor" aria-hidden="true" href="#solution-3-retrieval-augmented-concepts-rac">#</a></h3>
<p>When you need precise concept understanding and have the infrastructure for RAG, you can bypass the embedding problem entirely by retrieving concept definitions at inference time.</p>
<p><strong>This works well when:</strong></p>
<ul>
<li>You have a finite set of proprietary concepts</li>
<li>Precision is more important than latency</li>
<li>You can maintain a concept knowledge base</li>
<li>Your concepts evolve frequently</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Requires retrieval infrastructure (vector DB, embedding model, orchestration)</li>
<li>Adds 200-500ms latency per request</li>
<li>Model still won&rsquo;t deeply reason about your concept, it&rsquo;s following retrieved instructions</li>
<li>If retrieval fails, the model reverts to hallucinating based on token fragments</li>
</ul>
<h3 id="solution-4-prefix-tuning-for-concept-injection">Solution 4: Prefix Tuning for Concept Injection<a hidden class="anchor" aria-hidden="true" href="#solution-4-prefix-tuning-for-concept-injection">#</a></h3>
<p>Prefix tuning learns a set of continuous vectors (think of them as &ldquo;soft prompts&rdquo;) that prime the model for your concepts without changing the base model weights. Instead of changing what &ldquo;TurboIN&rdquo; means in embedding space, you learn a prefix that shifts the model&rsquo;s attention and processing to interpret existing embeddings differently.</p>
<p>It&rsquo;s like putting on special glasses that make the model see &ldquo;turbo + IN&rdquo; as your indexing system. You&rsquo;re not fighting the embedding space, you&rsquo;re learning how to reinterpret it. The model&rsquo;s weights stay frozen; only the prefix vectors are learned.</p>
<p>The limitation is that you need these prefix vectors at inference time (adding overhead), and they&rsquo;re specific to each concept family. But it&rsquo;s far more parameter-efficient than full fine-tuning and preserves the model&rsquo;s general capabilities.</p>
<h2 id="the-reality-check">The Reality Check<a hidden class="anchor" aria-hidden="true" href="#the-reality-check">#</a></h2>
<p>After all these solutions, here&rsquo;s what you&rsquo;re actually achieving versus what&rsquo;s impossible:</p>
<p><strong>What&rsquo;s Possible:</strong></p>
<ul>
<li>Pattern recognition: The model learns when and how to use your concept correctly</li>
<li>Contextual behavior: Given the right context, it produces appropriate outputs</li>
<li>Associative learning: It can link your concept to related ideas and outcomes</li>
<li>Functional approximation: For most business needs, it works well enough</li>
</ul>
<p><strong>What&rsquo;s Impossible:</strong></p>
<ul>
<li>True semantic understanding: The model doesn&rsquo;t have neurons for your concept</li>
<li>Novel reasoning: It can&rsquo;t derive properties you didn&rsquo;t explicitly train</li>
<li>Creative application: It won&rsquo;t innovate with your concept in unexpected ways</li>
<li>Deep compositionality: Complex reasoning about your concept will fail</li>
</ul>
<p>You&rsquo;re teaching sophisticated pattern matching, not genuine understanding. For most production systems, that&rsquo;s actually fine, you need correct behavior, not philosophical understanding.</p>
<h2 id="the-production-checklist">The Production Checklist<a hidden class="anchor" aria-hidden="true" href="#the-production-checklist">#</a></h2>
<p>Before you waste money on fine-tuning:</p>
<ol>
<li>
<p><strong>Tokenization Test</strong>: How does your concept tokenize? If it splits into unrelated tokens, you&rsquo;re starting with garbage representations.</p>
</li>
<li>
<p><strong>Semantic Neighbor Test</strong>: Use the base model to embed your concept and find nearest neighbors. If they&rsquo;re semantically unrelated, the model will hallucinate.</p>
</li>
<li>
<p><strong>Compositional Test</strong>: Can you break your concept into understood components? If yes, use compositional encoding.</p>
</li>
<li>
<p><strong>Behavioral Sufficiency</strong>: Do you need true understanding or just correct behavior? Most applications only need behavior.</p>
</li>
<li>
<p><strong>RAG Feasibility</strong>: Can you inject definitions at inference? Often more reliable than fine-tuning.</p>
</li>
</ol>
<hr>
<blockquote>
<p><strong>💡 The Hard Truth</strong>: Your model&rsquo;s conceptual universe was fixed at pretraining. Every &ldquo;new&rdquo; concept is just a projection onto that fixed space. You&rsquo;re not teaching new concepts, you&rsquo;re teaching patterns that trigger existing concept combinations. Plan accordingly or watch your project fail when someone asks your model to reason about &ldquo;TurboIN&rdquo; and it starts talking about turbochargers in Indianapolis.</p></blockquote>
<hr>
<p><strong>Takeaway:</strong> Stop trying to teach genuinely new concepts through fine-tuning alone. Use semantic anchoring to leverage existing understanding, compositional encoding to build from known pieces, or accept that you&rsquo;re teaching behavior patterns, not conceptual understanding. The model that outputs &ldquo;TurboIN&rdquo; correctly doesn&rsquo;t understand TurboIN, it just knows when to say it.</p>
<p><strong>Next Up:</strong> The context window lie: Why 128K tokens doesn&rsquo;t mean 128K understanding&hellip; →</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lucven.com/tags/tokenisation/">Tokenisation</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://lucven.com/posts/tokenization/what_are_tokens/">
    <span class="title">« Prev</span>
    <br>
    <span>Tokens Aren&#39;t Meaning — They&#39;re Compression Hacks</span>
  </a>
  <a class="next" href="https://lucven.com/posts/tokenization/gems/">
    <span class="title">Next »</span>
    <br>
    <span>10 Ways Tokenization Screws With Your Model (and Wallet)</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Why Your Model Can&#39;t Learn New Concepts (Even with Perfect Data) on x"
            href="https://x.com/intent/tweet/?text=Why%20Your%20Model%20Can%27t%20Learn%20New%20Concepts%20%28Even%20with%20Perfect%20Data%29&amp;url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2flearning_new_concepts%2f&amp;hashtags=Tokenisation">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Why Your Model Can&#39;t Learn New Concepts (Even with Perfect Data) on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2flearning_new_concepts%2f&amp;title=Why%20Your%20Model%20Can%27t%20Learn%20New%20Concepts%20%28Even%20with%20Perfect%20Data%29&amp;summary=Why%20Your%20Model%20Can%27t%20Learn%20New%20Concepts%20%28Even%20with%20Perfect%20Data%29&amp;source=https%3a%2f%2flucven.com%2fposts%2ftokenization%2flearning_new_concepts%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Why Your Model Can&#39;t Learn New Concepts (Even with Perfect Data) on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2flearning_new_concepts%2f&title=Why%20Your%20Model%20Can%27t%20Learn%20New%20Concepts%20%28Even%20with%20Perfect%20Data%29">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Why Your Model Can&#39;t Learn New Concepts (Even with Perfect Data) on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flucven.com%2fposts%2ftokenization%2flearning_new_concepts%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Why Your Model Can&#39;t Learn New Concepts (Even with Perfect Data) on whatsapp"
            href="https://api.whatsapp.com/send?text=Why%20Your%20Model%20Can%27t%20Learn%20New%20Concepts%20%28Even%20with%20Perfect%20Data%29%20-%20https%3a%2f%2flucven.com%2fposts%2ftokenization%2flearning_new_concepts%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Why Your Model Can&#39;t Learn New Concepts (Even with Perfect Data) on telegram"
            href="https://telegram.me/share/url?text=Why%20Your%20Model%20Can%27t%20Learn%20New%20Concepts%20%28Even%20with%20Perfect%20Data%29&amp;url=https%3a%2f%2flucven.com%2fposts%2ftokenization%2flearning_new_concepts%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Why Your Model Can&#39;t Learn New Concepts (Even with Perfect Data) on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Why%20Your%20Model%20Can%27t%20Learn%20New%20Concepts%20%28Even%20with%20Perfect%20Data%29&u=https%3a%2f%2flucven.com%2fposts%2ftokenization%2flearning_new_concepts%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer><script src="https://giscus.app/client.js"
  data-repo="lakshaychhabra/lucven-comments"
  data-repo-id="R_kgDOPYOEYw"
  data-category="Q&A"
  data-category-id="DIC_kwDOPYOEY84CtxWt"
  data-mapping="pathname"
  data-strict="0"
  data-reactions-enabled="1"
  data-emit-metadata="0"
  data-input-position="top"
  data-theme="preferred_color_scheme"
  data-lang="en"
  data-loading="lazy"
  crossorigin="anonymous"
  async>
</script>


</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://lucven.com/">Lucven AI</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
